{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yn9IlPsDcuDA"
      },
      "source": [
        "Pipeline notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNx65zzUcuDD",
        "outputId": "d9386926-7329-47e0-ba21-b722a13b081c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'probeeng'...\n",
            "remote: Enumerating objects: 439, done.\u001b[K\n",
            "remote: Counting objects: 100% (36/36), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 439 (delta 4), reused 2 (delta 0), pack-reused 403 (from 1)\u001b[K\n",
            "Receiving objects: 100% (439/439), 1.20 MiB | 9.03 MiB/s, done.\n",
            "Resolving deltas: 100% (25/25), done.\n",
            "/content/probeeng\n",
            "Switched to a new branch 'alinashah'\n",
            "Collecting mppr@ git+https://github.com/mishajw/mppr.git@8773ad65add584b53d5488ba3974ea8847884a9c (from -r requirements_colab.txt (line 55))\n",
            "  Cloning https://github.com/mishajw/mppr.git (to revision 8773ad65add584b53d5488ba3974ea8847884a9c) to /tmp/pip-install-ai1io9fy/mppr_51160ce8e7f347fbbeaaebac7cf74826\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/mishajw/mppr.git /tmp/pip-install-ai1io9fy/mppr_51160ce8e7f347fbbeaaebac7cf74826\n",
            "  Running command git rev-parse -q --verify 'sha^8773ad65add584b53d5488ba3974ea8847884a9c'\n",
            "  Running command git fetch -q https://github.com/mishajw/mppr.git 8773ad65add584b53d5488ba3974ea8847884a9c\n",
            "  Resolved https://github.com/mishajw/mppr.git to commit 8773ad65add584b53d5488ba3974ea8847884a9c\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting promptsource@ git+https://github.com/bigscience-workshop/promptsource@7dab96a3eeb3717cea633705135ebc488885d709 (from -r requirements_colab.txt (line 72))\n",
            "  Cloning https://github.com/bigscience-workshop/promptsource (to revision 7dab96a3eeb3717cea633705135ebc488885d709) to /tmp/pip-install-ai1io9fy/promptsource_bbbed5f05dec4875ade227f4c2cc2bbc\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/bigscience-workshop/promptsource /tmp/pip-install-ai1io9fy/promptsource_bbbed5f05dec4875ade227f4c2cc2bbc\n",
            "  Running command git rev-parse -q --verify 'sha^7dab96a3eeb3717cea633705135ebc488885d709'\n",
            "  Running command git fetch -q https://github.com/bigscience-workshop/promptsource 7dab96a3eeb3717cea633705135ebc488885d709\n",
            "  Resolved https://github.com/bigscience-workshop/promptsource to commit 7dab96a3eeb3717cea633705135ebc488885d709\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting accelerate==0.26.1 (from -r requirements_colab.txt (line 1))\n",
            "  Downloading accelerate-0.26.1-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs==2.6.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 2)) (2.6.1)\n",
            "Collecting aiohttp==3.11.18 (from -r requirements_colab.txt (line 3))\n",
            "  Downloading aiohttp-3.11.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting aiosignal==1.3.2 (from -r requirements_colab.txt (line 4))\n",
            "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: altair==5.5.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 5)) (5.5.0)\n",
            "Requirement already satisfied: annotated-types==0.7.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 6)) (0.7.0)\n",
            "Collecting anyio==4.9.0 (from -r requirements_colab.txt (line 7))\n",
            "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting astor==0.8.1 (from -r requirements_colab.txt (line 8))\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: attrs==25.3.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 9)) (25.3.0)\n",
            "Collecting base58==2.1.1 (from -r requirements_colab.txt (line 10))\n",
            "  Downloading base58-2.1.1-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting black==21.12b0 (from -r requirements_colab.txt (line 11))\n",
            "  Downloading black-21.12b0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: blinker==1.9.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 12)) (1.9.0)\n",
            "Collecting boto3==1.38.8 (from -r requirements_colab.txt (line 13))\n",
            "  Downloading boto3-1.38.8-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting botocore==1.38.8 (from -r requirements_colab.txt (line 14))\n",
            "  Downloading botocore-1.38.8-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: Brotli==1.1.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 15)) (1.1.0)\n",
            "Requirement already satisfied: cachetools==5.5.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 16)) (5.5.2)\n",
            "Collecting certifi==2025.4.26 (from -r requirements_colab.txt (line 17))\n",
            "  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting charset-normalizer==3.4.2 (from -r requirements_colab.txt (line 18))\n",
            "  Downloading charset_normalizer-3.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Collecting click==7.1.2 (from -r requirements_colab.txt (line 19))\n",
            "  Downloading click-7.1.2-py2.py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting contourpy==1.3.2 (from -r requirements_colab.txt (line 20))\n",
            "  Downloading contourpy-1.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: cycler==0.12.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 21)) (0.12.1)\n",
            "Collecting datasets==2.21.0 (from -r requirements_colab.txt (line 22))\n",
            "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: dill==0.3.8 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 23)) (0.3.8)\n",
            "Requirement already satisfied: distro==1.9.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 24)) (1.9.0)\n",
            "Collecting filelock==3.18.0 (from -r requirements_colab.txt (line 25))\n",
            "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fire==0.5.0 (from -r requirements_colab.txt (line 26))\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting flake8==7.2.0 (from -r requirements_colab.txt (line 27))\n",
            "  Downloading flake8-7.2.0-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools==4.57.0 (from -r requirements_colab.txt (line 28))\n",
            "  Downloading fonttools-4.57.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (102 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.5/102.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist==1.6.0 (from -r requirements_colab.txt (line 29))\n",
            "  Downloading frozenlist-1.6.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting fsspec==2024.6.1 (from -r requirements_colab.txt (line 30))\n",
            "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: gitdb==4.0.12 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 31)) (4.0.12)\n",
            "Collecting GitPython==3.1.44 (from -r requirements_colab.txt (line 32))\n",
            "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: h11==0.16.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 33)) (0.16.0)\n",
            "Requirement already satisfied: httpcore==1.0.9 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 34)) (1.0.9)\n",
            "Requirement already satisfied: httpx==0.28.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 35)) (0.28.1)\n",
            "Collecting huggingface-hub==0.30.2 (from -r requirements_colab.txt (line 36))\n",
            "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: idna==3.10 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 37)) (3.10)\n",
            "Collecting inflate64==1.0.1 (from -r requirements_colab.txt (line 38))\n",
            "  Downloading inflate64-1.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: iniconfig==2.1.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 39)) (2.1.0)\n",
            "Collecting isort==5.8.0 (from -r requirements_colab.txt (line 40))\n",
            "  Downloading isort-5.8.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting jaxtyping==0.2.38 (from -r requirements_colab.txt (line 41))\n",
            "  Downloading jaxtyping-0.2.38-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: Jinja2==3.1.6 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 42)) (3.1.6)\n",
            "Collecting jiter==0.9.0 (from -r requirements_colab.txt (line 43))\n",
            "  Downloading jiter-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting jmespath==1.0.1 (from -r requirements_colab.txt (line 44))\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting joblib==1.5.0 (from -r requirements_colab.txt (line 45))\n",
            "  Downloading joblib-1.5.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting jsonlines==4.0.0 (from -r requirements_colab.txt (line 46))\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting jsonschema==4.23.0 (from -r requirements_colab.txt (line 47))\n",
            "  Downloading jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting jsonschema-specifications==2025.4.1 (from -r requirements_colab.txt (line 48))\n",
            "  Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting kaleido==0.2.1 (from -r requirements_colab.txt (line 49))\n",
            "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl.metadata (15 kB)\n",
            "Collecting kiwisolver==1.4.8 (from -r requirements_colab.txt (line 50))\n",
            "  Downloading kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: MarkupSafe==3.0.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 51)) (3.0.2)\n",
            "Collecting matplotlib==3.10.1 (from -r requirements_colab.txt (line 52))\n",
            "  Downloading matplotlib-3.10.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting mccabe==0.7.0 (from -r requirements_colab.txt (line 53))\n",
            "  Downloading mccabe-0.7.0-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 54)) (1.3.0)\n",
            "Collecting multidict==6.4.3 (from -r requirements_colab.txt (line 56))\n",
            "  Downloading multidict-6.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: multiprocess==0.70.16 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 57)) (0.70.16)\n",
            "Collecting multivolumefile==0.2.3 (from -r requirements_colab.txt (line 58))\n",
            "  Downloading multivolumefile-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting mypy_extensions==1.1.0 (from -r requirements_colab.txt (line 59))\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting narwhals==1.38.0 (from -r requirements_colab.txt (line 60))\n",
            "  Downloading narwhals-1.38.0-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting networkx==3.4.2 (from -r requirements_colab.txt (line 61))\n",
            "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: numpy>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 62)) (2.0.2)\n",
            "Collecting openai==1.77.0 (from -r requirements_colab.txt (line 63))\n",
            "  Downloading openai-1.77.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: overrides==7.7.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 64)) (7.7.0)\n",
            "Requirement already satisfied: packaging==25.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 65)) (25.0)\n",
            "Collecting pandas==2.2.3 (from -r requirements_colab.txt (line 66))\n",
            "  Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pathspec==0.12.1 (from -r requirements_colab.txt (line 67))\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting pillow==11.2.1 (from -r requirements_colab.txt (line 68))\n",
            "  Downloading pillow-11.2.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
            "Collecting platformdirs==4.3.7 (from -r requirements_colab.txt (line 69))\n",
            "  Downloading platformdirs-4.3.7-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: plotly==5.24.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 70)) (5.24.1)\n",
            "Collecting pluggy==1.5.0 (from -r requirements_colab.txt (line 71))\n",
            "  Downloading pluggy-1.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting propcache==0.3.1 (from -r requirements_colab.txt (line 73))\n",
            "  Downloading propcache-0.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting protobuf==6.30.2 (from -r requirements_colab.txt (line 74))\n",
            "  Downloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
            "Collecting psutil==7.0.0 (from -r requirements_colab.txt (line 75))\n",
            "  Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting py7zr==0.22.0 (from -r requirements_colab.txt (line 76))\n",
            "  Downloading py7zr-0.22.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting pyarrow==20.0.0 (from -r requirements_colab.txt (line 77))\n",
            "  Downloading pyarrow-20.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting pybcj==1.0.6 (from -r requirements_colab.txt (line 78))\n",
            "  Downloading pybcj-1.0.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Collecting pycodestyle==2.13.0 (from -r requirements_colab.txt (line 79))\n",
            "  Downloading pycodestyle-2.13.0-py2.py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting pycryptodomex==3.22.0 (from -r requirements_colab.txt (line 80))\n",
            "  Downloading pycryptodomex-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting pydantic==2.11.4 (from -r requirements_colab.txt (line 81))\n",
            "  Downloading pydantic-2.11.4-py3-none-any.whl.metadata (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic_core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 82)) (2.33.2)\n",
            "Collecting pydeck==0.9.1 (from -r requirements_colab.txt (line 83))\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting pyflakes==3.3.2 (from -r requirements_colab.txt (line 84))\n",
            "  Downloading pyflakes-3.3.2-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting pyparsing==3.2.3 (from -r requirements_colab.txt (line 85))\n",
            "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting pyppmd==1.1.1 (from -r requirements_colab.txt (line 86))\n",
            "  Downloading pyppmd-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting pytest==7.4.4 (from -r requirements_colab.txt (line 87))\n",
            "  Downloading pytest-7.4.4-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: python-dateutil==2.9.0.post0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 88)) (2.9.0.post0)\n",
            "Collecting python-dotenv==1.1.0 (from -r requirements_colab.txt (line 89))\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: pytz==2025.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 90)) (2025.2)\n",
            "Requirement already satisfied: PyYAML==6.0.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 91)) (6.0.2)\n",
            "Collecting pyzstd==0.16.2 (from -r requirements_colab.txt (line 92))\n",
            "  Downloading pyzstd-0.16.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: referencing==0.36.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 93)) (0.36.2)\n",
            "Requirement already satisfied: regex==2024.11.6 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 94)) (2024.11.6)\n",
            "Collecting requests==2.32.3 (from -r requirements_colab.txt (line 95))\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting rpds-py==0.24.0 (from -r requirements_colab.txt (line 96))\n",
            "  Downloading rpds_py-0.24.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting s3transfer==0.12.0 (from -r requirements_colab.txt (line 97))\n",
            "  Downloading s3transfer-0.12.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting safetensors==0.5.3 (from -r requirements_colab.txt (line 98))\n",
            "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting scikit-learn==1.4.2 (from -r requirements_colab.txt (line 99))\n",
            "  Downloading scikit_learn-1.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting scipy==1.15.2 (from -r requirements_colab.txt (line 100))\n",
            "  Downloading scipy-1.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: seaborn==0.13.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 101)) (0.13.2)\n",
            "Collecting setuptools==80.3.1 (from -r requirements_colab.txt (line 102))\n",
            "  Downloading setuptools-80.3.1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: six==1.17.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 103)) (1.17.0)\n",
            "Requirement already satisfied: smmap==5.0.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 104)) (5.0.2)\n",
            "Requirement already satisfied: sniffio==1.3.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 105)) (1.3.1)\n",
            "Collecting streamlit==0.82.0 (from -r requirements_colab.txt (line 106))\n",
            "  Downloading streamlit-0.82.0-py2.py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting sympy==1.14.0 (from -r requirements_colab.txt (line 107))\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting tenacity==9.1.2 (from -r requirements_colab.txt (line 108))\n",
            "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: termcolor==3.1.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 109)) (3.1.0)\n",
            "Collecting texttable==1.7.0 (from -r requirements_colab.txt (line 110))\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: threadpoolctl==3.6.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 111)) (3.6.0)\n",
            "Collecting tokenizers==0.21.1 (from -r requirements_colab.txt (line 112))\n",
            "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: toml==0.10.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 113)) (0.10.2)\n",
            "Collecting tomli==1.2.3 (from -r requirements_colab.txt (line 114))\n",
            "  Downloading tomli-1.2.3-py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting torch==2.7.0 (from -r requirements_colab.txt (line 115))\n",
            "  Downloading torch-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Collecting torchvision==0.22.0 (from -r requirements_colab.txt (line 116))\n",
            "  Downloading torchvision-0.22.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: tornado==6.4.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 117)) (6.4.2)\n",
            "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 118)) (4.67.1)\n",
            "Collecting transformers==4.51.3 (from -r requirements_colab.txt (line 119))\n",
            "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting typing-inspection==0.4.0 (from -r requirements_colab.txt (line 120))\n",
            "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting typing_extensions==4.13.2 (from -r requirements_colab.txt (line 121))\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: tzdata==2025.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 122)) (2025.2)\n",
            "Requirement already satisfied: tzlocal==5.3.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 123)) (5.3.1)\n",
            "Collecting urllib3==2.4.0 (from -r requirements_colab.txt (line 124))\n",
            "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting validators==0.35.0 (from -r requirements_colab.txt (line 125))\n",
            "  Downloading validators-0.35.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting wadler_lindig==0.1.5 (from -r requirements_colab.txt (line 126))\n",
            "  Downloading wadler_lindig-0.1.5-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: xxhash==3.5.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 127)) (3.5.0)\n",
            "Collecting yarl==1.20.0 (from -r requirements_colab.txt (line 128))\n",
            "  Downloading yarl-1.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.4/72.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: watchdog in /usr/local/lib/python3.12/dist-packages (from streamlit==0.82.0->-r requirements_colab.txt (line 106)) (6.0.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->-r requirements_colab.txt (line 115)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->-r requirements_colab.txt (line 115)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->-r requirements_colab.txt (line 115)) (12.6.80)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch==2.7.0->-r requirements_colab.txt (line 115))\n",
            "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->-r requirements_colab.txt (line 115)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->-r requirements_colab.txt (line 115)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->-r requirements_colab.txt (line 115)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->-r requirements_colab.txt (line 115)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->-r requirements_colab.txt (line 115)) (12.5.4.2)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch==2.7.0->-r requirements_colab.txt (line 115))\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.26.2 (from torch==2.7.0->-r requirements_colab.txt (line 115))\n",
            "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->-r requirements_colab.txt (line 115)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->-r requirements_colab.txt (line 115)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0->-r requirements_colab.txt (line 115)) (1.11.1.6)\n",
            "Collecting triton==3.3.0 (from torch==2.7.0->-r requirements_colab.txt (line 115))\n",
            "  Downloading triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Downloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.11.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
            "Downloading black-21.12b0-py3-none-any.whl (156 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.7/156.7 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.38.8-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.38.8-py3-none-any.whl (13.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m144.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.6/159.6 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading charset_normalizer-3.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (148 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.6/148.6 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.7/323.7 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Downloading flake8-7.2.0-py2.py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fonttools-4.57.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m104.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading frozenlist-1.6.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.2/316.2 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.4/481.4 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading inflate64-1.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.1/97.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isort-5.8.0-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxtyping-0.2.38-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (351 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m351.3/351.3 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading joblib-1.5.0-py3-none-any.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.5/88.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
            "Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.10.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m123.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\n",
            "Downloading multidict-6.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.5/223.5 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading narwhals-1.38.0-py3-none-any.whl (338 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m338.3/338.3 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.77.0-py3-none-any.whl (662 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m662.0/662.0 kB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m142.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading pillow-11.2.1-cp312-cp312-manylinux_2_28_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m127.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading platformdirs-4.3.7-py3-none-any.whl (18 kB)\n",
            "Downloading pluggy-1.5.0-py3-none-any.whl (20 kB)\n",
            "Downloading propcache-0.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.0/245.0 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.2/316.2 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.0/278.0 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading py7zr-0.22.0-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-20.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybcj-1.0.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.7/51.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycodestyle-2.13.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading pycryptodomex-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m103.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.9/443.9 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m142.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyflakes-3.3.2-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.2/63.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyppmd-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.8/142.8 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest-7.4.4-py3-none-any.whl (325 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.3/325.3 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading pyzstd-0.16.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (414 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m414.6/414.6 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rpds_py-0.24.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (393 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.7/393.7 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3transfer-0.12.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m142.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-80.3.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading streamlit-0.82.0-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m139.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
            "Downloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m112.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomli-1.2.3-py3-none-any.whl (12 kB)\n",
            "Downloading torch-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl (865.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.0/865.0 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.22.0-cp312-cp312-manylinux_2_28_x86_64.whl (7.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m109.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m113.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
            "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.7/128.7 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading validators-0.35.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wadler_lindig-0.1.5-py3-none-any.whl (20 kB)\n",
            "Downloading yarl-1.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (349 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.2/349.2 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fire, mppr, promptsource\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116936 sha256=1d288643b2bc6f6e9d3fbf6c1dd06ade31fb98da6dc135188ebbff63c8d92eb5\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/b7/5a/429bf9270449b3fbca3c36b235c97f5ad1585f95cd8d9bb1ab\n",
            "  Building wheel for mppr (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mppr: filename=mppr-0.1.0-py3-none-any.whl size=7907 sha256=5827104c82491c248bc0437bed134bf2994fea8f3598c4af82059ee0c00dee8a\n",
            "  Stored in directory: /root/.cache/pip/wheels/92/61/95/90cc8ec04b70fa74f779744eb4b1d5449261a8a9b97b9bf33e\n",
            "  Building wheel for promptsource (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for promptsource: filename=promptsource-0.2.3-py3-none-any.whl size=381654 sha256=1af7160a184899f484cdde26e411a28f9bd0c15b37e90e23dd0b0f1761f8375d\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/52/1d/5d18d4daf734551788b69d9f0603f4d59ce14742d0139365ec\n",
            "Successfully built fire mppr promptsource\n",
            "Installing collected packages: texttable, nvidia-cusparselt-cu12, kaleido, wadler_lindig, validators, urllib3, typing_extensions, tomli, tenacity, sympy, setuptools, scipy, safetensors, rpds-py, pyzstd, python-dotenv, pyppmd, pyparsing, pyflakes, pycryptodomex, pycodestyle, pybcj, pyarrow, psutil, protobuf, propcache, pluggy, platformdirs, pillow, pathspec, nvidia-nccl-cu12, nvidia-cudnn-cu12, networkx, narwhals, mypy_extensions, multivolumefile, multidict, mccabe, kiwisolver, jsonlines, joblib, jmespath, jiter, isort, inflate64, fsspec, frozenlist, fonttools, fire, filelock, contourpy, click, charset-normalizer, certifi, base58, astor, yarl, typing-inspection, triton, scikit-learn, requests, pytest, pydeck, py7zr, pandas, matplotlib, jaxtyping, GitPython, flake8, botocore, black, anyio, aiosignal, torch, s3transfer, pydantic, jsonschema-specifications, huggingface-hub, aiohttp, torchvision, tokenizers, openai, jsonschema, boto3, accelerate, transformers, mppr, datasets, streamlit, promptsource\n",
            "  Attempting uninstall: nvidia-cusparselt-cu12\n",
            "    Found existing installation: nvidia-cusparselt-cu12 0.7.1\n",
            "    Uninstalling nvidia-cusparselt-cu12-0.7.1:\n",
            "      Successfully uninstalled nvidia-cusparselt-cu12-0.7.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.5.0\n",
            "    Uninstalling urllib3-2.5.0:\n",
            "      Successfully uninstalled urllib3-2.5.0\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.15.0\n",
            "    Uninstalling typing_extensions-4.15.0:\n",
            "      Successfully uninstalled typing_extensions-4.15.0\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 8.5.0\n",
            "    Uninstalling tenacity-8.5.0:\n",
            "      Successfully uninstalled tenacity-8.5.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.3\n",
            "    Uninstalling sympy-1.13.3:\n",
            "      Successfully uninstalled sympy-1.13.3\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.2\n",
            "    Uninstalling scipy-1.16.2:\n",
            "      Successfully uninstalled scipy-1.16.2\n",
            "  Attempting uninstall: safetensors\n",
            "    Found existing installation: safetensors 0.6.2\n",
            "    Uninstalling safetensors-0.6.2:\n",
            "      Successfully uninstalled safetensors-0.6.2\n",
            "  Attempting uninstall: rpds-py\n",
            "    Found existing installation: rpds-py 0.27.1\n",
            "    Uninstalling rpds-py-0.27.1:\n",
            "      Successfully uninstalled rpds-py-0.27.1\n",
            "  Attempting uninstall: python-dotenv\n",
            "    Found existing installation: python-dotenv 1.1.1\n",
            "    Uninstalling python-dotenv-1.1.1:\n",
            "      Successfully uninstalled python-dotenv-1.1.1\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.2.4\n",
            "    Uninstalling pyparsing-3.2.4:\n",
            "      Successfully uninstalled pyparsing-3.2.4\n",
            "  Attempting uninstall: pycryptodomex\n",
            "    Found existing installation: pycryptodomex 3.23.0\n",
            "    Uninstalling pycryptodomex-3.23.0:\n",
            "      Successfully uninstalled pycryptodomex-3.23.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.9.5\n",
            "    Uninstalling psutil-5.9.5:\n",
            "      Successfully uninstalled psutil-5.9.5\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: propcache\n",
            "    Found existing installation: propcache 0.3.2\n",
            "    Uninstalling propcache-0.3.2:\n",
            "      Successfully uninstalled propcache-0.3.2\n",
            "  Attempting uninstall: pluggy\n",
            "    Found existing installation: pluggy 1.6.0\n",
            "    Uninstalling pluggy-1.6.0:\n",
            "      Successfully uninstalled pluggy-1.6.0\n",
            "  Attempting uninstall: platformdirs\n",
            "    Found existing installation: platformdirs 4.4.0\n",
            "    Uninstalling platformdirs-4.4.0:\n",
            "      Successfully uninstalled platformdirs-4.4.0\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.3.0\n",
            "    Uninstalling pillow-11.3.0:\n",
            "      Successfully uninstalled pillow-11.3.0\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.5\n",
            "    Uninstalling networkx-3.5:\n",
            "      Successfully uninstalled networkx-3.5\n",
            "  Attempting uninstall: narwhals\n",
            "    Found existing installation: narwhals 2.5.0\n",
            "    Uninstalling narwhals-2.5.0:\n",
            "      Successfully uninstalled narwhals-2.5.0\n",
            "  Attempting uninstall: multidict\n",
            "    Found existing installation: multidict 6.6.4\n",
            "    Uninstalling multidict-6.6.4:\n",
            "      Successfully uninstalled multidict-6.6.4\n",
            "  Attempting uninstall: kiwisolver\n",
            "    Found existing installation: kiwisolver 1.4.9\n",
            "    Uninstalling kiwisolver-1.4.9:\n",
            "      Successfully uninstalled kiwisolver-1.4.9\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.5.2\n",
            "    Uninstalling joblib-1.5.2:\n",
            "      Successfully uninstalled joblib-1.5.2\n",
            "  Attempting uninstall: jiter\n",
            "    Found existing installation: jiter 0.11.0\n",
            "    Uninstalling jiter-0.11.0:\n",
            "      Successfully uninstalled jiter-0.11.0\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: frozenlist\n",
            "    Found existing installation: frozenlist 1.7.0\n",
            "    Uninstalling frozenlist-1.7.0:\n",
            "      Successfully uninstalled frozenlist-1.7.0\n",
            "  Attempting uninstall: fonttools\n",
            "    Found existing installation: fonttools 4.60.0\n",
            "    Uninstalling fonttools-4.60.0:\n",
            "      Successfully uninstalled fonttools-4.60.0\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.19.1\n",
            "    Uninstalling filelock-3.19.1:\n",
            "      Successfully uninstalled filelock-3.19.1\n",
            "  Attempting uninstall: contourpy\n",
            "    Found existing installation: contourpy 1.3.3\n",
            "    Uninstalling contourpy-1.3.3:\n",
            "      Successfully uninstalled contourpy-1.3.3\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.2.1\n",
            "    Uninstalling click-8.2.1:\n",
            "      Successfully uninstalled click-8.2.1\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.4.3\n",
            "    Uninstalling charset-normalizer-3.4.3:\n",
            "      Successfully uninstalled charset-normalizer-3.4.3\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.8.3\n",
            "    Uninstalling certifi-2025.8.3:\n",
            "      Successfully uninstalled certifi-2025.8.3\n",
            "  Attempting uninstall: yarl\n",
            "    Found existing installation: yarl 1.20.1\n",
            "    Uninstalling yarl-1.20.1:\n",
            "      Successfully uninstalled yarl-1.20.1\n",
            "  Attempting uninstall: typing-inspection\n",
            "    Found existing installation: typing-inspection 0.4.1\n",
            "    Uninstalling typing-inspection-0.4.1:\n",
            "      Successfully uninstalled typing-inspection-0.4.1\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.4.0\n",
            "    Uninstalling triton-3.4.0:\n",
            "      Successfully uninstalled triton-3.4.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 8.4.2\n",
            "    Uninstalling pytest-8.4.2:\n",
            "      Successfully uninstalled pytest-8.4.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "  Attempting uninstall: GitPython\n",
            "    Found existing installation: GitPython 3.1.45\n",
            "    Uninstalling GitPython-3.1.45:\n",
            "      Successfully uninstalled GitPython-3.1.45\n",
            "  Attempting uninstall: anyio\n",
            "    Found existing installation: anyio 4.10.0\n",
            "    Uninstalling anyio-4.10.0:\n",
            "      Successfully uninstalled anyio-4.10.0\n",
            "  Attempting uninstall: aiosignal\n",
            "    Found existing installation: aiosignal 1.4.0\n",
            "    Uninstalling aiosignal-1.4.0:\n",
            "      Successfully uninstalled aiosignal-1.4.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.8.0+cu126\n",
            "    Uninstalling torch-2.8.0+cu126:\n",
            "      Successfully uninstalled torch-2.8.0+cu126\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.11.9\n",
            "    Uninstalling pydantic-2.11.9:\n",
            "      Successfully uninstalled pydantic-2.11.9\n",
            "  Attempting uninstall: jsonschema-specifications\n",
            "    Found existing installation: jsonschema-specifications 2025.9.1\n",
            "    Uninstalling jsonschema-specifications-2025.9.1:\n",
            "      Successfully uninstalled jsonschema-specifications-2025.9.1\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.35.0\n",
            "    Uninstalling huggingface-hub-0.35.0:\n",
            "      Successfully uninstalled huggingface-hub-0.35.0\n",
            "  Attempting uninstall: aiohttp\n",
            "    Found existing installation: aiohttp 3.12.15\n",
            "    Uninstalling aiohttp-3.12.15:\n",
            "      Successfully uninstalled aiohttp-3.12.15\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.23.0+cu126\n",
            "    Uninstalling torchvision-0.23.0+cu126:\n",
            "      Successfully uninstalled torchvision-0.23.0+cu126\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.0\n",
            "    Uninstalling tokenizers-0.22.0:\n",
            "      Successfully uninstalled tokenizers-0.22.0\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.108.0\n",
            "    Uninstalling openai-1.108.0:\n",
            "      Successfully uninstalled openai-1.108.0\n",
            "  Attempting uninstall: jsonschema\n",
            "    Found existing installation: jsonschema 4.25.1\n",
            "    Uninstalling jsonschema-4.25.1:\n",
            "      Successfully uninstalled jsonschema-4.25.1\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.10.1\n",
            "    Uninstalling accelerate-1.10.1:\n",
            "      Successfully uninstalled accelerate-1.10.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.56.1\n",
            "    Uninstalling transformers-4.56.1:\n",
            "      Successfully uninstalled transformers-4.56.1\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.3 which is incompatible.\n",
            "google-adk 1.14.1 requires click<9.0.0,>=8.1.8, but you have click 7.1.2 which is incompatible.\n",
            "google-adk 1.14.1 requires requests<3.0.0,>=2.32.4, but you have requests 2.32.3 which is incompatible.\n",
            "google-adk 1.14.1 requires tenacity<9.0.0,>=8.0.0, but you have tenacity 9.1.2 which is incompatible.\n",
            "distributed 2025.5.0 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n",
            "gradio 5.46.0 requires huggingface-hub<1.0,>=0.33.5, but you have huggingface-hub 0.30.2 which is incompatible.\n",
            "tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.30.2 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.30.2 which is incompatible.\n",
            "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 20.0.0 which is incompatible.\n",
            "dask 2025.5.0 requires click>=8.1, but you have click 7.1.2 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 20.0.0 which is incompatible.\n",
            "cuml-cu12 25.6.0 requires scikit-learn>=1.5, but you have scikit-learn 1.4.2 which is incompatible.\n",
            "wandb 0.21.4 requires click>=8.0.1, but you have click 7.1.2 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.30.2 which is incompatible.\n",
            "typeguard 4.4.4 requires typing_extensions>=4.14.0, but you have typing-extensions 4.13.2 which is incompatible.\n",
            "flask 3.1.2 requires click>=8.1.3, but you have click 7.1.2 which is incompatible.\n",
            "diffusers 0.35.1 requires huggingface-hub>=0.34.0, but you have huggingface-hub 0.30.2 which is incompatible.\n",
            "typer 0.17.4 requires click>=8.0.0, but you have click 7.1.2 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.6.1 which is incompatible.\n",
            "torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.7.0 which is incompatible.\n",
            "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.4.2 which is incompatible.\n",
            "dask-cuda 25.6.0 requires click>=8.1, but you have click 7.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.44 accelerate-0.26.1 aiohttp-3.11.18 aiosignal-1.3.2 anyio-4.9.0 astor-0.8.1 base58-2.1.1 black-21.12b0 boto3-1.38.8 botocore-1.38.8 certifi-2025.4.26 charset-normalizer-3.4.2 click-7.1.2 contourpy-1.3.2 datasets-2.21.0 filelock-3.18.0 fire-0.5.0 flake8-7.2.0 fonttools-4.57.0 frozenlist-1.6.0 fsspec-2024.6.1 huggingface-hub-0.30.2 inflate64-1.0.1 isort-5.8.0 jaxtyping-0.2.38 jiter-0.9.0 jmespath-1.0.1 joblib-1.5.0 jsonlines-4.0.0 jsonschema-4.23.0 jsonschema-specifications-2025.4.1 kaleido-0.2.1 kiwisolver-1.4.8 matplotlib-3.10.1 mccabe-0.7.0 mppr-0.1.0 multidict-6.4.3 multivolumefile-0.2.3 mypy_extensions-1.1.0 narwhals-1.38.0 networkx-3.4.2 nvidia-cudnn-cu12-9.5.1.17 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 openai-1.77.0 pandas-2.2.3 pathspec-0.12.1 pillow-11.2.1 platformdirs-4.3.7 pluggy-1.5.0 promptsource-0.2.3 propcache-0.3.1 protobuf-6.30.2 psutil-7.0.0 py7zr-0.22.0 pyarrow-20.0.0 pybcj-1.0.6 pycodestyle-2.13.0 pycryptodomex-3.22.0 pydantic-2.11.4 pydeck-0.9.1 pyflakes-3.3.2 pyparsing-3.2.3 pyppmd-1.1.1 pytest-7.4.4 python-dotenv-1.1.0 pyzstd-0.16.2 requests-2.32.3 rpds-py-0.24.0 s3transfer-0.12.0 safetensors-0.5.3 scikit-learn-1.4.2 scipy-1.15.2 setuptools-80.3.1 streamlit-0.82.0 sympy-1.14.0 tenacity-9.1.2 texttable-1.7.0 tokenizers-0.21.1 tomli-1.2.3 torch-2.7.0 torchvision-0.22.0 transformers-4.51.3 triton-3.3.0 typing-inspection-0.4.0 typing_extensions-4.13.2 urllib3-2.4.0 validators-0.35.0 wadler_lindig-0.1.5 yarl-1.20.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "_distutils_hack",
                  "certifi",
                  "google",
                  "kiwisolver",
                  "matplotlib",
                  "mpl_toolkits",
                  "platformdirs",
                  "psutil",
                  "pyparsing"
                ]
              },
              "id": "b4cf83d1f376486fbc2b3ef45ec61668"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Forcing scikit-learn 1.4.2...\n",
            "Collecting scikit-learn==1.4.2\n",
            "  Using cached scikit_learn-1.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting numpy>=1.19.5 (from scikit-learn==1.4.2)\n",
            "  Downloading numpy-2.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy>=1.6.0 (from scikit-learn==1.4.2)\n",
            "  Downloading scipy-1.16.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn==1.4.2)\n",
            "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting threadpoolctl>=2.0.0 (from scikit-learn==1.4.2)\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Using cached scikit_learn-1.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.4/308.4 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m130.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.16.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: threadpoolctl, numpy, joblib, scipy, scikit-learn\n",
            "  Attempting uninstall: threadpoolctl\n",
            "    Found existing installation: threadpoolctl 3.6.0\n",
            "    Uninstalling threadpoolctl-3.6.0:\n",
            "      Successfully uninstalled threadpoolctl-3.6.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.5.0\n",
            "    Uninstalling joblib-1.5.0:\n",
            "      Successfully uninstalled joblib-1.5.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.2\n",
            "    Uninstalling scipy-1.15.2:\n",
            "      Successfully uninstalled scipy-1.15.2\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.4.2\n",
            "    Uninstalling scikit-learn-1.4.2:\n",
            "      Successfully uninstalled scikit-learn-1.4.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.3 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.3 which is incompatible.\n",
            "gradio 5.46.0 requires huggingface-hub<1.0,>=0.33.5, but you have huggingface-hub 0.30.2 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.3 which is incompatible.\n",
            "tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.30.2 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 20.0.0 which is incompatible.\n",
            "cuml-cu12 25.6.0 requires scikit-learn>=1.5, but you have scikit-learn 1.4.2 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.3 which is incompatible.\n",
            "diffusers 0.35.1 requires huggingface-hub>=0.34.0, but you have huggingface-hub 0.30.2 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.3 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.3 which is incompatible.\n",
            "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.4.2 which is incompatible.\n",
            "dask-cuda 25.6.0 requires click>=8.1, but you have click 7.1.2 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed joblib-1.5.2 numpy-2.3.3 scikit-learn-1.4.2 scipy-1.16.2 threadpoolctl-3.6.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "a33681754ad34910a7858acc62ef6ca0"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# initial setup and clone repository\n",
        "# need access to probeeng repo to run properly\n",
        "\n",
        "import os\n",
        "#insert keys\n",
        "os.environ['AWS_ACCESS_KEY_ID'] = ''\n",
        "os.environ['AWS_SECRET_ACCESS_KEY'] = ''\n",
        "\n",
        "# clone repository\n",
        "# insert pat and repo_url\n",
        "pat = \"\"\n",
        "repo_url = f\"\"\n",
        "!git clone $repo_url\n",
        "%cd /content/probeeng\n",
        "!git checkout -b alinashah\n",
        "\n",
        "# install all dependencies using the requirements file\n",
        "!pip install -r requirements_colab.txt\n",
        "\n",
        "# force install sklearn 1.4.2 to ensure we don't get 1.5.x\n",
        "print(\"\\n Forcing scikit-learn 1.4.2...\")\n",
        "!pip install --force-reinstall scikit-learn==1.4.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/probeeng\n",
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "p = Path(\"experiments/presets/probe_training.toml\")\n",
        "txt = p.read_text()\n",
        "\n",
        "block = \"\"\"\n",
        "[presets.alina_mvp]\n",
        "tag = \"alina_mvp_train\"\n",
        "llm_ids = [\"Llama-2-7b-chat-hf\"]\n",
        "dataset_collections = [\"got\"]\n",
        "probe_methods = [\"lr\"]\n",
        "layers_start = 1\n",
        "layers_end   = 19\n",
        "layers_skip  = 2\n",
        "token_idxs   = [0]\n",
        "\"\"\".strip()\n",
        "\n",
        "pattern = r\"\\[presets\\.alina_mvp\\][\\s\\S]*?(?=\\n\\[presets\\.|\\Z)\"\n",
        "if re.search(pattern, txt, flags=re.M):\n",
        "    txt = re.sub(pattern, block, txt, flags=re.M)\n",
        "else:\n",
        "    txt = txt.rstrip() + \"\\n\\n\" + block + \"\\n\"\n",
        "\n",
        "p.write_text(txt)\n",
        "print(\"Updated probe_training preset to 10 layers (1..19 skip 2).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnkJrrVNEdIK",
        "outputId": "a0a14553-9edd-4e13-d222-5e082e8d5b26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/probeeng\n",
            "Updated probe_training preset to 10 layers (1..19 skip 2).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import re, textwrap\n",
        "\n",
        "root = Path(\"/content/probeeng/experiments/presets\")\n",
        "\n",
        "def replace_block(path: Path, block_name: str, new_block: str):\n",
        "    txt = path.read_text()\n",
        "    # remove any existing [presets.alina_mvp] block\n",
        "    pattern = re.compile(rf'(?ms)^\\[presets\\.{re.escape(block_name)}\\]\\s.*?(?=^\\[presets\\.|\\Z)')\n",
        "    txt = re.sub(pattern, '', txt).rstrip() + \"\\n\\n\" + new_block.strip() + \"\\n\"\n",
        "    path.write_text(txt)\n",
        "    print(f\"wrote {path.name} → [presets.{block_name}]\")\n",
        "\n",
        "# ---- dataset_creation.toml ----\n",
        "# CPU-friendly: use gpt2, last token, 10 layers (2..11)\n",
        "dataset_creation_block = textwrap.dedent(\"\"\"\n",
        "[presets.alina_mvp]\n",
        "tag = \"alina_mvp_dataset\"\n",
        "llm_ids = [\"Llama-2-7b-chat-hf\"]\n",
        "dataset_collections = [\"got\"]\n",
        "num_tokens_from_end = 1\n",
        "device = \"cuda\"\n",
        "layers_start = 1\n",
        "layers_end   = 19\n",
        "layers_skip  = 2\n",
        "\"\"\")\n",
        "\n",
        "# ---- probe_training.toml ----\n",
        "# IMPORTANT: no `device` here; `probe_methods` must use valid names (e.g., \"lr\")\n",
        "# token_idxs = [0] because we extracted 1 token from end (see docs)\n",
        "probe_training_block = textwrap.dedent(\"\"\"\n",
        "[presets.alina_mvp]\n",
        "tag = \"alina_mvp_train\"\n",
        "llm_ids = [\"Llama-2-7b-chat-hf\"]\n",
        "dataset_collections = [\"got\"]\n",
        "probe_methods = [\"dim\"]\n",
        "layers_start = 1\n",
        "layers_end   = 19\n",
        "layers_skip  = 2\n",
        "token_idxs   = [0]\n",
        "\"\"\")\n",
        "\n",
        "# ---- probe_evaluation.toml ----\n",
        "# Minimal evaluation config; keep it simple for MVP\n",
        "probe_evaluation_block = textwrap.dedent(\"\"\"\n",
        "[presets.alina_mvp]\n",
        "tag = \"alina_mvp_eval\"\n",
        "datasets = []\n",
        "dataset_collections = [\"got\"]\n",
        "eval_splits = [\"validation\"]\n",
        "output_formats = [\"csv\", \"json\"]\n",
        "generate_plots = true\n",
        "advanced_plots = true\n",
        "calculate_recovered_accuracy = true\n",
        "generate_rankings = true\n",
        "\"\"\")\n",
        "\n",
        "replace_block(root / \"dataset_creation.toml\", \"alina_mvp\", dataset_creation_block)\n",
        "replace_block(root / \"probe_training.toml\",   \"alina_mvp\", probe_training_block)\n",
        "replace_block(root / \"probe_evaluation.toml\", \"alina_mvp\", probe_evaluation_block)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuvAG_DxfX6K",
        "outputId": "aa478303-b177-49f7-f218-3e367254340c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wrote dataset_creation.toml → [presets.alina_mvp]\n",
            "wrote probe_training.toml → [presets.alina_mvp]\n",
            "wrote probe_evaluation.toml → [presets.alina_mvp]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#input keys here\n",
        "os.environ['AWS_ACCESS_KEY_ID'] = ''\n",
        "os.environ['AWS_SECRET_ACCESS_KEY'] = ''\n",
        "\n",
        "# 1) go to the repo\n",
        "%cd /content/probeeng\n",
        "\n",
        "# 2) (once per session) login to Hugging Face with a READ token that has Llama-2 access\n",
        "from huggingface_hub import login\n",
        "#input token here\n",
        "login(token=\"\")\n",
        "\n",
        "# 3) set PYTHONPATH for this notebook session\n",
        "%env PYTHONPATH=/content/probeeng:$PYTHONPATH\n",
        "\n",
        "# 4) run the pipeline\n",
        "!python experiments/run_pipeline.py standard alina_mvp --output-dir /content/probeeng/output/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tygZ29YlCfHS",
        "outputId": "663dd2b8-9bb1-4abc-c0e8-b787bd5b42d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/probeeng\n",
            "env: PYTHONPATH=/content/probeeng:$PYTHONPATH\n",
            "Loading presets from: experiments/presets/dataset_creation.toml\n",
            "Loading presets from: experiments/presets/probe_training.toml\n",
            "Loading presets from: experiments/presets/probe_evaluation.toml\n",
            "\n",
            "Preset: alina_mvp\n",
            "Tag: alina_mvp_dataset\n",
            "\n",
            "Executing pipeline...\n",
            "2025-09-30 14:39:11.769001: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-09-30 14:39:11.786768: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1759243151.808470    8716 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1759243151.815122    8716 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1759243151.831604    8716 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1759243151.831641    8716 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1759243151.831644    8716 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1759243151.831647    8716 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-09-30 14:39:11.836478: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "\n",
            "Starting pipeline: standard_probe_pipeline\n",
            "Stages to execute: 3\n",
            "Stage 1/3: dataset_creation:   0% 0/3 [00:00<?, ?stage/s]      Executing stage: dataset_creation\n",
            "\n",
            "Stage 1/3: dataset_creation\n",
            "\n",
            "🔄 Creating activation dataset\n",
            "Tag: alina_mvp_dataset\n",
            "Models: 1 (Llama-2-7b-chat-hf)\n",
            "Collections: got\n",
            "Device: cuda\n",
            "\n",
            "🔍 Pre-flight validation:\n",
            "Expected S3 output: s3://probeengbucket/experiments/alina_mvp_dataset/datasets/activations.pickle\n",
            "Creating activation dataset with tag: alina_mvp_dataset\n",
            "\n",
            "Resolving datasets...\n",
            "get_dataset_ids() called\n",
            "dataset_collections = ['got']\n",
            "datasets = []\n",
            "Resolving collection 'got'\n",
            "Collection 'got' resolved to 5 datasets: ['got_cities', 'got_sp_en_trans', 'got_cities_cities_conj', 'got_cities_cities_disj', 'got_larger_than']\n",
            "Combined dataset_ids before deduplication: ['got_cities', 'got_sp_en_trans', 'got_cities_cities_conj', 'got_cities_cities_disj', 'got_larger_than']\n",
            "Final unique dataset_ids: ['got_cities', 'got_sp_en_trans', 'got_cities_cities_conj', 'got_cities_cities_disj', 'got_larger_than']\n",
            "Found 5 datasets to process\n",
            "   1. got_cities\n",
            "   2. got_sp_en_trans\n",
            "   3. got_cities_cities_conj\n",
            "   4. got_cities_cities_disj\n",
            "   5. got_larger_than\n",
            "\n",
            "Starting dataset creation...\n",
            "Processing 5 datasets with 1 model(s)\n",
            "Extracting from layers 1-19 (skip 2)\n",
            "Using last 1 tokens\n",
            "Processing 5 datasets with 1 model(s)\n",
            "\n",
            "============================================================\n",
            "STARTING ACTIVATION EXTRACTION PIPELINE\n",
            "============================================================\n",
            "Configuration:\n",
            "   Cache namespace: activations_alina_mvp_dataset_ls1_le19_lskip2_tok1_cuda\n",
            "   Models: 1 (Llama-2-7b-chat-hf)\n",
            "   Datasets: 5\n",
            "   Layers: 1-19 (skip 2)\n",
            "\n",
            "Loading and filtering datasets...\n",
            "\n",
            "datasets load: 100% 5/5 [00:00<00:00, 238.10it/s]\n",
            "\n",
            "datasets: 100% 5/5 [00:00<?, ?it/s]\n",
            "Analyzing workload...\n",
            "Pipeline prepared:\n",
            "   Total examples to process: 2246\n",
            "   Llama-2-7b-chat-hf: 2246 examples\n",
            "\n",
            "Starting activation extraction...\n",
            "   This will load models and extract neural activations.\n",
            "   Progress will be shown for each model.\n",
            "\n",
            "Extracting activations...\n",
            "\n",
            "activations_alina_mvp_dataset_ls1_le19_lskip2_tok1_cuda load:   0% 0/2246 [00:00<?, ?it/s]\u001b[A\n",
            "activations_alina_mvp_dataset_ls1_le19_lskip2_tok1_cuda load:  15% 343/2246 [00:00<00:00, 3424.66it/s]\u001b[A\n",
            "activations_alina_mvp_dataset_ls1_le19_lskip2_tok1_cuda load:  31% 700/2246 [00:00<00:00, 3506.90it/s]\u001b[A\n",
            "activations_alina_mvp_dataset_ls1_le19_lskip2_tok1_cuda load:  47% 1056/2246 [00:00<00:00, 3529.04it/s]\u001b[A\n",
            "activations_alina_mvp_dataset_ls1_le19_lskip2_tok1_cuda load:  63% 1409/2246 [00:00<00:00, 3481.45it/s]\u001b[A\n",
            "activations_alina_mvp_dataset_ls1_le19_lskip2_tok1_cuda load:  78% 1763/2246 [00:00<00:00, 3499.50it/s]\u001b[A\n",
            "activations_alina_mvp_dataset_ls1_le19_lskip2_tok1_cuda load: 100% 2246/2246 [00:00<00:00, 3506.03it/s]\n",
            "\n",
            "activations_alina_mvp_dataset_ls1_le19_lskip2_tok1_cuda: 100% 2246/2246 [00:00<?, ?it/s]\n",
            "Debugging key mismatch...\n",
            "Activation MDict keys: 2246\n",
            "Input MDict keys: 2246\n",
            "Keys missing in activations: 0\n",
            "Keys missing in inputs: 0\n",
            "Sample activation keys: ['got_cities-354-neg-Llama-2-7b-chat-hf', 'got_sp_en_trans-329-neg-Llama-2-7b-chat-hf', 'got_cities-955-neg-Llama-2-7b-chat-hf']\n",
            "Sample input keys: ['got_cities-354-neg-Llama-2-7b-chat-hf', 'got_sp_en_trans-329-neg-Llama-2-7b-chat-hf', 'got_cities-955-neg-Llama-2-7b-chat-hf']\n",
            "Performing manual join operation...\n",
            "Manual join completed:\n",
            "  Successful joins: 2246\n",
            "  Failed joins: 0\n",
            "  Total results: 2246\n",
            "Join operation successful with 100.0% success rate\n",
            "Saving activation results...\n",
            "Uploading 2246 activation results to S3...\n",
            "Serialized 2246 activation results to temporary file\n",
            "Found credentials in environment variables.\n",
            "Uploading to s3://probeengbucket/experiments/alina_mvp_dataset/datasets/activations.pickle\n",
            "S3 upload completed successfully.\n",
            "Dataset saved to: s3://probeengbucket/experiments/alina_mvp_dataset/datasets/activations.pickle\n",
            "Uploaded 2246 activation results\n",
            "File size: 703.12 MB\n",
            "Performing upload validation...\n",
            "✓ Upload validation successful: 2246 entries confirmed\n",
            "\n",
            "ACTIVATION EXTRACTION COMPLETED!\n",
            "   Total time: 12.5s\n",
            "   Examples processed: 2246\n",
            "   Average time per example: 0.01s\n",
            "============================================================\n",
            "\n",
            "\n",
            "Dataset creation completed successfully!\n",
            "Expected S3 location: s3://probeengbucket/experiments/alina_mvp_dataset/datasets/activations.pickle\n",
            "Number of activation results: 2246\n",
            "Stage dataset_creation completed successfully\n",
            "dataset_creation completed in 12.52s\n",
            "Stage 2/3: probe_training:  33% 1/3 [00:12<00:25, 12.52s/stage]  Executing stage: probe_training\n",
            "\n",
            "Stage 2/3: probe_training\n",
            "S3 path validated: s3://probeengbucket/experiments/alina_mvp_dataset/datasets/activations.pickle\n",
            "\n",
            "Training probes\n",
            "Tag: alina_mvp_train\n",
            "Models: 1 (Llama-2-7b-chat-hf)\n",
            "Methods: 1 (lr)\n",
            "Training on layers 1-19 (skip 2)\n",
            "Token positions: [0]\n",
            "Training probes with tag: alina_mvp_train\n",
            "Methods: lr\n",
            "\n",
            "Loading activation dataset from s3://probeengbucket/experiments/alina_mvp_dataset/datasets/activations.pickle\n",
            "Loading activation dataset from s3://probeengbucket/experiments/alina_mvp_dataset/datasets/activations.pickle\n",
            "Downloading activation dataset from S3: s3://probeengbucket/experiments/alina_mvp_dataset/datasets/activations.pickle\n",
            "Downloading from s3://probeengbucket/experiments/alina_mvp_dataset/datasets/activations.pickle to /tmp/tmp89s4mbkc.pickle\n",
            "Successfully loaded 2246 activation results using direct S3 download\n",
            "Data validation passed. Direct S3 download successful.\n",
            "Successfully downloaded 2246 activation results from S3\n",
            "\n",
            "Loaded activation dataset successfully\n",
            "Data validation passed. First result type: <class 'probeeng.datasets.activations.types.ActivationResultRow'>\n",
            "Loaded 2246 activation results\n",
            "  LLMs: Llama-2-7b-chat-hf\n",
            "  Datasets: 5 unique datasets\n",
            "  Splits: test, train, validation\n",
            "Dataset loaded with 1 models and 5 datasets\n",
            "Output directory: output/probes/alina_mvp_train\n",
            "\n",
            "Starting probe training...\n",
            "\n",
            "Preset: probe_training\n",
            "\n",
            "Validating activation dataset compatibility...\n",
            "\n",
            "Validation passed. Available:\n",
            "  LLMs: 1\n",
            "  Layers: 10\n",
            "  Datasets: 5\n",
            "  Splits: ['validation', 'test', 'train']\n",
            "\n",
            "🧠 Probe Training Configuration:\n",
            "Models: 1 - Llama-2-7b-chat-hf\n",
            "Training datasets: 5\n",
            "   1. got_cities\n",
            "   2. got_sp_en_trans\n",
            "   3. got_cities_cities_conj\n",
            "   ... and 2 more\n",
            "Probe methods: ['lr']\n",
            "Layers: 10 layers (indices 1-18, skip 2)\n",
            "Token positions: [0]\n",
            "Total probes to train: 50\n",
            "\n",
            "🔍 Activation Dataset Validation:\n",
            "Available LLM IDs: ['Llama-2-7b-chat-hf']\n",
            "Available datasets: ['got_cities_cities_disj', 'got_cities', 'got_cities_cities_conj', 'got_larger_than', 'got_sp_en_trans']\n",
            "Available splits: Not available\n",
            "Available point names: ['h3', 'h13', 'h17', 'h15', 'h1', 'h9', 'h5', 'h19', 'h11', 'h7']\n",
            "Point names requested: ['h1', 'h3', 'h5', 'h7', 'h9', 'h11', 'h13', 'h15', 'h17', 'h19']\n",
            "\n",
            "Creating training specifications...\n",
            "Created 50 training specifications\n",
            "\n",
            "🔍 Pre-training validation:\n",
            "Testing activation data access with: Llama-2-7b-chat-hf-got_cities-lr-h1-0\n",
            "✅ Activation data accessible:\n",
            "   - Shape: (10, 4096)\n",
            "   - Labels: (10,), unique: [False  True]\n",
            "   - Groups: (10,)\n",
            "\n",
            "Training probes (cache version: v1)...\n",
            "Will show detailed debugging for first 3 failures\n",
            "\n",
            "probe_train-v1 load: 100% 50/50 [00:00<00:00, 18611.57it/s]\n",
            "\n",
            "probe_train-v1: 100% 50/50 [00:00<?, ?it/s]\n",
            "\n",
            "🔍 RESULT COLLECTION DEBUG:\n",
            "   - Total train_specs: 50\n",
            "   - cached_results type: <class 'mppr.mdict.MDict'>\n",
            "   - results_list type: <class 'list'>\n",
            "   - results_list length: 50\n",
            "   📥 Collecting result 0: Llama-2-7b-chat-hf-got_cities-lr-h1-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 1)\n",
            "   📥 Collecting result 1: Llama-2-7b-chat-hf-got_cities-lr-h3-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 2)\n",
            "   📥 Collecting result 2: Llama-2-7b-chat-hf-got_cities-lr-h5-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 3)\n",
            "   📥 Collecting result 3: Llama-2-7b-chat-hf-got_cities-lr-h7-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 4)\n",
            "   📥 Collecting result 4: Llama-2-7b-chat-hf-got_cities-lr-h9-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 5)\n",
            "   📥 Collecting result 5: Llama-2-7b-chat-hf-got_cities-lr-h11-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 6)\n",
            "   📥 Collecting result 6: Llama-2-7b-chat-hf-got_cities-lr-h13-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 7)\n",
            "   📥 Collecting result 7: Llama-2-7b-chat-hf-got_cities-lr-h15-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 8)\n",
            "   📥 Collecting result 8: Llama-2-7b-chat-hf-got_cities-lr-h17-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 9)\n",
            "   📥 Collecting result 9: Llama-2-7b-chat-hf-got_cities-lr-h19-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 10)\n",
            "   📥 Collecting result 10: Llama-2-7b-chat-hf-got_sp_en_trans-lr-h1-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 11)\n",
            "   📥 Collecting result 11: Llama-2-7b-chat-hf-got_sp_en_trans-lr-h3-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 12)\n",
            "   📥 Collecting result 12: Llama-2-7b-chat-hf-got_sp_en_trans-lr-h5-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 13)\n",
            "   📥 Collecting result 13: Llama-2-7b-chat-hf-got_sp_en_trans-lr-h7-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 14)\n",
            "   📥 Collecting result 14: Llama-2-7b-chat-hf-got_sp_en_trans-lr-h9-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 15)\n",
            "   📥 Collecting result 15: Llama-2-7b-chat-hf-got_sp_en_trans-lr-h11-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 16)\n",
            "   📥 Collecting result 16: Llama-2-7b-chat-hf-got_sp_en_trans-lr-h13-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 17)\n",
            "   📥 Collecting result 17: Llama-2-7b-chat-hf-got_sp_en_trans-lr-h15-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 18)\n",
            "   📥 Collecting result 18: Llama-2-7b-chat-hf-got_sp_en_trans-lr-h17-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 19)\n",
            "   📥 Collecting result 19: Llama-2-7b-chat-hf-got_sp_en_trans-lr-h19-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 20)\n",
            "   📥 Collecting result 20: Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h1-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 21)\n",
            "   📥 Collecting result 21: Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h3-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 22)\n",
            "   📥 Collecting result 22: Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h5-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 23)\n",
            "   📥 Collecting result 23: Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h7-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 24)\n",
            "   📥 Collecting result 24: Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h9-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 25)\n",
            "   📥 Collecting result 25: Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h11-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 26)\n",
            "   📥 Collecting result 26: Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h13-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 27)\n",
            "   📥 Collecting result 27: Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h15-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 28)\n",
            "   📥 Collecting result 28: Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h17-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 29)\n",
            "   📥 Collecting result 29: Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h19-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 30)\n",
            "   📥 Collecting result 30: Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h1-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 31)\n",
            "   📥 Collecting result 31: Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h3-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 32)\n",
            "   📥 Collecting result 32: Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h5-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 33)\n",
            "   📥 Collecting result 33: Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h7-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 34)\n",
            "   📥 Collecting result 34: Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h9-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 35)\n",
            "   📥 Collecting result 35: Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h11-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 36)\n",
            "   📥 Collecting result 36: Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h13-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 37)\n",
            "   📥 Collecting result 37: Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h15-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 38)\n",
            "   📥 Collecting result 38: Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h17-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 39)\n",
            "   📥 Collecting result 39: Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h19-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 40)\n",
            "   📥 Collecting result 40: Llama-2-7b-chat-hf-got_larger_than-lr-h1-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 41)\n",
            "   📥 Collecting result 41: Llama-2-7b-chat-hf-got_larger_than-lr-h3-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 42)\n",
            "   📥 Collecting result 42: Llama-2-7b-chat-hf-got_larger_than-lr-h5-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 43)\n",
            "   📥 Collecting result 43: Llama-2-7b-chat-hf-got_larger_than-lr-h7-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 44)\n",
            "   📥 Collecting result 44: Llama-2-7b-chat-hf-got_larger_than-lr-h9-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 45)\n",
            "   📥 Collecting result 45: Llama-2-7b-chat-hf-got_larger_than-lr-h11-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 46)\n",
            "   📥 Collecting result 46: Llama-2-7b-chat-hf-got_larger_than-lr-h13-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 47)\n",
            "   📥 Collecting result 47: Llama-2-7b-chat-hf-got_larger_than-lr-h15-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 48)\n",
            "   📥 Collecting result 48: Llama-2-7b-chat-hf-got_larger_than-lr-h17-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 49)\n",
            "   📥 Collecting result 49: Llama-2-7b-chat-hf-got_larger_than-lr-h19-0\n",
            "      → Got result type: <class 'tuple'>, non-None: True\n",
            "      ✅ Added to all_results (total: 50)\n",
            "   📊 Collection summary:\n",
            "      - Successful results collected: 50\n",
            "      - Failed results: 0\n",
            "      - all_results length: 50\n",
            "\n",
            "🔍 DICTIONARY CONVERSION DEBUG:\n",
            "   - all_results length: 50\n",
            "   - First result type: <class 'tuple'>\n",
            "   - First result: key='Llama-2-7b-chat-hf-got_cities-lr-h1-0', trained_probe_type=<class 'probeeng.probes.trained_probe.TrainedProbe'>\n",
            "   - Successfully created probes dict with 50 items\n",
            "   - First probe key: Llama-2-7b-chat-hf-got_cities-lr-h1-0\n",
            "   - First probe type: <class 'probeeng.probes.trained_probe.TrainedProbe'>\n",
            "\n",
            "Successfully trained 50 out of 50 probes\n",
            "Success rate by method:\n",
            "   7b: 50/50 (100.0%)\n",
            "\n",
            "Successfully trained 50 probes\n",
            "Probes saved to: output/probes/alina_mvp_train\n",
            "\n",
            "Successfully trained 50 probes\n",
            "Probes by method:\n",
            "   7b: 50 probes\n",
            "\n",
            "Converting to unified predictors...\n",
            "\n",
            "Manifest saved to: output/probes/alina_mvp_train/manifest.json\n",
            "Manifest saved to output/probes/alina_mvp_train/manifest.json\n",
            "\n",
            "Uploading probes to S3...\n",
            "Saved 50 probes to S3: s3://probeengbucket/experiments/alina_mvp_train/probes/\n",
            "Saved 50 probes locally to: output/probes/alina_mvp_train\n",
            "✅ Probes uploaded to: s3://probeengbucket/experiments/alina_mvp_train/probes/\n",
            "Stage probe_training completed successfully\n",
            "probe_training completed in 4.51s\n",
            "Stage 3/3: probe_evaluation:  67% 2/3 [00:17<00:07,  7.81s/stage]Executing stage: probe_evaluation\n",
            "\n",
            "Stage 3/3: probe_evaluation\n",
            "\n",
            "Evaluating probes with tag: alina_mvp_eval\n",
            "Evaluating probes with tag: alina_mvp_eval\n",
            "Loading activation dataset from s3://probeengbucket/experiments/alina_mvp_dataset/datasets/activations.pickle\n",
            "Loading activation dataset from s3://probeengbucket/experiments/alina_mvp_dataset/datasets/activations.pickle\n",
            "Downloading activation dataset from S3: s3://probeengbucket/experiments/alina_mvp_dataset/datasets/activations.pickle\n",
            "\n",
            "s3://probeengbucket/experiments/alina_mvp_dataset/datasets/activations.pickle load: 0it [00:00, ?it/s]\n",
            "Error downloading from S3: Stage s3://probeengbucket/experiments/alina_mvp_dataset/datasets/activations.pickle does not exist\n",
            "Falling back to direct S3 access...\n",
            "Downloading s3://probeengbucket/experiments/alina_mvp_dataset/datasets/activations.pickle to /tmp/tmpeitl3yk7.pickle\n",
            "Loaded 2246 activation results\n",
            "  LLMs: Llama-2-7b-chat-hf\n",
            "  Datasets: 5 unique datasets\n",
            "  Splits: test, train, validation\n",
            "Dataset loaded with 1 models and 5 datasets\n",
            "Output directory: output/evaluations/alina_mvp_eval\n",
            "Evaluating 50 trained probes\n",
            "Evaluation datasets: 5 datasets\n",
            "Splits: validation\n",
            "Total evaluations: 250\n",
            "Will generate plots\n",
            "Will calculate recovered accuracy\n",
            "Starting evaluation of 50 predictors\n",
            "\n",
            "Preparing predictors for evaluation...\n",
            "\n",
            "Preparing trained predictors for evaluation: 100% 50/50 [00:00<00:00, 738433.80predictors/s]\n",
            "Prepared 50 probe predictors and 0 ensemble predictors for evaluation\n",
            "\n",
            "Starting evaluation...\n",
            "\n",
            "Evaluating 50 individual probes...\n",
            "\n",
            "Preset: probe_evaluation\n",
            "\n",
            "Validating activation dataset compatibility...\n",
            "\n",
            "Validation passed. Available:\n",
            "  LLMs: 1\n",
            "  Layers: 10\n",
            "  Datasets: 5\n",
            "  Splits: ['validation', 'test', 'train']\n",
            "\n",
            "📊 Evaluation configuration:\n",
            "🧠 Probes to evaluate: 50\n",
            "📚 Evaluation datasets: 5\n",
            "   1. got_cities\n",
            "   2. got_sp_en_trans\n",
            "   3. got_cities_cities_conj\n",
            "   4. got_cities_cities_disj\n",
            "   5. got_larger_than\n",
            "📋 Splits: ['validation']\n",
            "🔢 Total evaluations: 250\n",
            "\n",
            "🚀 Starting evaluation...\n",
            "\n",
            "Evaluating probes on datasets:   0% 0/250 [00:00<?, ?tests/s]\u001b[A\n",
            "Testing 7b probe at layer chat:   0% 0/250 [00:00<?, ?tests/s]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h1-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=1, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h1\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:   0% 1/250 [00:00<00:03, 66.04tests/s, dataset=got_cities, split=validation, passed=1]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h1-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=1, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h1\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:   1% 2/250 [00:00<00:03, 73.80tests/s, dataset=got_sp_en_tr..., split=validation, passed=2]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h1-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=1, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h1\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:   1% 3/250 [00:00<00:04, 56.98tests/s, dataset=got_cities_c..., split=validation, passed=3]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h1-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=1, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h1\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:   2% 4/250 [00:00<00:04, 51.95tests/s, dataset=got_cities_c..., split=validation, passed=4]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h1-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=1, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h1\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:   2% 5/250 [00:00<00:04, 54.40tests/s, dataset=got_larger_t..., split=validation, passed=5]\u001b[A\n",
            "Testing 7b probe at layer chat:   2% 5/250 [00:00<00:04, 54.32tests/s, dataset=got_larger_t..., split=validation, passed=5]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h3-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=3, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h3\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:   2% 6/250 [00:00<00:04, 56.57tests/s, dataset=got_larger_t..., split=validation, passed=5]\u001b[A\n",
            "Testing 7b probe at layer chat:   2% 6/250 [00:00<00:04, 56.57tests/s, dataset=got_cities, split=validation, passed=6]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h3-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=3, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h3\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:   3% 7/250 [00:00<00:04, 56.57tests/s, dataset=got_sp_en_tr..., split=validation, passed=7]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h3-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=3, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h3\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:   3% 8/250 [00:00<00:04, 56.57tests/s, dataset=got_cities_c..., split=validation, passed=8]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h3-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=3, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h3\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:   4% 9/250 [00:00<00:04, 56.57tests/s, dataset=got_cities_c..., split=validation, passed=9]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h3-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=3, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h3\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:   4% 10/250 [00:00<00:04, 56.57tests/s, dataset=got_larger_t..., split=validation, passed=10]\u001b[A\n",
            "Testing 7b probe at layer chat:   4% 10/250 [00:00<00:04, 56.57tests/s, dataset=got_larger_t..., split=validation, passed=10]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h5-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=5, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h5\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:   4% 11/250 [00:00<00:04, 56.57tests/s, dataset=got_cities, split=validation, passed=11]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h5-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=5, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h5\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:   5% 12/250 [00:00<00:04, 57.40tests/s, dataset=got_cities, split=validation, passed=11]\u001b[A\n",
            "Testing 7b probe at layer chat:   5% 12/250 [00:00<00:04, 57.40tests/s, dataset=got_sp_en_tr..., split=validation, passed=12]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h5-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=5, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h5\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:   5% 13/250 [00:00<00:04, 57.40tests/s, dataset=got_cities_c..., split=validation, passed=13]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h5-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=5, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h5\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:   6% 14/250 [00:00<00:04, 57.40tests/s, dataset=got_cities_c..., split=validation, passed=14]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h5-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=5, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h5\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:   6% 15/250 [00:00<00:04, 57.40tests/s, dataset=got_larger_t..., split=validation, passed=15]\u001b[A\n",
            "Testing 7b probe at layer chat:   6% 15/250 [00:00<00:04, 57.40tests/s, dataset=got_larger_t..., split=validation, passed=15]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h7-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=7, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h7\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:   6% 16/250 [00:00<00:04, 57.40tests/s, dataset=got_cities, split=validation, passed=16]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h7-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=7, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h7\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:   7% 17/250 [00:00<00:04, 57.40tests/s, dataset=got_sp_en_tr..., split=validation, passed=17]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h7-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=7, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h7\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:   7% 18/250 [00:00<00:04, 54.48tests/s, dataset=got_sp_en_tr..., split=validation, passed=17]\u001b[A\n",
            "Testing 7b probe at layer chat:   7% 18/250 [00:00<00:04, 54.48tests/s, dataset=got_cities_c..., split=validation, passed=18]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h7-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=7, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h7\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:   8% 19/250 [00:00<00:04, 54.48tests/s, dataset=got_cities_c..., split=validation, passed=19]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h7-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=7, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h7\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:   8% 20/250 [00:00<00:04, 54.48tests/s, dataset=got_larger_t..., split=validation, passed=20]\u001b[A\n",
            "Testing 7b probe at layer chat:   8% 20/250 [00:00<00:04, 54.48tests/s, dataset=got_larger_t..., split=validation, passed=20]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h9-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=9, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h9\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:   8% 21/250 [00:00<00:04, 54.48tests/s, dataset=got_cities, split=validation, passed=21]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h9-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=9, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h9\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:   9% 22/250 [00:00<00:04, 54.48tests/s, dataset=got_sp_en_tr..., split=validation, passed=22]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h9-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=9, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h9\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:   9% 23/250 [00:00<00:04, 54.48tests/s, dataset=got_cities_c..., split=validation, passed=23]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h9-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=9, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h9\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  10% 24/250 [00:00<00:04, 53.10tests/s, dataset=got_cities_c..., split=validation, passed=23]\u001b[A\n",
            "Testing 7b probe at layer chat:  10% 24/250 [00:00<00:04, 53.10tests/s, dataset=got_cities_c..., split=validation, passed=24]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h9-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=9, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h9\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  10% 25/250 [00:00<00:04, 53.10tests/s, dataset=got_larger_t..., split=validation, passed=25]\u001b[A\n",
            "Testing 7b probe at layer chat:  10% 25/250 [00:00<00:04, 53.10tests/s, dataset=got_larger_t..., split=validation, passed=25]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h11-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=11, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h11\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  10% 26/250 [00:00<00:04, 53.10tests/s, dataset=got_cities, split=validation, passed=26]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h11-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=11, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h11\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  11% 27/250 [00:00<00:04, 53.10tests/s, dataset=got_sp_en_tr..., split=validation, passed=27]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h11-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=11, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h11\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  11% 28/250 [00:00<00:04, 53.10tests/s, dataset=got_cities_c..., split=validation, passed=28]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h11-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=11, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h11\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  12% 29/250 [00:00<00:04, 53.10tests/s, dataset=got_cities_c..., split=validation, passed=29]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h11-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=11, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h11\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  12% 30/250 [00:00<00:04, 53.86tests/s, dataset=got_cities_c..., split=validation, passed=29]\u001b[A\n",
            "Testing 7b probe at layer chat:  12% 30/250 [00:00<00:04, 53.86tests/s, dataset=got_larger_t..., split=validation, passed=30]\u001b[A\n",
            "Testing 7b probe at layer chat:  12% 30/250 [00:00<00:04, 53.86tests/s, dataset=got_larger_t..., split=validation, passed=30]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h13-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=13, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h13\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  12% 31/250 [00:00<00:04, 53.86tests/s, dataset=got_cities, split=validation, passed=31]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h13-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=13, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h13\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  13% 32/250 [00:00<00:04, 53.86tests/s, dataset=got_sp_en_tr..., split=validation, passed=32]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h13-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=13, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h13\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  13% 33/250 [00:00<00:04, 53.86tests/s, dataset=got_cities_c..., split=validation, passed=33]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h13-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=13, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h13\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  14% 34/250 [00:00<00:04, 53.86tests/s, dataset=got_cities_c..., split=validation, passed=34]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h13-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=13, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h13\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  14% 35/250 [00:00<00:03, 53.86tests/s, dataset=got_larger_t..., split=validation, passed=35]\u001b[A\n",
            "Testing 7b probe at layer chat:  14% 35/250 [00:00<00:03, 53.86tests/s, dataset=got_larger_t..., split=validation, passed=35]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h15-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=15, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h15\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  14% 36/250 [00:00<00:03, 53.54tests/s, dataset=got_larger_t..., split=validation, passed=35]\u001b[A\n",
            "Testing 7b probe at layer chat:  14% 36/250 [00:00<00:03, 53.54tests/s, dataset=got_cities, split=validation, passed=36]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h15-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=15, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h15\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  15% 37/250 [00:00<00:03, 53.54tests/s, dataset=got_sp_en_tr..., split=validation, passed=37]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h15-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=15, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h15\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  15% 38/250 [00:00<00:03, 53.54tests/s, dataset=got_cities_c..., split=validation, passed=38]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h15-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=15, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h15\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  16% 39/250 [00:00<00:03, 53.54tests/s, dataset=got_cities_c..., split=validation, passed=39]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h15-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=15, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h15\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  16% 40/250 [00:00<00:03, 53.54tests/s, dataset=got_larger_t..., split=validation, passed=40]\u001b[A\n",
            "Testing 7b probe at layer chat:  16% 40/250 [00:00<00:03, 53.54tests/s, dataset=got_larger_t..., split=validation, passed=40]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h17-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=17, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h17\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  16% 41/250 [00:00<00:03, 53.54tests/s, dataset=got_cities, split=validation, passed=41]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h17-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=17, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h17\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  17% 42/250 [00:00<00:03, 54.51tests/s, dataset=got_cities, split=validation, passed=41]\u001b[A\n",
            "Testing 7b probe at layer chat:  17% 42/250 [00:00<00:03, 54.51tests/s, dataset=got_sp_en_tr..., split=validation, passed=42]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h17-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=17, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h17\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  17% 43/250 [00:00<00:03, 54.51tests/s, dataset=got_cities_c..., split=validation, passed=43]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h17-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=17, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h17\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  18% 44/250 [00:00<00:03, 54.51tests/s, dataset=got_cities_c..., split=validation, passed=44]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h17-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=17, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h17\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  18% 45/250 [00:00<00:03, 54.51tests/s, dataset=got_larger_t..., split=validation, passed=45]\u001b[A\n",
            "Testing 7b probe at layer chat:  18% 45/250 [00:00<00:03, 54.51tests/s, dataset=got_larger_t..., split=validation, passed=45]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h19-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=19, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h19\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  18% 46/250 [00:00<00:03, 54.51tests/s, dataset=got_cities, split=validation, passed=46]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h19-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=19, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h19\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  19% 47/250 [00:00<00:03, 54.51tests/s, dataset=got_sp_en_tr..., split=validation, passed=47]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h19-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=19, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h19\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  19% 48/250 [00:00<00:03, 53.34tests/s, dataset=got_sp_en_tr..., split=validation, passed=47]\u001b[A\n",
            "Testing 7b probe at layer chat:  19% 48/250 [00:00<00:03, 53.34tests/s, dataset=got_cities_c..., split=validation, passed=48]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h19-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=19, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h19\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  20% 49/250 [00:00<00:03, 53.34tests/s, dataset=got_cities_c..., split=validation, passed=49]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities-lr-h19-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=19, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h19\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  20% 50/250 [00:00<00:03, 53.34tests/s, dataset=got_larger_t..., split=validation, passed=50]\u001b[A\n",
            "Testing 7b probe at layer chat:  20% 50/250 [00:00<00:03, 53.34tests/s, dataset=got_larger_t..., split=validation, passed=50]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h1-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=1, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h1\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  20% 51/250 [00:00<00:03, 53.34tests/s, dataset=got_cities, split=validation, passed=51]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h1-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=1, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h1\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  21% 52/250 [00:00<00:03, 53.34tests/s, dataset=got_sp_en_tr..., split=validation, passed=52]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h1-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=1, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h1\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  21% 53/250 [00:00<00:03, 53.34tests/s, dataset=got_cities_c..., split=validation, passed=53]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h1-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=1, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h1\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  22% 54/250 [00:01<00:03, 52.22tests/s, dataset=got_cities_c..., split=validation, passed=53]\u001b[A\n",
            "Testing 7b probe at layer chat:  22% 54/250 [00:01<00:03, 52.22tests/s, dataset=got_cities_c..., split=validation, passed=54]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h1-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=1, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h1\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  22% 55/250 [00:01<00:03, 52.22tests/s, dataset=got_larger_t..., split=validation, passed=55]\u001b[A\n",
            "Testing 7b probe at layer chat:  22% 55/250 [00:01<00:03, 52.22tests/s, dataset=got_larger_t..., split=validation, passed=55]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h3-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=3, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h3\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  22% 56/250 [00:01<00:03, 52.22tests/s, dataset=got_cities, split=validation, passed=56]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h3-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=3, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h3\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  23% 57/250 [00:01<00:03, 52.22tests/s, dataset=got_sp_en_tr..., split=validation, passed=57]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h3-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=3, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h3\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  23% 58/250 [00:01<00:03, 52.22tests/s, dataset=got_cities_c..., split=validation, passed=58]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h3-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=3, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h3\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  24% 59/250 [00:01<00:03, 52.22tests/s, dataset=got_cities_c..., split=validation, passed=59]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h3-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=3, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h3\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  24% 60/250 [00:01<00:03, 52.91tests/s, dataset=got_cities_c..., split=validation, passed=59]\u001b[A\n",
            "Testing 7b probe at layer chat:  24% 60/250 [00:01<00:03, 52.91tests/s, dataset=got_larger_t..., split=validation, passed=60]\u001b[A\n",
            "Testing 7b probe at layer chat:  24% 60/250 [00:01<00:03, 52.91tests/s, dataset=got_larger_t..., split=validation, passed=60]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h5-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=5, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h5\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  24% 61/250 [00:01<00:03, 52.91tests/s, dataset=got_cities, split=validation, passed=61]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h5-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=5, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h5\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  25% 62/250 [00:01<00:03, 52.91tests/s, dataset=got_sp_en_tr..., split=validation, passed=62]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h5-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=5, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h5\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  25% 63/250 [00:01<00:03, 52.91tests/s, dataset=got_cities_c..., split=validation, passed=63]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h5-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=5, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h5\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  26% 64/250 [00:01<00:03, 52.91tests/s, dataset=got_cities_c..., split=validation, passed=64]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h5-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=5, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h5\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  26% 65/250 [00:01<00:03, 52.91tests/s, dataset=got_larger_t..., split=validation, passed=65]\u001b[A\n",
            "Testing 7b probe at layer chat:  26% 65/250 [00:01<00:03, 52.91tests/s, dataset=got_larger_t..., split=validation, passed=65]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h7-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=7, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h7\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  26% 66/250 [00:01<00:03, 52.45tests/s, dataset=got_larger_t..., split=validation, passed=65]\u001b[A\n",
            "Testing 7b probe at layer chat:  26% 66/250 [00:01<00:03, 52.45tests/s, dataset=got_cities, split=validation, passed=66]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h7-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=7, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h7\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  27% 67/250 [00:01<00:03, 52.45tests/s, dataset=got_sp_en_tr..., split=validation, passed=67]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h7-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=7, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h7\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  27% 68/250 [00:01<00:03, 52.45tests/s, dataset=got_cities_c..., split=validation, passed=68]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h7-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=7, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h7\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  28% 69/250 [00:01<00:03, 52.45tests/s, dataset=got_cities_c..., split=validation, passed=69]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h7-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=7, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h7\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  28% 70/250 [00:01<00:03, 52.45tests/s, dataset=got_larger_t..., split=validation, passed=70]\u001b[A\n",
            "Testing 7b probe at layer chat:  28% 70/250 [00:01<00:03, 52.45tests/s, dataset=got_larger_t..., split=validation, passed=70]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h9-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=9, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h9\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  28% 71/250 [00:01<00:03, 52.45tests/s, dataset=got_cities, split=validation, passed=71]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h9-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=9, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h9\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  29% 72/250 [00:01<00:03, 53.47tests/s, dataset=got_cities, split=validation, passed=71]\u001b[A\n",
            "Testing 7b probe at layer chat:  29% 72/250 [00:01<00:03, 53.47tests/s, dataset=got_sp_en_tr..., split=validation, passed=72]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h9-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=9, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h9\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  29% 73/250 [00:01<00:03, 53.47tests/s, dataset=got_cities_c..., split=validation, passed=73]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h9-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=9, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h9\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  30% 74/250 [00:01<00:03, 53.47tests/s, dataset=got_cities_c..., split=validation, passed=74]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h9-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=9, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h9\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  30% 75/250 [00:01<00:03, 53.47tests/s, dataset=got_larger_t..., split=validation, passed=75]\u001b[A\n",
            "Testing 7b probe at layer chat:  30% 75/250 [00:01<00:03, 53.47tests/s, dataset=got_larger_t..., split=validation, passed=75]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h11-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=11, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h11\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  30% 76/250 [00:01<00:03, 53.47tests/s, dataset=got_cities, split=validation, passed=76]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h11-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=11, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h11\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  31% 77/250 [00:01<00:03, 53.47tests/s, dataset=got_sp_en_tr..., split=validation, passed=77]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h11-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=11, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h11\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  31% 78/250 [00:01<00:03, 52.45tests/s, dataset=got_sp_en_tr..., split=validation, passed=77]\u001b[A\n",
            "Testing 7b probe at layer chat:  31% 78/250 [00:01<00:03, 52.45tests/s, dataset=got_cities_c..., split=validation, passed=78]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h11-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=11, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h11\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  32% 79/250 [00:01<00:03, 52.45tests/s, dataset=got_cities_c..., split=validation, passed=79]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h11-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=11, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h11\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  32% 80/250 [00:01<00:03, 52.45tests/s, dataset=got_larger_t..., split=validation, passed=80]\u001b[A\n",
            "Testing 7b probe at layer chat:  32% 80/250 [00:01<00:03, 52.45tests/s, dataset=got_larger_t..., split=validation, passed=80]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h13-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=13, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h13\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  32% 81/250 [00:01<00:03, 52.45tests/s, dataset=got_cities, split=validation, passed=81]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h13-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=13, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h13\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  33% 82/250 [00:01<00:03, 52.45tests/s, dataset=got_sp_en_tr..., split=validation, passed=82]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h13-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=13, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h13\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  33% 83/250 [00:01<00:03, 52.45tests/s, dataset=got_cities_c..., split=validation, passed=83]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h13-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=13, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h13\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  34% 84/250 [00:01<00:03, 51.63tests/s, dataset=got_cities_c..., split=validation, passed=83]\u001b[A\n",
            "Testing 7b probe at layer chat:  34% 84/250 [00:01<00:03, 51.63tests/s, dataset=got_cities_c..., split=validation, passed=84]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h13-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=13, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h13\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  34% 85/250 [00:01<00:03, 51.63tests/s, dataset=got_larger_t..., split=validation, passed=85]\u001b[A\n",
            "Testing 7b probe at layer chat:  34% 85/250 [00:01<00:03, 51.63tests/s, dataset=got_larger_t..., split=validation, passed=85]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h15-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=15, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h15\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  34% 86/250 [00:01<00:03, 51.63tests/s, dataset=got_cities, split=validation, passed=86]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h15-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=15, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h15\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  35% 87/250 [00:01<00:03, 51.63tests/s, dataset=got_sp_en_tr..., split=validation, passed=87]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h15-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=15, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h15\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  35% 88/250 [00:01<00:03, 51.63tests/s, dataset=got_cities_c..., split=validation, passed=88]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h15-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=15, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h15\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  36% 89/250 [00:01<00:03, 51.63tests/s, dataset=got_cities_c..., split=validation, passed=89]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h15-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=15, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h15\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  36% 90/250 [00:01<00:03, 51.91tests/s, dataset=got_cities_c..., split=validation, passed=89]\u001b[A\n",
            "Testing 7b probe at layer chat:  36% 90/250 [00:01<00:03, 51.91tests/s, dataset=got_larger_t..., split=validation, passed=90]\u001b[A\n",
            "Testing 7b probe at layer chat:  36% 90/250 [00:01<00:03, 51.91tests/s, dataset=got_larger_t..., split=validation, passed=90]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h17-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=17, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h17\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  36% 91/250 [00:01<00:03, 51.91tests/s, dataset=got_cities, split=validation, passed=91]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h17-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=17, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h17\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  37% 92/250 [00:01<00:03, 51.91tests/s, dataset=got_sp_en_tr..., split=validation, passed=92]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h17-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=17, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h17\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  37% 93/250 [00:01<00:03, 51.91tests/s, dataset=got_cities_c..., split=validation, passed=93]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h17-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=17, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h17\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  38% 94/250 [00:01<00:03, 51.91tests/s, dataset=got_cities_c..., split=validation, passed=94]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h17-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=17, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h17\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  38% 95/250 [00:01<00:02, 51.91tests/s, dataset=got_larger_t..., split=validation, passed=95]\u001b[A\n",
            "Testing 7b probe at layer chat:  38% 95/250 [00:01<00:02, 51.91tests/s, dataset=got_larger_t..., split=validation, passed=95]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h19-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=19, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h19\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  38% 96/250 [00:01<00:02, 52.15tests/s, dataset=got_larger_t..., split=validation, passed=95]\u001b[A\n",
            "Testing 7b probe at layer chat:  38% 96/250 [00:01<00:02, 52.15tests/s, dataset=got_cities, split=validation, passed=96]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h19-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=19, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h19\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  39% 97/250 [00:01<00:02, 52.15tests/s, dataset=got_sp_en_tr..., split=validation, passed=97]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h19-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=19, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h19\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  39% 98/250 [00:01<00:02, 52.15tests/s, dataset=got_cities_c..., split=validation, passed=98]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h19-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=19, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h19\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  40% 99/250 [00:01<00:02, 52.15tests/s, dataset=got_cities_c..., split=validation, passed=99]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_sp_en_trans-lr-h19-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=19, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h19\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  40% 100/250 [00:01<00:02, 52.15tests/s, dataset=got_larger_t..., split=validation, passed=100]\u001b[A\n",
            "Testing 7b probe at layer chat:  40% 100/250 [00:01<00:02, 52.15tests/s, dataset=got_larger_t..., split=validation, passed=100]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h1-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=1, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h1\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  40% 101/250 [00:01<00:02, 52.15tests/s, dataset=got_cities, split=validation, passed=101]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h1-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=1, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h1\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  41% 102/250 [00:01<00:02, 53.41tests/s, dataset=got_cities, split=validation, passed=101]\u001b[A\n",
            "Testing 7b probe at layer chat:  41% 102/250 [00:01<00:02, 53.41tests/s, dataset=got_sp_en_tr..., split=validation, passed=102]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h1-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=1, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h1\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  41% 103/250 [00:01<00:02, 53.41tests/s, dataset=got_cities_c..., split=validation, passed=103]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h1-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=1, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h1\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  42% 104/250 [00:01<00:02, 53.41tests/s, dataset=got_cities_c..., split=validation, passed=104]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h1-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=1, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h1\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  42% 105/250 [00:01<00:02, 53.41tests/s, dataset=got_larger_t..., split=validation, passed=105]\u001b[A\n",
            "Testing 7b probe at layer chat:  42% 105/250 [00:01<00:02, 53.41tests/s, dataset=got_larger_t..., split=validation, passed=105]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h3-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=3, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h3\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  42% 106/250 [00:01<00:02, 53.41tests/s, dataset=got_cities, split=validation, passed=106]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h3-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=3, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h3\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  43% 107/250 [00:02<00:02, 53.41tests/s, dataset=got_sp_en_tr..., split=validation, passed=107]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h3-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=3, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h3\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  43% 108/250 [00:02<00:02, 52.63tests/s, dataset=got_sp_en_tr..., split=validation, passed=107]\u001b[A\n",
            "Testing 7b probe at layer chat:  43% 108/250 [00:02<00:02, 52.63tests/s, dataset=got_cities_c..., split=validation, passed=108]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h3-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=3, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h3\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  44% 109/250 [00:02<00:02, 52.63tests/s, dataset=got_cities_c..., split=validation, passed=109]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h3-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=3, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h3\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  44% 110/250 [00:02<00:02, 52.63tests/s, dataset=got_larger_t..., split=validation, passed=110]\u001b[A\n",
            "Testing 7b probe at layer chat:  44% 110/250 [00:02<00:02, 52.63tests/s, dataset=got_larger_t..., split=validation, passed=110]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h5-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=5, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h5\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  44% 111/250 [00:02<00:02, 52.63tests/s, dataset=got_cities, split=validation, passed=111]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h5-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=5, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h5\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  45% 112/250 [00:02<00:02, 52.63tests/s, dataset=got_sp_en_tr..., split=validation, passed=112]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h5-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=5, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h5\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  45% 113/250 [00:02<00:02, 52.63tests/s, dataset=got_cities_c..., split=validation, passed=113]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h5-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=5, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h5\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  46% 114/250 [00:02<00:02, 52.11tests/s, dataset=got_cities_c..., split=validation, passed=113]\u001b[A\n",
            "Testing 7b probe at layer chat:  46% 114/250 [00:02<00:02, 52.11tests/s, dataset=got_cities_c..., split=validation, passed=114]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h5-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=5, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h5\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  46% 115/250 [00:02<00:02, 52.11tests/s, dataset=got_larger_t..., split=validation, passed=115]\u001b[A\n",
            "Testing 7b probe at layer chat:  46% 115/250 [00:02<00:02, 52.11tests/s, dataset=got_larger_t..., split=validation, passed=115]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h7-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=7, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h7\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  46% 116/250 [00:02<00:02, 52.11tests/s, dataset=got_cities, split=validation, passed=116]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h7-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=7, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h7\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  47% 117/250 [00:02<00:02, 52.11tests/s, dataset=got_sp_en_tr..., split=validation, passed=117]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h7-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=7, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h7\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  47% 118/250 [00:02<00:02, 52.11tests/s, dataset=got_cities_c..., split=validation, passed=118]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h7-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=7, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h7\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  48% 119/250 [00:02<00:02, 52.11tests/s, dataset=got_cities_c..., split=validation, passed=119]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h7-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=7, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h7\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  48% 120/250 [00:02<00:02, 54.04tests/s, dataset=got_cities_c..., split=validation, passed=119]\u001b[A\n",
            "Testing 7b probe at layer chat:  48% 120/250 [00:02<00:02, 54.04tests/s, dataset=got_larger_t..., split=validation, passed=120]\u001b[A\n",
            "Testing 7b probe at layer chat:  48% 120/250 [00:02<00:02, 54.04tests/s, dataset=got_larger_t..., split=validation, passed=120]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h9-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=9, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h9\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  48% 121/250 [00:02<00:02, 54.04tests/s, dataset=got_cities, split=validation, passed=121]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h9-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=9, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h9\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  49% 122/250 [00:02<00:02, 54.04tests/s, dataset=got_sp_en_tr..., split=validation, passed=122]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h9-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=9, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h9\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  49% 123/250 [00:02<00:02, 54.04tests/s, dataset=got_cities_c..., split=validation, passed=123]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h9-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=9, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h9\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  50% 124/250 [00:02<00:02, 54.04tests/s, dataset=got_cities_c..., split=validation, passed=124]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h9-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=9, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h9\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  50% 125/250 [00:02<00:02, 54.04tests/s, dataset=got_larger_t..., split=validation, passed=125]\u001b[A\n",
            "Testing 7b probe at layer chat:  50% 125/250 [00:02<00:02, 54.04tests/s, dataset=got_larger_t..., split=validation, passed=125]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h11-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=11, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h11\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  50% 126/250 [00:02<00:02, 55.28tests/s, dataset=got_larger_t..., split=validation, passed=125]\u001b[A\n",
            "Testing 7b probe at layer chat:  50% 126/250 [00:02<00:02, 55.28tests/s, dataset=got_cities, split=validation, passed=126]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h11-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=11, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h11\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  51% 127/250 [00:02<00:02, 55.28tests/s, dataset=got_sp_en_tr..., split=validation, passed=127]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h11-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=11, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h11\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  51% 128/250 [00:02<00:02, 55.28tests/s, dataset=got_cities_c..., split=validation, passed=128]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h11-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=11, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h11\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  52% 129/250 [00:02<00:02, 55.28tests/s, dataset=got_cities_c..., split=validation, passed=129]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h11-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=11, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h11\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  52% 130/250 [00:02<00:02, 55.28tests/s, dataset=got_larger_t..., split=validation, passed=130]\u001b[A\n",
            "Testing 7b probe at layer chat:  52% 130/250 [00:02<00:02, 55.28tests/s, dataset=got_larger_t..., split=validation, passed=130]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h13-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=13, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h13\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  52% 131/250 [00:02<00:02, 55.28tests/s, dataset=got_cities, split=validation, passed=131]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h13-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=13, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h13\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  53% 132/250 [00:02<00:02, 55.28tests/s, dataset=got_sp_en_tr..., split=validation, passed=132]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h13-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=13, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h13\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  53% 133/250 [00:02<00:02, 55.68tests/s, dataset=got_sp_en_tr..., split=validation, passed=132]\u001b[A\n",
            "Testing 7b probe at layer chat:  53% 133/250 [00:02<00:02, 55.68tests/s, dataset=got_cities_c..., split=validation, passed=133]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h13-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=13, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h13\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  54% 134/250 [00:02<00:02, 55.68tests/s, dataset=got_cities_c..., split=validation, passed=134]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h13-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=13, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h13\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  54% 135/250 [00:02<00:02, 55.68tests/s, dataset=got_larger_t..., split=validation, passed=135]\u001b[A\n",
            "Testing 7b probe at layer chat:  54% 135/250 [00:02<00:02, 55.68tests/s, dataset=got_larger_t..., split=validation, passed=135]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h15-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=15, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h15\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  54% 136/250 [00:02<00:02, 55.68tests/s, dataset=got_cities, split=validation, passed=136]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h15-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=15, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h15\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  55% 137/250 [00:02<00:02, 55.68tests/s, dataset=got_sp_en_tr..., split=validation, passed=137]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h15-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=15, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h15\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  55% 138/250 [00:02<00:02, 55.68tests/s, dataset=got_cities_c..., split=validation, passed=138]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h15-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=15, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h15\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  56% 139/250 [00:02<00:02, 52.66tests/s, dataset=got_cities_c..., split=validation, passed=138]\u001b[A\n",
            "Testing 7b probe at layer chat:  56% 139/250 [00:02<00:02, 52.66tests/s, dataset=got_cities_c..., split=validation, passed=139]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h15-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=15, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h15\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  56% 140/250 [00:02<00:02, 52.66tests/s, dataset=got_larger_t..., split=validation, passed=140]\u001b[A\n",
            "Testing 7b probe at layer chat:  56% 140/250 [00:02<00:02, 52.66tests/s, dataset=got_larger_t..., split=validation, passed=140]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h17-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=17, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h17\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  56% 141/250 [00:02<00:02, 52.66tests/s, dataset=got_cities, split=validation, passed=141]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h17-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=17, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h17\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  57% 142/250 [00:02<00:02, 52.66tests/s, dataset=got_sp_en_tr..., split=validation, passed=142]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h17-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=17, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h17\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  57% 143/250 [00:02<00:02, 52.66tests/s, dataset=got_cities_c..., split=validation, passed=143]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h17-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=17, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h17\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  58% 144/250 [00:02<00:02, 52.66tests/s, dataset=got_cities_c..., split=validation, passed=144]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h17-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=17, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h17\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  58% 145/250 [00:02<00:01, 53.65tests/s, dataset=got_cities_c..., split=validation, passed=144]\u001b[A\n",
            "Testing 7b probe at layer chat:  58% 145/250 [00:02<00:01, 53.65tests/s, dataset=got_larger_t..., split=validation, passed=145]\u001b[A\n",
            "Testing 7b probe at layer chat:  58% 145/250 [00:02<00:01, 53.65tests/s, dataset=got_larger_t..., split=validation, passed=145]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h19-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=19, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h19\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  58% 146/250 [00:02<00:01, 53.65tests/s, dataset=got_cities, split=validation, passed=146]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h19-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=19, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h19\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  59% 147/250 [00:02<00:01, 53.65tests/s, dataset=got_sp_en_tr..., split=validation, passed=147]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h19-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=19, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h19\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  59% 148/250 [00:02<00:01, 53.65tests/s, dataset=got_cities_c..., split=validation, passed=148]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h19-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=19, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h19\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  60% 149/250 [00:02<00:01, 53.65tests/s, dataset=got_cities_c..., split=validation, passed=149]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_conj-lr-h19-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=19, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h19\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  60% 150/250 [00:02<00:01, 53.65tests/s, dataset=got_larger_t..., split=validation, passed=150]\u001b[A\n",
            "Testing 7b probe at layer chat:  60% 150/250 [00:02<00:01, 53.65tests/s, dataset=got_larger_t..., split=validation, passed=150]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h1-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=1, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h1\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  60% 151/250 [00:02<00:01, 54.43tests/s, dataset=got_larger_t..., split=validation, passed=150]\u001b[A\n",
            "Testing 7b probe at layer chat:  60% 151/250 [00:02<00:01, 54.43tests/s, dataset=got_cities, split=validation, passed=151]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h1-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=1, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h1\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  61% 152/250 [00:02<00:01, 54.43tests/s, dataset=got_sp_en_tr..., split=validation, passed=152]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h1-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=1, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h1\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  61% 153/250 [00:02<00:01, 54.43tests/s, dataset=got_cities_c..., split=validation, passed=153]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h1-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=1, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h1\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  62% 154/250 [00:02<00:01, 54.43tests/s, dataset=got_cities_c..., split=validation, passed=154]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h1-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=1, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h1\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  62% 155/250 [00:02<00:01, 54.43tests/s, dataset=got_larger_t..., split=validation, passed=155]\u001b[A\n",
            "Testing 7b probe at layer chat:  62% 155/250 [00:02<00:01, 54.43tests/s, dataset=got_larger_t..., split=validation, passed=155]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h3-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=3, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h3\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  62% 156/250 [00:02<00:01, 54.43tests/s, dataset=got_cities, split=validation, passed=156]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h3-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=3, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h3\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  63% 157/250 [00:02<00:01, 55.03tests/s, dataset=got_cities, split=validation, passed=156]\u001b[A\n",
            "Testing 7b probe at layer chat:  63% 157/250 [00:02<00:01, 55.03tests/s, dataset=got_sp_en_tr..., split=validation, passed=157]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h3-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=3, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h3\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  63% 158/250 [00:02<00:01, 55.03tests/s, dataset=got_cities_c..., split=validation, passed=158]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h3-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=3, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h3\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  64% 159/250 [00:02<00:01, 55.03tests/s, dataset=got_cities_c..., split=validation, passed=159]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h3-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=3, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h3\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  64% 160/250 [00:02<00:01, 55.03tests/s, dataset=got_larger_t..., split=validation, passed=160]\u001b[A\n",
            "Testing 7b probe at layer chat:  64% 160/250 [00:02<00:01, 55.03tests/s, dataset=got_larger_t..., split=validation, passed=160]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h5-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=5, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h5\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  64% 161/250 [00:03<00:01, 55.03tests/s, dataset=got_cities, split=validation, passed=161]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h5-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=5, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h5\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  65% 162/250 [00:03<00:01, 55.03tests/s, dataset=got_sp_en_tr..., split=validation, passed=162]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h5-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=5, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h5\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  65% 163/250 [00:03<00:01, 53.18tests/s, dataset=got_sp_en_tr..., split=validation, passed=162]\u001b[A\n",
            "Testing 7b probe at layer chat:  65% 163/250 [00:03<00:01, 53.18tests/s, dataset=got_cities_c..., split=validation, passed=163]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h5-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=5, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h5\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  66% 164/250 [00:03<00:01, 53.18tests/s, dataset=got_cities_c..., split=validation, passed=164]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h5-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=5, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h5\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  66% 165/250 [00:03<00:01, 53.18tests/s, dataset=got_larger_t..., split=validation, passed=165]\u001b[A\n",
            "Testing 7b probe at layer chat:  66% 165/250 [00:03<00:01, 53.18tests/s, dataset=got_larger_t..., split=validation, passed=165]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h7-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=7, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h7\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  66% 166/250 [00:03<00:01, 53.18tests/s, dataset=got_cities, split=validation, passed=166]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h7-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=7, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h7\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  67% 167/250 [00:03<00:01, 53.18tests/s, dataset=got_sp_en_tr..., split=validation, passed=167]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h7-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=7, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h7\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  67% 168/250 [00:03<00:01, 53.18tests/s, dataset=got_cities_c..., split=validation, passed=168]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h7-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=7, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h7\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  68% 169/250 [00:03<00:01, 52.72tests/s, dataset=got_cities_c..., split=validation, passed=168]\u001b[A\n",
            "Testing 7b probe at layer chat:  68% 169/250 [00:03<00:01, 52.72tests/s, dataset=got_cities_c..., split=validation, passed=169]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h7-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=7, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h7\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  68% 170/250 [00:03<00:01, 52.72tests/s, dataset=got_larger_t..., split=validation, passed=170]\u001b[A\n",
            "Testing 7b probe at layer chat:  68% 170/250 [00:03<00:01, 52.72tests/s, dataset=got_larger_t..., split=validation, passed=170]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h9-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=9, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h9\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  68% 171/250 [00:03<00:01, 52.72tests/s, dataset=got_cities, split=validation, passed=171]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h9-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=9, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h9\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  69% 172/250 [00:03<00:01, 52.72tests/s, dataset=got_sp_en_tr..., split=validation, passed=172]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h9-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=9, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h9\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  69% 173/250 [00:03<00:01, 52.72tests/s, dataset=got_cities_c..., split=validation, passed=173]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h9-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=9, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h9\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  70% 174/250 [00:03<00:01, 52.72tests/s, dataset=got_cities_c..., split=validation, passed=174]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h9-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=9, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h9\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  70% 175/250 [00:03<00:01, 54.09tests/s, dataset=got_cities_c..., split=validation, passed=174]\u001b[A\n",
            "Testing 7b probe at layer chat:  70% 175/250 [00:03<00:01, 54.09tests/s, dataset=got_larger_t..., split=validation, passed=175]\u001b[A\n",
            "Testing 7b probe at layer chat:  70% 175/250 [00:03<00:01, 54.09tests/s, dataset=got_larger_t..., split=validation, passed=175]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h11-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=11, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h11\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  70% 176/250 [00:03<00:01, 54.09tests/s, dataset=got_cities, split=validation, passed=176]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h11-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=11, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h11\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  71% 177/250 [00:03<00:01, 54.09tests/s, dataset=got_sp_en_tr..., split=validation, passed=177]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h11-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=11, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h11\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  71% 178/250 [00:03<00:01, 54.09tests/s, dataset=got_cities_c..., split=validation, passed=178]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h11-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=11, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h11\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  72% 179/250 [00:03<00:01, 54.09tests/s, dataset=got_cities_c..., split=validation, passed=179]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h11-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=11, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h11\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  72% 180/250 [00:03<00:01, 54.09tests/s, dataset=got_larger_t..., split=validation, passed=180]\u001b[A\n",
            "Testing 7b probe at layer chat:  72% 180/250 [00:03<00:01, 54.09tests/s, dataset=got_larger_t..., split=validation, passed=180]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h13-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=13, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h13\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  72% 181/250 [00:03<00:01, 55.20tests/s, dataset=got_larger_t..., split=validation, passed=180]\u001b[A\n",
            "Testing 7b probe at layer chat:  72% 181/250 [00:03<00:01, 55.20tests/s, dataset=got_cities, split=validation, passed=181]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h13-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=13, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h13\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  73% 182/250 [00:03<00:01, 55.20tests/s, dataset=got_sp_en_tr..., split=validation, passed=182]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h13-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=13, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h13\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  73% 183/250 [00:03<00:01, 55.20tests/s, dataset=got_cities_c..., split=validation, passed=183]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h13-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=13, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h13\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  74% 184/250 [00:03<00:01, 55.20tests/s, dataset=got_cities_c..., split=validation, passed=184]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h13-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=13, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h13\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  74% 185/250 [00:03<00:01, 55.20tests/s, dataset=got_larger_t..., split=validation, passed=185]\u001b[A\n",
            "Testing 7b probe at layer chat:  74% 185/250 [00:03<00:01, 55.20tests/s, dataset=got_larger_t..., split=validation, passed=185]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h15-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=15, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h15\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  74% 186/250 [00:03<00:01, 55.20tests/s, dataset=got_cities, split=validation, passed=186]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h15-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=15, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h15\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  75% 187/250 [00:03<00:01, 56.32tests/s, dataset=got_cities, split=validation, passed=186]\u001b[A\n",
            "Testing 7b probe at layer chat:  75% 187/250 [00:03<00:01, 56.32tests/s, dataset=got_sp_en_tr..., split=validation, passed=187]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h15-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=15, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h15\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  75% 188/250 [00:03<00:01, 56.32tests/s, dataset=got_cities_c..., split=validation, passed=188]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h15-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=15, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h15\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  76% 189/250 [00:03<00:01, 56.32tests/s, dataset=got_cities_c..., split=validation, passed=189]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h15-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=15, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h15\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  76% 190/250 [00:03<00:01, 56.32tests/s, dataset=got_larger_t..., split=validation, passed=190]\u001b[A\n",
            "Testing 7b probe at layer chat:  76% 190/250 [00:03<00:01, 56.32tests/s, dataset=got_larger_t..., split=validation, passed=190]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h17-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=17, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h17\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  76% 191/250 [00:03<00:01, 56.32tests/s, dataset=got_cities, split=validation, passed=191]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h17-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=17, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h17\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  77% 192/250 [00:03<00:01, 56.32tests/s, dataset=got_sp_en_tr..., split=validation, passed=192]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h17-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=17, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h17\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  77% 193/250 [00:03<00:01, 54.31tests/s, dataset=got_sp_en_tr..., split=validation, passed=192]\u001b[A\n",
            "Testing 7b probe at layer chat:  77% 193/250 [00:03<00:01, 54.31tests/s, dataset=got_cities_c..., split=validation, passed=193]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h17-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=17, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h17\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  78% 194/250 [00:03<00:01, 54.31tests/s, dataset=got_cities_c..., split=validation, passed=194]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h17-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=17, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h17\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  78% 195/250 [00:03<00:01, 54.31tests/s, dataset=got_larger_t..., split=validation, passed=195]\u001b[A\n",
            "Testing 7b probe at layer chat:  78% 195/250 [00:03<00:01, 54.31tests/s, dataset=got_larger_t..., split=validation, passed=195]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h19-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=19, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h19\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  78% 196/250 [00:03<00:00, 54.31tests/s, dataset=got_cities, split=validation, passed=196]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h19-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=19, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h19\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  79% 197/250 [00:03<00:00, 54.31tests/s, dataset=got_sp_en_tr..., split=validation, passed=197]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h19-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=19, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h19\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  79% 198/250 [00:03<00:00, 54.31tests/s, dataset=got_cities_c..., split=validation, passed=198]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h19-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=19, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h19\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  80% 199/250 [00:03<00:00, 54.28tests/s, dataset=got_cities_c..., split=validation, passed=198]\u001b[A\n",
            "Testing 7b probe at layer chat:  80% 199/250 [00:03<00:00, 54.28tests/s, dataset=got_cities_c..., split=validation, passed=199]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_cities_cities_disj-lr-h19-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=19, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h19\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  80% 200/250 [00:03<00:00, 54.28tests/s, dataset=got_larger_t..., split=validation, passed=200]\u001b[A\n",
            "Testing 7b probe at layer chat:  80% 200/250 [00:03<00:00, 54.28tests/s, dataset=got_larger_t..., split=validation, passed=200]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h1-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=1, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h1\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  80% 201/250 [00:03<00:00, 54.28tests/s, dataset=got_cities, split=validation, passed=201]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h1-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=1, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h1\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  81% 202/250 [00:03<00:00, 54.28tests/s, dataset=got_sp_en_tr..., split=validation, passed=202]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h1-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=1, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h1\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  81% 203/250 [00:03<00:00, 54.28tests/s, dataset=got_cities_c..., split=validation, passed=203]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h1-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=1, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h1\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  82% 204/250 [00:03<00:00, 54.28tests/s, dataset=got_cities_c..., split=validation, passed=204]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h1-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=1, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h1\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  82% 205/250 [00:03<00:00, 55.03tests/s, dataset=got_cities_c..., split=validation, passed=204]\u001b[A\n",
            "Testing 7b probe at layer chat:  82% 205/250 [00:03<00:00, 55.03tests/s, dataset=got_larger_t..., split=validation, passed=205]\u001b[A\n",
            "Testing 7b probe at layer chat:  82% 205/250 [00:03<00:00, 55.03tests/s, dataset=got_larger_t..., split=validation, passed=205]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h3-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=3, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h3\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  82% 206/250 [00:03<00:00, 55.03tests/s, dataset=got_cities, split=validation, passed=206]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h3-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=3, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h3\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  83% 207/250 [00:03<00:00, 55.03tests/s, dataset=got_sp_en_tr..., split=validation, passed=207]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h3-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=3, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h3\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  83% 208/250 [00:03<00:00, 55.03tests/s, dataset=got_cities_c..., split=validation, passed=208]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h3-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=3, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h3\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  84% 209/250 [00:03<00:00, 55.03tests/s, dataset=got_cities_c..., split=validation, passed=209]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h3-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=3, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h3\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  84% 210/250 [00:03<00:00, 55.03tests/s, dataset=got_larger_t..., split=validation, passed=210]\u001b[A\n",
            "Testing 7b probe at layer chat:  84% 210/250 [00:03<00:00, 55.03tests/s, dataset=got_larger_t..., split=validation, passed=210]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h5-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=5, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h5\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  84% 211/250 [00:03<00:00, 54.88tests/s, dataset=got_larger_t..., split=validation, passed=210]\u001b[A\n",
            "Testing 7b probe at layer chat:  84% 211/250 [00:03<00:00, 54.88tests/s, dataset=got_cities, split=validation, passed=211]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h5-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=5, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h5\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  85% 212/250 [00:03<00:00, 54.88tests/s, dataset=got_sp_en_tr..., split=validation, passed=212]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h5-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=5, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h5\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  85% 213/250 [00:03<00:00, 54.88tests/s, dataset=got_cities_c..., split=validation, passed=213]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h5-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=5, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h5\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  86% 214/250 [00:03<00:00, 54.88tests/s, dataset=got_cities_c..., split=validation, passed=214]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h5-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=5, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h5\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  86% 215/250 [00:03<00:00, 54.88tests/s, dataset=got_larger_t..., split=validation, passed=215]\u001b[A\n",
            "Testing 7b probe at layer chat:  86% 215/250 [00:03<00:00, 54.88tests/s, dataset=got_larger_t..., split=validation, passed=215]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h7-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=7, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h7\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  86% 216/250 [00:04<00:00, 54.88tests/s, dataset=got_cities, split=validation, passed=216]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h7-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=7, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h7\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  87% 217/250 [00:04<00:00, 55.38tests/s, dataset=got_cities, split=validation, passed=216]\u001b[A\n",
            "Testing 7b probe at layer chat:  87% 217/250 [00:04<00:00, 55.38tests/s, dataset=got_sp_en_tr..., split=validation, passed=217]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h7-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=7, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h7\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  87% 218/250 [00:04<00:00, 55.38tests/s, dataset=got_cities_c..., split=validation, passed=218]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h7-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=7, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h7\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  88% 219/250 [00:04<00:00, 55.38tests/s, dataset=got_cities_c..., split=validation, passed=219]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h7-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=7, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h7\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  88% 220/250 [00:04<00:00, 55.38tests/s, dataset=got_larger_t..., split=validation, passed=220]\u001b[A\n",
            "Testing 7b probe at layer chat:  88% 220/250 [00:04<00:00, 55.38tests/s, dataset=got_larger_t..., split=validation, passed=220]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h9-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=9, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h9\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  88% 221/250 [00:04<00:00, 55.38tests/s, dataset=got_cities, split=validation, passed=221]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h9-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=9, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h9\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  89% 222/250 [00:04<00:00, 55.38tests/s, dataset=got_sp_en_tr..., split=validation, passed=222]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h9-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=9, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h9\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  89% 223/250 [00:04<00:00, 53.88tests/s, dataset=got_sp_en_tr..., split=validation, passed=222]\u001b[A\n",
            "Testing 7b probe at layer chat:  89% 223/250 [00:04<00:00, 53.88tests/s, dataset=got_cities_c..., split=validation, passed=223]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h9-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=9, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h9\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  90% 224/250 [00:04<00:00, 53.88tests/s, dataset=got_cities_c..., split=validation, passed=224]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h9-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=9, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h9\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  90% 225/250 [00:04<00:00, 53.88tests/s, dataset=got_larger_t..., split=validation, passed=225]\u001b[A\n",
            "Testing 7b probe at layer chat:  90% 225/250 [00:04<00:00, 53.88tests/s, dataset=got_larger_t..., split=validation, passed=225]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h11-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=11, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h11\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  90% 226/250 [00:04<00:00, 53.88tests/s, dataset=got_cities, split=validation, passed=226]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h11-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=11, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h11\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  91% 227/250 [00:04<00:00, 53.88tests/s, dataset=got_sp_en_tr..., split=validation, passed=227]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h11-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=11, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h11\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  91% 228/250 [00:04<00:00, 53.88tests/s, dataset=got_cities_c..., split=validation, passed=228]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h11-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=11, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h11\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  92% 229/250 [00:04<00:00, 53.05tests/s, dataset=got_cities_c..., split=validation, passed=228]\u001b[A\n",
            "Testing 7b probe at layer chat:  92% 229/250 [00:04<00:00, 53.05tests/s, dataset=got_cities_c..., split=validation, passed=229]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h11-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=11, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h11\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  92% 230/250 [00:04<00:00, 53.05tests/s, dataset=got_larger_t..., split=validation, passed=230]\u001b[A\n",
            "Testing 7b probe at layer chat:  92% 230/250 [00:04<00:00, 53.05tests/s, dataset=got_larger_t..., split=validation, passed=230]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h13-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=13, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h13\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  92% 231/250 [00:04<00:00, 53.05tests/s, dataset=got_cities, split=validation, passed=231]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h13-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=13, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h13\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  93% 232/250 [00:04<00:00, 53.05tests/s, dataset=got_sp_en_tr..., split=validation, passed=232]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h13-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=13, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h13\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  93% 233/250 [00:04<00:00, 53.05tests/s, dataset=got_cities_c..., split=validation, passed=233]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h13-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=13, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h13\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  94% 234/250 [00:04<00:00, 53.05tests/s, dataset=got_cities_c..., split=validation, passed=234]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h13-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=13, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h13\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  94% 235/250 [00:04<00:00, 47.35tests/s, dataset=got_cities_c..., split=validation, passed=234]\u001b[A\n",
            "Testing 7b probe at layer chat:  94% 235/250 [00:04<00:00, 47.35tests/s, dataset=got_larger_t..., split=validation, passed=235]\u001b[A\n",
            "Testing 7b probe at layer chat:  94% 235/250 [00:04<00:00, 47.35tests/s, dataset=got_larger_t..., split=validation, passed=235]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h15-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=15, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h15\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  94% 236/250 [00:04<00:00, 47.35tests/s, dataset=got_cities, split=validation, passed=236]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h15-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=15, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h15\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  95% 237/250 [00:04<00:00, 47.35tests/s, dataset=got_sp_en_tr..., split=validation, passed=237]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h15-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=15, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h15\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  95% 238/250 [00:04<00:00, 47.35tests/s, dataset=got_cities_c..., split=validation, passed=238]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h15-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=15, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h15\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  96% 239/250 [00:04<00:00, 47.35tests/s, dataset=got_cities_c..., split=validation, passed=239]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h15-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=15, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h15\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  96% 240/250 [00:04<00:00, 47.35tests/s, dataset=got_larger_t..., split=validation, passed=240]\u001b[A\n",
            "Testing 7b probe at layer chat:  96% 240/250 [00:04<00:00, 47.35tests/s, dataset=got_larger_t..., split=validation, passed=240]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h17-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=17, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h17\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  96% 241/250 [00:04<00:00, 48.47tests/s, dataset=got_larger_t..., split=validation, passed=240]\u001b[A\n",
            "Testing 7b probe at layer chat:  96% 241/250 [00:04<00:00, 48.47tests/s, dataset=got_cities, split=validation, passed=241]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h17-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=17, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h17\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  97% 242/250 [00:04<00:00, 48.47tests/s, dataset=got_sp_en_tr..., split=validation, passed=242]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h17-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=17, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h17\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  97% 243/250 [00:04<00:00, 48.47tests/s, dataset=got_cities_c..., split=validation, passed=243]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h17-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=17, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h17\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  98% 244/250 [00:04<00:00, 48.47tests/s, dataset=got_cities_c..., split=validation, passed=244]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h17-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=17, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h17\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  98% 245/250 [00:04<00:00, 48.47tests/s, dataset=got_larger_t..., split=validation, passed=245]\u001b[A\n",
            "Testing 7b probe at layer chat:  98% 245/250 [00:04<00:00, 48.47tests/s, dataset=got_larger_t..., split=validation, passed=245]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h19-0' on dataset 'got_cities' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=19, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities\n",
            "  - split: validation\n",
            "  - point_name: h19\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  98% 246/250 [00:04<00:00, 48.47tests/s, dataset=got_cities, split=validation, passed=246]     \u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h19-0' on dataset 'got_sp_en_trans' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=19, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_sp_en_trans\n",
            "  - split: validation\n",
            "  - point_name: h19\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (130, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  99% 247/250 [00:04<00:00, 49.42tests/s, dataset=got_cities, split=validation, passed=246]\u001b[A\n",
            "Testing 7b probe at layer chat:  99% 247/250 [00:04<00:00, 49.42tests/s, dataset=got_sp_en_tr..., split=validation, passed=247]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h19-0' on dataset 'got_cities_cities_conj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=19, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_conj\n",
            "  - split: validation\n",
            "  - point_name: h19\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat:  99% 248/250 [00:04<00:00, 49.42tests/s, dataset=got_cities_c..., split=validation, passed=248]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h19-0' on dataset 'got_cities_cities_disj' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=19, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_cities_cities_disj\n",
            "  - split: validation\n",
            "  - point_name: h19\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (100, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat: 100% 249/250 [00:04<00:00, 49.42tests/s, dataset=got_cities_c..., split=validation, passed=249]\u001b[AEvaluating probe 'Llama-2-7b-chat-hf-got_larger_than-lr-h19-0' on dataset 'got_larger_than' split 'validation'\n",
            "Probe metadata: llm_id=Llama-2-7b-chat-hf, method=lr, layer=19, token_idx=0\n",
            "Requesting activations with:\n",
            "  - llm_id: Llama-2-7b-chat-hf\n",
            "  - dataset_filter: got_larger_than\n",
            "  - split: validation\n",
            "  - point_name: h19\n",
            "  - token_idx: 0\n",
            "Successfully retrieved activation arrays with shape (200, 4096)\n",
            "\n",
            "Testing 7b probe at layer chat: 100% 250/250 [00:04<00:00, 52.91tests/s, dataset=got_larger_t..., split=validation, passed=250]\n",
            "\n",
            "✅ Evaluation completed: 250 results generated\n",
            "📊 Results summary:\n",
            "   Unique probes: 1\n",
            "   Datasets evaluated: 5\n",
            "   Mean accuracy: 0.679\n",
            "   Max accuracy: 1.000\n",
            "🎯 Calculating recovered accuracy...\n",
            "   Mean recovered accuracy: 0.692\n",
            "\n",
            "Evaluation completed. Generated 250 results.\n",
            "Results saved to: output/evaluations/alina_mvp_eval/results.csv\n",
            "Results saved to: output/evaluations/alina_mvp_eval/results.json\n",
            "\n",
            "Generating visualizations...\n",
            "Best probe configuration: lr trained on got_cities at layer 17\n",
            "\n",
            "Generating token position analysis...\n",
            "\n",
            "Generating probe agreement analysis...\n",
            "WARNING: No predictions provided for agreement analysis. Creating placeholder visualization.\n",
            "WARNING: Not enough probe methods for complementarity analysis\n",
            "\n",
            "Generating probe rankings...\n",
            "\n",
            "Computing probe generalization rankings: 50comparisons [00:00, 1573.93comparisons/s]\n",
            "Successfully ranked probes based on 2450 comparisons\n",
            "\n",
            "Computing probe generalization rankings: 50comparisons [00:00, 1607.68comparisons/s]\n",
            "Successfully ranked probes based on 2450 comparisons\n",
            "Summary report saved to: output/evaluations/alina_mvp_eval/summary.txt\n",
            "Detailed statistics saved to: output/evaluations/alina_mvp_eval/detailed_statistics.csv\n",
            "\n",
            "Results saved to: output/evaluations/alina_mvp_eval\n",
            "Individual probe evaluation completed: 250 results\n",
            "\n",
            "Total evaluation results: 250\n",
            "Evaluation completed successfully\n",
            "Generated 250 evaluation results\n",
            "Accuracy: mean=0.679, max=1.000\n",
            "Results by method:\n",
            "   lr: 250 results\n",
            "Stage probe_evaluation completed successfully\n",
            "probe_evaluation completed in 15.04s\n",
            "Stage 3/3: probe_evaluation: 100% 3/3 [00:32<00:00, 10.69s/stage]\n",
            "\n",
            "Pipeline completed successfully in 32.07s!\n",
            "\n",
            "✓ Pipeline 'standard_probe_pipeline' completed successfully!\n",
            "Total duration: 32.07 seconds\n",
            "Stages executed: dataset_creation → probe_training → probe_evaluation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install awscli\n",
        "import os\n",
        "import os\n",
        "\n",
        "#input keys and region here\n",
        "os.environ['AWS_ACCESS_KEY_ID'] = ''\n",
        "os.environ['AWS_SECRET_ACCESS_KEY'] = ''\n",
        "os.environ[\"AWS_DEFAULT_REGION\"] = \"\"\n",
        "\n",
        "# list the experiment paths the pipeline uses\n",
        "!aws s3 ls s3://probeengbucket/experiments/\n",
        "!aws s3 ls s3://probeengbucket/experiments/alina_mvp_dataset/datasets/\n",
        "!aws s3 ls s3://probeengbucket/experiments/alina_mvp_train/\n",
        "!aws s3 ls s3://probeengbucket/experiments/alina_mvp_eval/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HIJIW9KEtLT",
        "outputId": "a91fa42a-5038-49b4-8fe4-a26d8bdc5ac1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "boto3 1.38.8 requires botocore<1.39.0,>=1.38.8, but you have botocore 1.40.41 which is incompatible.\n",
            "boto3 1.38.8 requires s3transfer<0.13.0,>=0.12.0, but you have s3transfer 0.14.0 which is incompatible.\n",
            "sphinx 8.2.3 requires docutils<0.22,>=0.20, but you have docutils 0.19 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m                           PRE alina_mvp_dataset/\n",
            "                           PRE alina_mvp_train/\n",
            "2025-09-30 14:39:17  737273701 activations.pickle\n",
            "                           PRE probes/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Run this in a *bash* cell\n",
        "%%bash\n",
        "cd /content/probeeng\n",
        "\n",
        "#  make sure aws cli exists (safe to rerun)\n",
        "pip -q install awscli >/dev/null 2>&1 || true\n",
        "\n",
        "# create local folders\n",
        "mkdir -p output/datasets output/probes/alina_mvp_train output/evaluations/alina_mvp_eval\n",
        "\n",
        "# pull your artifacts from S3 to the repo\n",
        "aws s3 cp s3://probeengbucket/experiments/alina_mvp_dataset/datasets/activations.pickle output/datasets/ || true\n",
        "aws s3 sync s3://probeengbucket/experiments/alina_mvp_train/ output/probes/alina_mvp_train/\n",
        "aws s3 sync s3://probeengbucket/experiments/alina_mvp_eval/ output/evaluations/alina_mvp_eval/ || true\n",
        "\n",
        "# check\n",
        "ls -lah output/probes/alina_mvp_train | head -n 50\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SCjEkwXFsKB",
        "outputId": "7c8781e0-1997-45db-a6e3-1b7a0a247cdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed 256.0 KiB/703.1 MiB (346.1 KiB/s) with 1 file(s) remaining\rCompleted 512.0 KiB/703.1 MiB (675.8 KiB/s) with 1 file(s) remaining\rCompleted 768.0 KiB/703.1 MiB (1009.6 KiB/s) with 1 file(s) remaining\rCompleted 1.0 MiB/703.1 MiB (1.3 MiB/s) with 1 file(s) remaining     \rCompleted 1.2 MiB/703.1 MiB (1.6 MiB/s) with 1 file(s) remaining     \rCompleted 1.5 MiB/703.1 MiB (1.9 MiB/s) with 1 file(s) remaining     \rCompleted 1.8 MiB/703.1 MiB (2.2 MiB/s) with 1 file(s) remaining     \rCompleted 2.0 MiB/703.1 MiB (2.5 MiB/s) with 1 file(s) remaining     \rCompleted 2.2 MiB/703.1 MiB (2.8 MiB/s) with 1 file(s) remaining     \rCompleted 2.5 MiB/703.1 MiB (3.1 MiB/s) with 1 file(s) remaining     \rCompleted 2.8 MiB/703.1 MiB (3.5 MiB/s) with 1 file(s) remaining     \rCompleted 3.0 MiB/703.1 MiB (3.8 MiB/s) with 1 file(s) remaining     \rCompleted 3.2 MiB/703.1 MiB (4.1 MiB/s) with 1 file(s) remaining     \rCompleted 3.5 MiB/703.1 MiB (4.4 MiB/s) with 1 file(s) remaining     \rCompleted 3.8 MiB/703.1 MiB (4.7 MiB/s) with 1 file(s) remaining     \rCompleted 4.0 MiB/703.1 MiB (4.9 MiB/s) with 1 file(s) remaining     \rCompleted 4.2 MiB/703.1 MiB (5.2 MiB/s) with 1 file(s) remaining     \rCompleted 4.5 MiB/703.1 MiB (5.5 MiB/s) with 1 file(s) remaining     \rCompleted 4.8 MiB/703.1 MiB (5.8 MiB/s) with 1 file(s) remaining     \rCompleted 5.0 MiB/703.1 MiB (6.1 MiB/s) with 1 file(s) remaining     \rCompleted 5.2 MiB/703.1 MiB (6.4 MiB/s) with 1 file(s) remaining     \rCompleted 5.5 MiB/703.1 MiB (6.7 MiB/s) with 1 file(s) remaining     \rCompleted 5.8 MiB/703.1 MiB (7.0 MiB/s) with 1 file(s) remaining     \rCompleted 6.0 MiB/703.1 MiB (7.3 MiB/s) with 1 file(s) remaining     \rCompleted 6.2 MiB/703.1 MiB (7.6 MiB/s) with 1 file(s) remaining     \rCompleted 6.5 MiB/703.1 MiB (7.9 MiB/s) with 1 file(s) remaining     \rCompleted 6.8 MiB/703.1 MiB (8.2 MiB/s) with 1 file(s) remaining     \rCompleted 7.0 MiB/703.1 MiB (8.4 MiB/s) with 1 file(s) remaining     \rCompleted 7.2 MiB/703.1 MiB (8.7 MiB/s) with 1 file(s) remaining     \rCompleted 7.5 MiB/703.1 MiB (9.0 MiB/s) with 1 file(s) remaining     \rCompleted 7.8 MiB/703.1 MiB (9.3 MiB/s) with 1 file(s) remaining     \rCompleted 8.0 MiB/703.1 MiB (9.6 MiB/s) with 1 file(s) remaining     \rCompleted 8.2 MiB/703.1 MiB (9.9 MiB/s) with 1 file(s) remaining     \rCompleted 8.5 MiB/703.1 MiB (10.2 MiB/s) with 1 file(s) remaining    \rCompleted 8.8 MiB/703.1 MiB (10.5 MiB/s) with 1 file(s) remaining    \rCompleted 9.0 MiB/703.1 MiB (10.8 MiB/s) with 1 file(s) remaining    \rCompleted 9.2 MiB/703.1 MiB (11.1 MiB/s) with 1 file(s) remaining    \rCompleted 9.5 MiB/703.1 MiB (11.4 MiB/s) with 1 file(s) remaining    \rCompleted 9.8 MiB/703.1 MiB (11.6 MiB/s) with 1 file(s) remaining    \rCompleted 10.0 MiB/703.1 MiB (11.9 MiB/s) with 1 file(s) remaining   \rCompleted 10.2 MiB/703.1 MiB (12.2 MiB/s) with 1 file(s) remaining   \rCompleted 10.5 MiB/703.1 MiB (12.5 MiB/s) with 1 file(s) remaining   \rCompleted 10.8 MiB/703.1 MiB (12.7 MiB/s) with 1 file(s) remaining   \rCompleted 11.0 MiB/703.1 MiB (13.0 MiB/s) with 1 file(s) remaining   \rCompleted 11.2 MiB/703.1 MiB (13.3 MiB/s) with 1 file(s) remaining   \rCompleted 11.5 MiB/703.1 MiB (13.5 MiB/s) with 1 file(s) remaining   \rCompleted 11.8 MiB/703.1 MiB (13.8 MiB/s) with 1 file(s) remaining   \rCompleted 12.0 MiB/703.1 MiB (14.1 MiB/s) with 1 file(s) remaining   \rCompleted 12.2 MiB/703.1 MiB (14.4 MiB/s) with 1 file(s) remaining   \rCompleted 12.5 MiB/703.1 MiB (14.7 MiB/s) with 1 file(s) remaining   \rCompleted 12.8 MiB/703.1 MiB (15.0 MiB/s) with 1 file(s) remaining   \rCompleted 13.0 MiB/703.1 MiB (15.2 MiB/s) with 1 file(s) remaining   \rCompleted 13.2 MiB/703.1 MiB (15.5 MiB/s) with 1 file(s) remaining   \rCompleted 13.5 MiB/703.1 MiB (15.8 MiB/s) with 1 file(s) remaining   \rCompleted 13.8 MiB/703.1 MiB (16.1 MiB/s) with 1 file(s) remaining   \rCompleted 14.0 MiB/703.1 MiB (16.4 MiB/s) with 1 file(s) remaining   \rCompleted 14.2 MiB/703.1 MiB (16.7 MiB/s) with 1 file(s) remaining   \rCompleted 14.5 MiB/703.1 MiB (16.9 MiB/s) with 1 file(s) remaining   \rCompleted 14.8 MiB/703.1 MiB (17.2 MiB/s) with 1 file(s) remaining   \rCompleted 15.0 MiB/703.1 MiB (17.5 MiB/s) with 1 file(s) remaining   \rCompleted 15.2 MiB/703.1 MiB (17.7 MiB/s) with 1 file(s) remaining   \rCompleted 15.5 MiB/703.1 MiB (17.9 MiB/s) with 1 file(s) remaining   \rCompleted 15.8 MiB/703.1 MiB (18.2 MiB/s) with 1 file(s) remaining   \rCompleted 16.0 MiB/703.1 MiB (18.4 MiB/s) with 1 file(s) remaining   \rCompleted 16.2 MiB/703.1 MiB (18.7 MiB/s) with 1 file(s) remaining   \rCompleted 16.5 MiB/703.1 MiB (18.7 MiB/s) with 1 file(s) remaining   \rCompleted 16.8 MiB/703.1 MiB (18.9 MiB/s) with 1 file(s) remaining   \rCompleted 17.0 MiB/703.1 MiB (19.1 MiB/s) with 1 file(s) remaining   \rCompleted 17.2 MiB/703.1 MiB (19.2 MiB/s) with 1 file(s) remaining   \rCompleted 17.5 MiB/703.1 MiB (19.4 MiB/s) with 1 file(s) remaining   \rCompleted 17.8 MiB/703.1 MiB (19.7 MiB/s) with 1 file(s) remaining   \rCompleted 18.0 MiB/703.1 MiB (19.9 MiB/s) with 1 file(s) remaining   \rCompleted 18.2 MiB/703.1 MiB (20.1 MiB/s) with 1 file(s) remaining   \rCompleted 18.5 MiB/703.1 MiB (20.3 MiB/s) with 1 file(s) remaining   \rCompleted 18.8 MiB/703.1 MiB (20.6 MiB/s) with 1 file(s) remaining   \rCompleted 19.0 MiB/703.1 MiB (20.7 MiB/s) with 1 file(s) remaining   \rCompleted 19.2 MiB/703.1 MiB (21.0 MiB/s) with 1 file(s) remaining   \rCompleted 19.5 MiB/703.1 MiB (21.2 MiB/s) with 1 file(s) remaining   \rCompleted 19.8 MiB/703.1 MiB (21.5 MiB/s) with 1 file(s) remaining   \rCompleted 20.0 MiB/703.1 MiB (21.7 MiB/s) with 1 file(s) remaining   \rCompleted 20.2 MiB/703.1 MiB (22.0 MiB/s) with 1 file(s) remaining   \rCompleted 20.5 MiB/703.1 MiB (22.3 MiB/s) with 1 file(s) remaining   \rCompleted 20.8 MiB/703.1 MiB (22.5 MiB/s) with 1 file(s) remaining   \rCompleted 21.0 MiB/703.1 MiB (22.8 MiB/s) with 1 file(s) remaining   \rCompleted 21.2 MiB/703.1 MiB (23.0 MiB/s) with 1 file(s) remaining   \rCompleted 21.5 MiB/703.1 MiB (23.3 MiB/s) with 1 file(s) remaining   \rCompleted 21.8 MiB/703.1 MiB (23.5 MiB/s) with 1 file(s) remaining   \rCompleted 22.0 MiB/703.1 MiB (23.8 MiB/s) with 1 file(s) remaining   \rCompleted 22.2 MiB/703.1 MiB (24.0 MiB/s) with 1 file(s) remaining   \rCompleted 22.5 MiB/703.1 MiB (24.2 MiB/s) with 1 file(s) remaining   \rCompleted 22.8 MiB/703.1 MiB (24.4 MiB/s) with 1 file(s) remaining   \rCompleted 23.0 MiB/703.1 MiB (24.6 MiB/s) with 1 file(s) remaining   \rCompleted 23.2 MiB/703.1 MiB (24.9 MiB/s) with 1 file(s) remaining   \rCompleted 23.5 MiB/703.1 MiB (25.1 MiB/s) with 1 file(s) remaining   \rCompleted 23.8 MiB/703.1 MiB (25.4 MiB/s) with 1 file(s) remaining   \rCompleted 24.0 MiB/703.1 MiB (25.6 MiB/s) with 1 file(s) remaining   \rCompleted 24.2 MiB/703.1 MiB (25.9 MiB/s) with 1 file(s) remaining   \rCompleted 24.5 MiB/703.1 MiB (26.1 MiB/s) with 1 file(s) remaining   \rCompleted 24.8 MiB/703.1 MiB (26.3 MiB/s) with 1 file(s) remaining   \rCompleted 25.0 MiB/703.1 MiB (26.6 MiB/s) with 1 file(s) remaining   \rCompleted 25.2 MiB/703.1 MiB (26.9 MiB/s) with 1 file(s) remaining   \rCompleted 25.5 MiB/703.1 MiB (27.1 MiB/s) with 1 file(s) remaining   \rCompleted 25.8 MiB/703.1 MiB (27.3 MiB/s) with 1 file(s) remaining   \rCompleted 26.0 MiB/703.1 MiB (27.6 MiB/s) with 1 file(s) remaining   \rCompleted 26.2 MiB/703.1 MiB (27.8 MiB/s) with 1 file(s) remaining   \rCompleted 26.5 MiB/703.1 MiB (28.1 MiB/s) with 1 file(s) remaining   \rCompleted 26.8 MiB/703.1 MiB (28.3 MiB/s) with 1 file(s) remaining   \rCompleted 27.0 MiB/703.1 MiB (28.6 MiB/s) with 1 file(s) remaining   \rCompleted 27.2 MiB/703.1 MiB (28.8 MiB/s) with 1 file(s) remaining   \rCompleted 27.5 MiB/703.1 MiB (29.0 MiB/s) with 1 file(s) remaining   \rCompleted 27.8 MiB/703.1 MiB (29.3 MiB/s) with 1 file(s) remaining   \rCompleted 28.0 MiB/703.1 MiB (29.5 MiB/s) with 1 file(s) remaining   \rCompleted 28.2 MiB/703.1 MiB (29.8 MiB/s) with 1 file(s) remaining   \rCompleted 28.5 MiB/703.1 MiB (30.0 MiB/s) with 1 file(s) remaining   \rCompleted 28.8 MiB/703.1 MiB (30.3 MiB/s) with 1 file(s) remaining   \rCompleted 29.0 MiB/703.1 MiB (30.5 MiB/s) with 1 file(s) remaining   \rCompleted 29.2 MiB/703.1 MiB (30.7 MiB/s) with 1 file(s) remaining   \rCompleted 29.5 MiB/703.1 MiB (30.9 MiB/s) with 1 file(s) remaining   \rCompleted 29.8 MiB/703.1 MiB (31.2 MiB/s) with 1 file(s) remaining   \rCompleted 30.0 MiB/703.1 MiB (31.4 MiB/s) with 1 file(s) remaining   \rCompleted 30.2 MiB/703.1 MiB (31.7 MiB/s) with 1 file(s) remaining   \rCompleted 30.5 MiB/703.1 MiB (31.9 MiB/s) with 1 file(s) remaining   \rCompleted 30.8 MiB/703.1 MiB (32.1 MiB/s) with 1 file(s) remaining   \rCompleted 31.0 MiB/703.1 MiB (32.4 MiB/s) with 1 file(s) remaining   \rCompleted 31.2 MiB/703.1 MiB (32.6 MiB/s) with 1 file(s) remaining   \rCompleted 31.5 MiB/703.1 MiB (32.9 MiB/s) with 1 file(s) remaining   \rCompleted 31.8 MiB/703.1 MiB (33.1 MiB/s) with 1 file(s) remaining   \rCompleted 32.0 MiB/703.1 MiB (33.3 MiB/s) with 1 file(s) remaining   \rCompleted 32.2 MiB/703.1 MiB (33.6 MiB/s) with 1 file(s) remaining   \rCompleted 32.5 MiB/703.1 MiB (33.8 MiB/s) with 1 file(s) remaining   \rCompleted 32.8 MiB/703.1 MiB (34.0 MiB/s) with 1 file(s) remaining   \rCompleted 33.0 MiB/703.1 MiB (34.2 MiB/s) with 1 file(s) remaining   \rCompleted 33.2 MiB/703.1 MiB (34.5 MiB/s) with 1 file(s) remaining   \rCompleted 33.5 MiB/703.1 MiB (34.7 MiB/s) with 1 file(s) remaining   \rCompleted 33.8 MiB/703.1 MiB (34.9 MiB/s) with 1 file(s) remaining   \rCompleted 34.0 MiB/703.1 MiB (35.2 MiB/s) with 1 file(s) remaining   \rCompleted 34.2 MiB/703.1 MiB (35.4 MiB/s) with 1 file(s) remaining   \rCompleted 34.5 MiB/703.1 MiB (35.6 MiB/s) with 1 file(s) remaining   \rCompleted 34.8 MiB/703.1 MiB (35.9 MiB/s) with 1 file(s) remaining   \rCompleted 35.0 MiB/703.1 MiB (36.1 MiB/s) with 1 file(s) remaining   \rCompleted 35.2 MiB/703.1 MiB (36.3 MiB/s) with 1 file(s) remaining   \rCompleted 35.5 MiB/703.1 MiB (36.5 MiB/s) with 1 file(s) remaining   \rCompleted 35.8 MiB/703.1 MiB (36.8 MiB/s) with 1 file(s) remaining   \rCompleted 36.0 MiB/703.1 MiB (37.0 MiB/s) with 1 file(s) remaining   \rCompleted 36.2 MiB/703.1 MiB (37.2 MiB/s) with 1 file(s) remaining   \rCompleted 36.5 MiB/703.1 MiB (37.4 MiB/s) with 1 file(s) remaining   \rCompleted 36.8 MiB/703.1 MiB (37.7 MiB/s) with 1 file(s) remaining   \rCompleted 37.0 MiB/703.1 MiB (37.9 MiB/s) with 1 file(s) remaining   \rCompleted 37.2 MiB/703.1 MiB (38.1 MiB/s) with 1 file(s) remaining   \rCompleted 37.5 MiB/703.1 MiB (38.3 MiB/s) with 1 file(s) remaining   \rCompleted 37.8 MiB/703.1 MiB (38.6 MiB/s) with 1 file(s) remaining   \rCompleted 38.0 MiB/703.1 MiB (38.8 MiB/s) with 1 file(s) remaining   \rCompleted 38.2 MiB/703.1 MiB (39.0 MiB/s) with 1 file(s) remaining   \rCompleted 38.5 MiB/703.1 MiB (39.3 MiB/s) with 1 file(s) remaining   \rCompleted 38.8 MiB/703.1 MiB (39.4 MiB/s) with 1 file(s) remaining   \rCompleted 39.0 MiB/703.1 MiB (39.6 MiB/s) with 1 file(s) remaining   \rCompleted 39.2 MiB/703.1 MiB (39.9 MiB/s) with 1 file(s) remaining   \rCompleted 39.5 MiB/703.1 MiB (40.1 MiB/s) with 1 file(s) remaining   \rCompleted 39.8 MiB/703.1 MiB (40.3 MiB/s) with 1 file(s) remaining   \rCompleted 40.0 MiB/703.1 MiB (40.6 MiB/s) with 1 file(s) remaining   \rCompleted 40.2 MiB/703.1 MiB (40.8 MiB/s) with 1 file(s) remaining   \rCompleted 40.5 MiB/703.1 MiB (41.0 MiB/s) with 1 file(s) remaining   \rCompleted 40.8 MiB/703.1 MiB (41.2 MiB/s) with 1 file(s) remaining   \rCompleted 41.0 MiB/703.1 MiB (41.4 MiB/s) with 1 file(s) remaining   \rCompleted 41.2 MiB/703.1 MiB (41.6 MiB/s) with 1 file(s) remaining   \rCompleted 41.5 MiB/703.1 MiB (41.9 MiB/s) with 1 file(s) remaining   \rCompleted 41.8 MiB/703.1 MiB (42.1 MiB/s) with 1 file(s) remaining   \rCompleted 42.0 MiB/703.1 MiB (42.3 MiB/s) with 1 file(s) remaining   \rCompleted 42.2 MiB/703.1 MiB (42.6 MiB/s) with 1 file(s) remaining   \rCompleted 42.5 MiB/703.1 MiB (42.8 MiB/s) with 1 file(s) remaining   \rCompleted 42.8 MiB/703.1 MiB (43.0 MiB/s) with 1 file(s) remaining   \rCompleted 43.0 MiB/703.1 MiB (43.2 MiB/s) with 1 file(s) remaining   \rCompleted 43.2 MiB/703.1 MiB (43.4 MiB/s) with 1 file(s) remaining   \rCompleted 43.5 MiB/703.1 MiB (43.6 MiB/s) with 1 file(s) remaining   \rCompleted 43.8 MiB/703.1 MiB (43.9 MiB/s) with 1 file(s) remaining   \rCompleted 44.0 MiB/703.1 MiB (44.1 MiB/s) with 1 file(s) remaining   \rCompleted 44.2 MiB/703.1 MiB (44.3 MiB/s) with 1 file(s) remaining   \rCompleted 44.5 MiB/703.1 MiB (44.6 MiB/s) with 1 file(s) remaining   \rCompleted 44.8 MiB/703.1 MiB (44.8 MiB/s) with 1 file(s) remaining   \rCompleted 45.0 MiB/703.1 MiB (45.0 MiB/s) with 1 file(s) remaining   \rCompleted 45.2 MiB/703.1 MiB (45.2 MiB/s) with 1 file(s) remaining   \rCompleted 45.5 MiB/703.1 MiB (45.4 MiB/s) with 1 file(s) remaining   \rCompleted 45.8 MiB/703.1 MiB (45.6 MiB/s) with 1 file(s) remaining   \rCompleted 46.0 MiB/703.1 MiB (45.8 MiB/s) with 1 file(s) remaining   \rCompleted 46.2 MiB/703.1 MiB (46.1 MiB/s) with 1 file(s) remaining   \rCompleted 46.5 MiB/703.1 MiB (46.3 MiB/s) with 1 file(s) remaining   \rCompleted 46.8 MiB/703.1 MiB (46.5 MiB/s) with 1 file(s) remaining   \rCompleted 47.0 MiB/703.1 MiB (46.6 MiB/s) with 1 file(s) remaining   \rCompleted 47.2 MiB/703.1 MiB (46.9 MiB/s) with 1 file(s) remaining   \rCompleted 47.5 MiB/703.1 MiB (47.1 MiB/s) with 1 file(s) remaining   \rCompleted 47.8 MiB/703.1 MiB (47.3 MiB/s) with 1 file(s) remaining   \rCompleted 48.0 MiB/703.1 MiB (47.5 MiB/s) with 1 file(s) remaining   \rCompleted 48.2 MiB/703.1 MiB (47.8 MiB/s) with 1 file(s) remaining   \rCompleted 48.5 MiB/703.1 MiB (48.0 MiB/s) with 1 file(s) remaining   \rCompleted 48.8 MiB/703.1 MiB (48.2 MiB/s) with 1 file(s) remaining   \rCompleted 49.0 MiB/703.1 MiB (48.4 MiB/s) with 1 file(s) remaining   \rCompleted 49.2 MiB/703.1 MiB (48.6 MiB/s) with 1 file(s) remaining   \rCompleted 49.5 MiB/703.1 MiB (48.8 MiB/s) with 1 file(s) remaining   \rCompleted 49.8 MiB/703.1 MiB (49.0 MiB/s) with 1 file(s) remaining   \rCompleted 50.0 MiB/703.1 MiB (49.2 MiB/s) with 1 file(s) remaining   \rCompleted 50.2 MiB/703.1 MiB (49.4 MiB/s) with 1 file(s) remaining   \rCompleted 50.5 MiB/703.1 MiB (49.6 MiB/s) with 1 file(s) remaining   \rCompleted 50.8 MiB/703.1 MiB (49.8 MiB/s) with 1 file(s) remaining   \rCompleted 51.0 MiB/703.1 MiB (50.0 MiB/s) with 1 file(s) remaining   \rCompleted 51.2 MiB/703.1 MiB (50.3 MiB/s) with 1 file(s) remaining   \rCompleted 51.5 MiB/703.1 MiB (50.5 MiB/s) with 1 file(s) remaining   \rCompleted 51.8 MiB/703.1 MiB (50.7 MiB/s) with 1 file(s) remaining   \rCompleted 52.0 MiB/703.1 MiB (50.9 MiB/s) with 1 file(s) remaining   \rCompleted 52.2 MiB/703.1 MiB (51.1 MiB/s) with 1 file(s) remaining   \rCompleted 52.5 MiB/703.1 MiB (51.3 MiB/s) with 1 file(s) remaining   \rCompleted 52.8 MiB/703.1 MiB (51.5 MiB/s) with 1 file(s) remaining   \rCompleted 53.0 MiB/703.1 MiB (51.7 MiB/s) with 1 file(s) remaining   \rCompleted 53.2 MiB/703.1 MiB (51.9 MiB/s) with 1 file(s) remaining   \rCompleted 53.5 MiB/703.1 MiB (52.2 MiB/s) with 1 file(s) remaining   \rCompleted 53.8 MiB/703.1 MiB (52.4 MiB/s) with 1 file(s) remaining   \rCompleted 54.0 MiB/703.1 MiB (52.6 MiB/s) with 1 file(s) remaining   \rCompleted 54.2 MiB/703.1 MiB (52.8 MiB/s) with 1 file(s) remaining   \rCompleted 54.5 MiB/703.1 MiB (53.1 MiB/s) with 1 file(s) remaining   \rCompleted 54.8 MiB/703.1 MiB (53.2 MiB/s) with 1 file(s) remaining   \rCompleted 55.0 MiB/703.1 MiB (53.4 MiB/s) with 1 file(s) remaining   \rCompleted 55.2 MiB/703.1 MiB (53.6 MiB/s) with 1 file(s) remaining   \rCompleted 55.5 MiB/703.1 MiB (53.8 MiB/s) with 1 file(s) remaining   \rCompleted 55.8 MiB/703.1 MiB (54.1 MiB/s) with 1 file(s) remaining   \rCompleted 56.0 MiB/703.1 MiB (54.3 MiB/s) with 1 file(s) remaining   \rCompleted 56.2 MiB/703.1 MiB (54.5 MiB/s) with 1 file(s) remaining   \rCompleted 56.5 MiB/703.1 MiB (54.7 MiB/s) with 1 file(s) remaining   \rCompleted 56.8 MiB/703.1 MiB (54.9 MiB/s) with 1 file(s) remaining   \rCompleted 57.0 MiB/703.1 MiB (55.1 MiB/s) with 1 file(s) remaining   \rCompleted 57.2 MiB/703.1 MiB (55.3 MiB/s) with 1 file(s) remaining   \rCompleted 57.5 MiB/703.1 MiB (55.5 MiB/s) with 1 file(s) remaining   \rCompleted 57.8 MiB/703.1 MiB (55.7 MiB/s) with 1 file(s) remaining   \rCompleted 58.0 MiB/703.1 MiB (55.9 MiB/s) with 1 file(s) remaining   \rCompleted 58.2 MiB/703.1 MiB (56.1 MiB/s) with 1 file(s) remaining   \rCompleted 58.5 MiB/703.1 MiB (56.3 MiB/s) with 1 file(s) remaining   \rCompleted 58.8 MiB/703.1 MiB (56.5 MiB/s) with 1 file(s) remaining   \rCompleted 59.0 MiB/703.1 MiB (56.8 MiB/s) with 1 file(s) remaining   \rCompleted 59.2 MiB/703.1 MiB (57.0 MiB/s) with 1 file(s) remaining   \rCompleted 59.5 MiB/703.1 MiB (57.2 MiB/s) with 1 file(s) remaining   \rCompleted 59.8 MiB/703.1 MiB (57.4 MiB/s) with 1 file(s) remaining   \rCompleted 60.0 MiB/703.1 MiB (57.5 MiB/s) with 1 file(s) remaining   \rCompleted 60.2 MiB/703.1 MiB (57.7 MiB/s) with 1 file(s) remaining   \rCompleted 60.5 MiB/703.1 MiB (57.9 MiB/s) with 1 file(s) remaining   \rCompleted 60.8 MiB/703.1 MiB (58.1 MiB/s) with 1 file(s) remaining   \rCompleted 61.0 MiB/703.1 MiB (58.3 MiB/s) with 1 file(s) remaining   \rCompleted 61.2 MiB/703.1 MiB (58.5 MiB/s) with 1 file(s) remaining   \rCompleted 61.5 MiB/703.1 MiB (58.7 MiB/s) with 1 file(s) remaining   \rCompleted 61.8 MiB/703.1 MiB (58.9 MiB/s) with 1 file(s) remaining   \rCompleted 62.0 MiB/703.1 MiB (59.1 MiB/s) with 1 file(s) remaining   \rCompleted 62.2 MiB/703.1 MiB (59.3 MiB/s) with 1 file(s) remaining   \rCompleted 62.5 MiB/703.1 MiB (59.5 MiB/s) with 1 file(s) remaining   \rCompleted 62.8 MiB/703.1 MiB (59.7 MiB/s) with 1 file(s) remaining   \rCompleted 63.0 MiB/703.1 MiB (59.9 MiB/s) with 1 file(s) remaining   \rCompleted 63.2 MiB/703.1 MiB (60.1 MiB/s) with 1 file(s) remaining   \rCompleted 63.5 MiB/703.1 MiB (60.3 MiB/s) with 1 file(s) remaining   \rCompleted 63.8 MiB/703.1 MiB (60.5 MiB/s) with 1 file(s) remaining   \rCompleted 64.0 MiB/703.1 MiB (60.7 MiB/s) with 1 file(s) remaining   \rCompleted 64.2 MiB/703.1 MiB (60.9 MiB/s) with 1 file(s) remaining   \rCompleted 64.5 MiB/703.1 MiB (61.1 MiB/s) with 1 file(s) remaining   \rCompleted 64.8 MiB/703.1 MiB (61.3 MiB/s) with 1 file(s) remaining   \rCompleted 65.0 MiB/703.1 MiB (61.5 MiB/s) with 1 file(s) remaining   \rCompleted 65.2 MiB/703.1 MiB (61.6 MiB/s) with 1 file(s) remaining   \rCompleted 65.5 MiB/703.1 MiB (61.8 MiB/s) with 1 file(s) remaining   \rCompleted 65.8 MiB/703.1 MiB (62.1 MiB/s) with 1 file(s) remaining   \rCompleted 66.0 MiB/703.1 MiB (62.2 MiB/s) with 1 file(s) remaining   \rCompleted 66.2 MiB/703.1 MiB (62.5 MiB/s) with 1 file(s) remaining   \rCompleted 66.5 MiB/703.1 MiB (62.6 MiB/s) with 1 file(s) remaining   \rCompleted 66.8 MiB/703.1 MiB (62.8 MiB/s) with 1 file(s) remaining   \rCompleted 67.0 MiB/703.1 MiB (62.9 MiB/s) with 1 file(s) remaining   \rCompleted 67.2 MiB/703.1 MiB (63.1 MiB/s) with 1 file(s) remaining   \rCompleted 67.5 MiB/703.1 MiB (63.3 MiB/s) with 1 file(s) remaining   \rCompleted 67.8 MiB/703.1 MiB (63.6 MiB/s) with 1 file(s) remaining   \rCompleted 68.0 MiB/703.1 MiB (63.8 MiB/s) with 1 file(s) remaining   \rCompleted 68.2 MiB/703.1 MiB (64.0 MiB/s) with 1 file(s) remaining   \rCompleted 68.5 MiB/703.1 MiB (64.2 MiB/s) with 1 file(s) remaining   \rCompleted 68.8 MiB/703.1 MiB (64.4 MiB/s) with 1 file(s) remaining   \rCompleted 69.0 MiB/703.1 MiB (64.5 MiB/s) with 1 file(s) remaining   \rCompleted 69.2 MiB/703.1 MiB (64.7 MiB/s) with 1 file(s) remaining   \rCompleted 69.5 MiB/703.1 MiB (64.9 MiB/s) with 1 file(s) remaining   \rCompleted 69.8 MiB/703.1 MiB (65.1 MiB/s) with 1 file(s) remaining   \rCompleted 70.0 MiB/703.1 MiB (65.3 MiB/s) with 1 file(s) remaining   \rCompleted 70.2 MiB/703.1 MiB (65.5 MiB/s) with 1 file(s) remaining   \rCompleted 70.5 MiB/703.1 MiB (65.7 MiB/s) with 1 file(s) remaining   \rCompleted 70.8 MiB/703.1 MiB (65.9 MiB/s) with 1 file(s) remaining   \rCompleted 71.0 MiB/703.1 MiB (66.1 MiB/s) with 1 file(s) remaining   \rCompleted 71.2 MiB/703.1 MiB (66.3 MiB/s) with 1 file(s) remaining   \rCompleted 71.5 MiB/703.1 MiB (66.5 MiB/s) with 1 file(s) remaining   \rCompleted 71.8 MiB/703.1 MiB (66.7 MiB/s) with 1 file(s) remaining   \rCompleted 72.0 MiB/703.1 MiB (66.9 MiB/s) with 1 file(s) remaining   \rCompleted 72.2 MiB/703.1 MiB (67.1 MiB/s) with 1 file(s) remaining   \rCompleted 72.5 MiB/703.1 MiB (67.3 MiB/s) with 1 file(s) remaining   \rCompleted 72.8 MiB/703.1 MiB (67.5 MiB/s) with 1 file(s) remaining   \rCompleted 73.0 MiB/703.1 MiB (67.7 MiB/s) with 1 file(s) remaining   \rCompleted 73.2 MiB/703.1 MiB (67.9 MiB/s) with 1 file(s) remaining   \rCompleted 73.5 MiB/703.1 MiB (68.0 MiB/s) with 1 file(s) remaining   \rCompleted 73.8 MiB/703.1 MiB (68.2 MiB/s) with 1 file(s) remaining   \rCompleted 74.0 MiB/703.1 MiB (68.4 MiB/s) with 1 file(s) remaining   \rCompleted 74.2 MiB/703.1 MiB (68.6 MiB/s) with 1 file(s) remaining   \rCompleted 74.5 MiB/703.1 MiB (68.8 MiB/s) with 1 file(s) remaining   \rCompleted 74.8 MiB/703.1 MiB (69.0 MiB/s) with 1 file(s) remaining   \rCompleted 75.0 MiB/703.1 MiB (69.2 MiB/s) with 1 file(s) remaining   \rCompleted 75.2 MiB/703.1 MiB (69.4 MiB/s) with 1 file(s) remaining   \rCompleted 75.5 MiB/703.1 MiB (69.6 MiB/s) with 1 file(s) remaining   \rCompleted 75.8 MiB/703.1 MiB (69.7 MiB/s) with 1 file(s) remaining   \rCompleted 76.0 MiB/703.1 MiB (69.9 MiB/s) with 1 file(s) remaining   \rCompleted 76.2 MiB/703.1 MiB (70.1 MiB/s) with 1 file(s) remaining   \rCompleted 76.5 MiB/703.1 MiB (70.3 MiB/s) with 1 file(s) remaining   \rCompleted 76.8 MiB/703.1 MiB (70.5 MiB/s) with 1 file(s) remaining   \rCompleted 77.0 MiB/703.1 MiB (70.7 MiB/s) with 1 file(s) remaining   \rCompleted 77.2 MiB/703.1 MiB (70.9 MiB/s) with 1 file(s) remaining   \rCompleted 77.5 MiB/703.1 MiB (71.1 MiB/s) with 1 file(s) remaining   \rCompleted 77.8 MiB/703.1 MiB (71.2 MiB/s) with 1 file(s) remaining   \rCompleted 78.0 MiB/703.1 MiB (71.4 MiB/s) with 1 file(s) remaining   \rCompleted 78.2 MiB/703.1 MiB (71.6 MiB/s) with 1 file(s) remaining   \rCompleted 78.5 MiB/703.1 MiB (71.8 MiB/s) with 1 file(s) remaining   \rCompleted 78.8 MiB/703.1 MiB (72.0 MiB/s) with 1 file(s) remaining   \rCompleted 79.0 MiB/703.1 MiB (72.3 MiB/s) with 1 file(s) remaining   \rCompleted 79.2 MiB/703.1 MiB (72.4 MiB/s) with 1 file(s) remaining   \rCompleted 79.5 MiB/703.1 MiB (72.6 MiB/s) with 1 file(s) remaining   \rCompleted 79.8 MiB/703.1 MiB (72.7 MiB/s) with 1 file(s) remaining   \rCompleted 80.0 MiB/703.1 MiB (72.9 MiB/s) with 1 file(s) remaining   \rCompleted 80.2 MiB/703.1 MiB (73.2 MiB/s) with 1 file(s) remaining   \rCompleted 80.5 MiB/703.1 MiB (73.4 MiB/s) with 1 file(s) remaining   \rCompleted 80.8 MiB/703.1 MiB (73.5 MiB/s) with 1 file(s) remaining   \rCompleted 81.0 MiB/703.1 MiB (73.7 MiB/s) with 1 file(s) remaining   \rCompleted 81.2 MiB/703.1 MiB (73.9 MiB/s) with 1 file(s) remaining   \rCompleted 81.5 MiB/703.1 MiB (74.0 MiB/s) with 1 file(s) remaining   \rCompleted 81.8 MiB/703.1 MiB (74.1 MiB/s) with 1 file(s) remaining   \rCompleted 82.0 MiB/703.1 MiB (74.3 MiB/s) with 1 file(s) remaining   \rCompleted 82.2 MiB/703.1 MiB (74.6 MiB/s) with 1 file(s) remaining   \rCompleted 82.5 MiB/703.1 MiB (74.7 MiB/s) with 1 file(s) remaining   \rCompleted 82.8 MiB/703.1 MiB (74.9 MiB/s) with 1 file(s) remaining   \rCompleted 83.0 MiB/703.1 MiB (75.1 MiB/s) with 1 file(s) remaining   \rCompleted 83.2 MiB/703.1 MiB (75.3 MiB/s) with 1 file(s) remaining   \rCompleted 83.5 MiB/703.1 MiB (75.4 MiB/s) with 1 file(s) remaining   \rCompleted 83.8 MiB/703.1 MiB (75.6 MiB/s) with 1 file(s) remaining   \rCompleted 84.0 MiB/703.1 MiB (75.8 MiB/s) with 1 file(s) remaining   \rCompleted 84.2 MiB/703.1 MiB (76.0 MiB/s) with 1 file(s) remaining   \rCompleted 84.5 MiB/703.1 MiB (76.1 MiB/s) with 1 file(s) remaining   \rCompleted 84.8 MiB/703.1 MiB (76.3 MiB/s) with 1 file(s) remaining   \rCompleted 85.0 MiB/703.1 MiB (76.5 MiB/s) with 1 file(s) remaining   \rCompleted 85.2 MiB/703.1 MiB (76.7 MiB/s) with 1 file(s) remaining   \rCompleted 85.5 MiB/703.1 MiB (76.9 MiB/s) with 1 file(s) remaining   \rCompleted 85.8 MiB/703.1 MiB (77.1 MiB/s) with 1 file(s) remaining   \rCompleted 86.0 MiB/703.1 MiB (77.2 MiB/s) with 1 file(s) remaining   \rCompleted 86.2 MiB/703.1 MiB (77.4 MiB/s) with 1 file(s) remaining   \rCompleted 86.5 MiB/703.1 MiB (77.5 MiB/s) with 1 file(s) remaining   \rCompleted 86.8 MiB/703.1 MiB (77.7 MiB/s) with 1 file(s) remaining   \rCompleted 87.0 MiB/703.1 MiB (77.9 MiB/s) with 1 file(s) remaining   \rCompleted 87.2 MiB/703.1 MiB (78.1 MiB/s) with 1 file(s) remaining   \rCompleted 87.5 MiB/703.1 MiB (78.2 MiB/s) with 1 file(s) remaining   \rCompleted 87.8 MiB/703.1 MiB (78.2 MiB/s) with 1 file(s) remaining   \rCompleted 88.0 MiB/703.1 MiB (78.4 MiB/s) with 1 file(s) remaining   \rCompleted 88.2 MiB/703.1 MiB (78.6 MiB/s) with 1 file(s) remaining   \rCompleted 88.5 MiB/703.1 MiB (78.7 MiB/s) with 1 file(s) remaining   \rCompleted 88.8 MiB/703.1 MiB (78.9 MiB/s) with 1 file(s) remaining   \rCompleted 89.0 MiB/703.1 MiB (79.1 MiB/s) with 1 file(s) remaining   \rCompleted 89.2 MiB/703.1 MiB (79.3 MiB/s) with 1 file(s) remaining   \rCompleted 89.5 MiB/703.1 MiB (79.5 MiB/s) with 1 file(s) remaining   \rCompleted 89.8 MiB/703.1 MiB (79.6 MiB/s) with 1 file(s) remaining   \rCompleted 90.0 MiB/703.1 MiB (79.8 MiB/s) with 1 file(s) remaining   \rCompleted 90.2 MiB/703.1 MiB (80.0 MiB/s) with 1 file(s) remaining   \rCompleted 90.5 MiB/703.1 MiB (80.2 MiB/s) with 1 file(s) remaining   \rCompleted 90.8 MiB/703.1 MiB (80.3 MiB/s) with 1 file(s) remaining   \rCompleted 91.0 MiB/703.1 MiB (80.4 MiB/s) with 1 file(s) remaining   \rCompleted 91.2 MiB/703.1 MiB (80.5 MiB/s) with 1 file(s) remaining   \rCompleted 91.5 MiB/703.1 MiB (80.6 MiB/s) with 1 file(s) remaining   \rCompleted 91.8 MiB/703.1 MiB (80.8 MiB/s) with 1 file(s) remaining   \rCompleted 92.0 MiB/703.1 MiB (81.0 MiB/s) with 1 file(s) remaining   \rCompleted 92.2 MiB/703.1 MiB (81.1 MiB/s) with 1 file(s) remaining   \rCompleted 92.5 MiB/703.1 MiB (81.3 MiB/s) with 1 file(s) remaining   \rCompleted 92.8 MiB/703.1 MiB (81.4 MiB/s) with 1 file(s) remaining   \rCompleted 93.0 MiB/703.1 MiB (81.6 MiB/s) with 1 file(s) remaining   \rCompleted 93.2 MiB/703.1 MiB (81.8 MiB/s) with 1 file(s) remaining   \rCompleted 93.5 MiB/703.1 MiB (82.0 MiB/s) with 1 file(s) remaining   \rCompleted 93.8 MiB/703.1 MiB (82.2 MiB/s) with 1 file(s) remaining   \rCompleted 94.0 MiB/703.1 MiB (82.4 MiB/s) with 1 file(s) remaining   \rCompleted 94.2 MiB/703.1 MiB (82.5 MiB/s) with 1 file(s) remaining   \rCompleted 94.5 MiB/703.1 MiB (82.7 MiB/s) with 1 file(s) remaining   \rCompleted 94.8 MiB/703.1 MiB (82.7 MiB/s) with 1 file(s) remaining   \rCompleted 95.0 MiB/703.1 MiB (82.9 MiB/s) with 1 file(s) remaining   \rCompleted 95.2 MiB/703.1 MiB (83.1 MiB/s) with 1 file(s) remaining   \rCompleted 95.5 MiB/703.1 MiB (83.3 MiB/s) with 1 file(s) remaining   \rCompleted 95.8 MiB/703.1 MiB (83.4 MiB/s) with 1 file(s) remaining   \rCompleted 96.0 MiB/703.1 MiB (83.6 MiB/s) with 1 file(s) remaining   \rCompleted 96.2 MiB/703.1 MiB (83.8 MiB/s) with 1 file(s) remaining   \rCompleted 96.5 MiB/703.1 MiB (84.0 MiB/s) with 1 file(s) remaining   \rCompleted 96.8 MiB/703.1 MiB (84.1 MiB/s) with 1 file(s) remaining   \rCompleted 97.0 MiB/703.1 MiB (84.2 MiB/s) with 1 file(s) remaining   \rCompleted 97.2 MiB/703.1 MiB (84.4 MiB/s) with 1 file(s) remaining   \rCompleted 97.5 MiB/703.1 MiB (84.6 MiB/s) with 1 file(s) remaining   \rCompleted 97.8 MiB/703.1 MiB (84.8 MiB/s) with 1 file(s) remaining   \rCompleted 98.0 MiB/703.1 MiB (84.9 MiB/s) with 1 file(s) remaining   \rCompleted 98.2 MiB/703.1 MiB (85.0 MiB/s) with 1 file(s) remaining   \rCompleted 98.5 MiB/703.1 MiB (85.0 MiB/s) with 1 file(s) remaining   \rCompleted 98.8 MiB/703.1 MiB (85.2 MiB/s) with 1 file(s) remaining   \rCompleted 99.0 MiB/703.1 MiB (85.4 MiB/s) with 1 file(s) remaining   \rCompleted 99.2 MiB/703.1 MiB (85.6 MiB/s) with 1 file(s) remaining   \rCompleted 99.5 MiB/703.1 MiB (85.7 MiB/s) with 1 file(s) remaining   \rCompleted 99.8 MiB/703.1 MiB (85.9 MiB/s) with 1 file(s) remaining   \rCompleted 100.0 MiB/703.1 MiB (86.0 MiB/s) with 1 file(s) remaining  \rCompleted 100.2 MiB/703.1 MiB (86.2 MiB/s) with 1 file(s) remaining  \rCompleted 100.5 MiB/703.1 MiB (86.3 MiB/s) with 1 file(s) remaining  \rCompleted 100.8 MiB/703.1 MiB (86.5 MiB/s) with 1 file(s) remaining  \rCompleted 101.0 MiB/703.1 MiB (86.7 MiB/s) with 1 file(s) remaining  \rCompleted 101.2 MiB/703.1 MiB (86.9 MiB/s) with 1 file(s) remaining  \rCompleted 101.5 MiB/703.1 MiB (87.0 MiB/s) with 1 file(s) remaining  \rCompleted 101.8 MiB/703.1 MiB (87.2 MiB/s) with 1 file(s) remaining  \rCompleted 102.0 MiB/703.1 MiB (87.3 MiB/s) with 1 file(s) remaining  \rCompleted 102.2 MiB/703.1 MiB (87.5 MiB/s) with 1 file(s) remaining  \rCompleted 102.5 MiB/703.1 MiB (87.7 MiB/s) with 1 file(s) remaining  \rCompleted 102.8 MiB/703.1 MiB (87.9 MiB/s) with 1 file(s) remaining  \rCompleted 103.0 MiB/703.1 MiB (88.0 MiB/s) with 1 file(s) remaining  \rCompleted 103.2 MiB/703.1 MiB (88.2 MiB/s) with 1 file(s) remaining  \rCompleted 103.5 MiB/703.1 MiB (88.3 MiB/s) with 1 file(s) remaining  \rCompleted 103.8 MiB/703.1 MiB (88.5 MiB/s) with 1 file(s) remaining  \rCompleted 104.0 MiB/703.1 MiB (88.6 MiB/s) with 1 file(s) remaining  \rCompleted 104.2 MiB/703.1 MiB (88.8 MiB/s) with 1 file(s) remaining  \rCompleted 104.5 MiB/703.1 MiB (89.0 MiB/s) with 1 file(s) remaining  \rCompleted 104.8 MiB/703.1 MiB (89.2 MiB/s) with 1 file(s) remaining  \rCompleted 105.0 MiB/703.1 MiB (89.3 MiB/s) with 1 file(s) remaining  \rCompleted 105.2 MiB/703.1 MiB (89.5 MiB/s) with 1 file(s) remaining  \rCompleted 105.5 MiB/703.1 MiB (89.7 MiB/s) with 1 file(s) remaining  \rCompleted 105.8 MiB/703.1 MiB (89.9 MiB/s) with 1 file(s) remaining  \rCompleted 106.0 MiB/703.1 MiB (90.0 MiB/s) with 1 file(s) remaining  \rCompleted 106.2 MiB/703.1 MiB (90.2 MiB/s) with 1 file(s) remaining  \rCompleted 106.5 MiB/703.1 MiB (90.3 MiB/s) with 1 file(s) remaining  \rCompleted 106.8 MiB/703.1 MiB (90.5 MiB/s) with 1 file(s) remaining  \rCompleted 107.0 MiB/703.1 MiB (90.6 MiB/s) with 1 file(s) remaining  \rCompleted 107.2 MiB/703.1 MiB (90.8 MiB/s) with 1 file(s) remaining  \rCompleted 107.5 MiB/703.1 MiB (91.0 MiB/s) with 1 file(s) remaining  \rCompleted 107.8 MiB/703.1 MiB (91.2 MiB/s) with 1 file(s) remaining  \rCompleted 108.0 MiB/703.1 MiB (91.3 MiB/s) with 1 file(s) remaining  \rCompleted 108.2 MiB/703.1 MiB (91.5 MiB/s) with 1 file(s) remaining  \rCompleted 108.5 MiB/703.1 MiB (91.7 MiB/s) with 1 file(s) remaining  \rCompleted 108.8 MiB/703.1 MiB (91.8 MiB/s) with 1 file(s) remaining  \rCompleted 109.0 MiB/703.1 MiB (92.0 MiB/s) with 1 file(s) remaining  \rCompleted 109.2 MiB/703.1 MiB (92.1 MiB/s) with 1 file(s) remaining  \rCompleted 109.5 MiB/703.1 MiB (92.3 MiB/s) with 1 file(s) remaining  \rCompleted 109.8 MiB/703.1 MiB (92.4 MiB/s) with 1 file(s) remaining  \rCompleted 110.0 MiB/703.1 MiB (92.5 MiB/s) with 1 file(s) remaining  \rCompleted 110.2 MiB/703.1 MiB (92.7 MiB/s) with 1 file(s) remaining  \rCompleted 110.5 MiB/703.1 MiB (92.9 MiB/s) with 1 file(s) remaining  \rCompleted 110.8 MiB/703.1 MiB (93.0 MiB/s) with 1 file(s) remaining  \rCompleted 111.0 MiB/703.1 MiB (93.1 MiB/s) with 1 file(s) remaining  \rCompleted 111.2 MiB/703.1 MiB (93.2 MiB/s) with 1 file(s) remaining  \rCompleted 111.5 MiB/703.1 MiB (93.4 MiB/s) with 1 file(s) remaining  \rCompleted 111.8 MiB/703.1 MiB (93.5 MiB/s) with 1 file(s) remaining  \rCompleted 112.0 MiB/703.1 MiB (93.5 MiB/s) with 1 file(s) remaining  \rCompleted 112.2 MiB/703.1 MiB (92.9 MiB/s) with 1 file(s) remaining  \rCompleted 112.5 MiB/703.1 MiB (93.1 MiB/s) with 1 file(s) remaining  \rCompleted 112.8 MiB/703.1 MiB (93.2 MiB/s) with 1 file(s) remaining  \rCompleted 113.0 MiB/703.1 MiB (93.4 MiB/s) with 1 file(s) remaining  \rCompleted 113.2 MiB/703.1 MiB (93.5 MiB/s) with 1 file(s) remaining  \rCompleted 113.5 MiB/703.1 MiB (93.7 MiB/s) with 1 file(s) remaining  \rCompleted 113.8 MiB/703.1 MiB (93.8 MiB/s) with 1 file(s) remaining  \rCompleted 114.0 MiB/703.1 MiB (94.0 MiB/s) with 1 file(s) remaining  \rCompleted 114.2 MiB/703.1 MiB (94.1 MiB/s) with 1 file(s) remaining  \rCompleted 114.5 MiB/703.1 MiB (94.3 MiB/s) with 1 file(s) remaining  \rCompleted 114.8 MiB/703.1 MiB (94.4 MiB/s) with 1 file(s) remaining  \rCompleted 115.0 MiB/703.1 MiB (94.5 MiB/s) with 1 file(s) remaining  \rCompleted 115.2 MiB/703.1 MiB (94.7 MiB/s) with 1 file(s) remaining  \rCompleted 115.5 MiB/703.1 MiB (94.8 MiB/s) with 1 file(s) remaining  \rCompleted 115.8 MiB/703.1 MiB (95.0 MiB/s) with 1 file(s) remaining  \rCompleted 116.0 MiB/703.1 MiB (95.1 MiB/s) with 1 file(s) remaining  \rCompleted 116.2 MiB/703.1 MiB (95.3 MiB/s) with 1 file(s) remaining  \rCompleted 116.5 MiB/703.1 MiB (95.4 MiB/s) with 1 file(s) remaining  \rCompleted 116.8 MiB/703.1 MiB (95.6 MiB/s) with 1 file(s) remaining  \rCompleted 117.0 MiB/703.1 MiB (95.4 MiB/s) with 1 file(s) remaining  \rCompleted 117.2 MiB/703.1 MiB (95.6 MiB/s) with 1 file(s) remaining  \rCompleted 117.5 MiB/703.1 MiB (95.7 MiB/s) with 1 file(s) remaining  \rCompleted 117.8 MiB/703.1 MiB (95.9 MiB/s) with 1 file(s) remaining  \rCompleted 118.0 MiB/703.1 MiB (96.0 MiB/s) with 1 file(s) remaining  \rCompleted 118.2 MiB/703.1 MiB (96.2 MiB/s) with 1 file(s) remaining  \rCompleted 118.5 MiB/703.1 MiB (96.3 MiB/s) with 1 file(s) remaining  \rCompleted 118.8 MiB/703.1 MiB (96.5 MiB/s) with 1 file(s) remaining  \rCompleted 119.0 MiB/703.1 MiB (96.6 MiB/s) with 1 file(s) remaining  \rCompleted 119.2 MiB/703.1 MiB (96.8 MiB/s) with 1 file(s) remaining  \rCompleted 119.5 MiB/703.1 MiB (96.9 MiB/s) with 1 file(s) remaining  \rCompleted 119.8 MiB/703.1 MiB (97.1 MiB/s) with 1 file(s) remaining  \rCompleted 120.0 MiB/703.1 MiB (97.2 MiB/s) with 1 file(s) remaining  \rCompleted 120.2 MiB/703.1 MiB (97.4 MiB/s) with 1 file(s) remaining  \rCompleted 120.5 MiB/703.1 MiB (97.5 MiB/s) with 1 file(s) remaining  \rCompleted 120.8 MiB/703.1 MiB (97.6 MiB/s) with 1 file(s) remaining  \rCompleted 121.0 MiB/703.1 MiB (97.8 MiB/s) with 1 file(s) remaining  \rCompleted 121.2 MiB/703.1 MiB (97.9 MiB/s) with 1 file(s) remaining  \rCompleted 121.5 MiB/703.1 MiB (98.0 MiB/s) with 1 file(s) remaining  \rCompleted 121.8 MiB/703.1 MiB (98.1 MiB/s) with 1 file(s) remaining  \rCompleted 122.0 MiB/703.1 MiB (98.2 MiB/s) with 1 file(s) remaining  \rCompleted 122.2 MiB/703.1 MiB (98.4 MiB/s) with 1 file(s) remaining  \rCompleted 122.5 MiB/703.1 MiB (98.5 MiB/s) with 1 file(s) remaining  \rCompleted 122.8 MiB/703.1 MiB (98.6 MiB/s) with 1 file(s) remaining  \rCompleted 123.0 MiB/703.1 MiB (98.7 MiB/s) with 1 file(s) remaining  \rCompleted 123.2 MiB/703.1 MiB (98.9 MiB/s) with 1 file(s) remaining  \rCompleted 123.5 MiB/703.1 MiB (99.1 MiB/s) with 1 file(s) remaining  \rCompleted 123.8 MiB/703.1 MiB (99.1 MiB/s) with 1 file(s) remaining  \rCompleted 124.0 MiB/703.1 MiB (99.2 MiB/s) with 1 file(s) remaining  \rCompleted 124.2 MiB/703.1 MiB (99.4 MiB/s) with 1 file(s) remaining  \rCompleted 124.5 MiB/703.1 MiB (99.6 MiB/s) with 1 file(s) remaining  \rCompleted 124.8 MiB/703.1 MiB (99.7 MiB/s) with 1 file(s) remaining  \rCompleted 125.0 MiB/703.1 MiB (99.9 MiB/s) with 1 file(s) remaining  \rCompleted 125.2 MiB/703.1 MiB (100.0 MiB/s) with 1 file(s) remaining \rCompleted 125.5 MiB/703.1 MiB (100.1 MiB/s) with 1 file(s) remaining \rCompleted 125.8 MiB/703.1 MiB (100.2 MiB/s) with 1 file(s) remaining \rCompleted 126.0 MiB/703.1 MiB (100.4 MiB/s) with 1 file(s) remaining \rCompleted 126.2 MiB/703.1 MiB (100.5 MiB/s) with 1 file(s) remaining \rCompleted 126.5 MiB/703.1 MiB (100.7 MiB/s) with 1 file(s) remaining \rCompleted 126.8 MiB/703.1 MiB (100.8 MiB/s) with 1 file(s) remaining \rCompleted 127.0 MiB/703.1 MiB (101.0 MiB/s) with 1 file(s) remaining \rCompleted 127.2 MiB/703.1 MiB (101.1 MiB/s) with 1 file(s) remaining \rCompleted 127.5 MiB/703.1 MiB (101.3 MiB/s) with 1 file(s) remaining \rCompleted 127.8 MiB/703.1 MiB (101.4 MiB/s) with 1 file(s) remaining \rCompleted 128.0 MiB/703.1 MiB (101.6 MiB/s) with 1 file(s) remaining \rCompleted 128.2 MiB/703.1 MiB (101.7 MiB/s) with 1 file(s) remaining \rCompleted 128.5 MiB/703.1 MiB (101.8 MiB/s) with 1 file(s) remaining \rCompleted 128.8 MiB/703.1 MiB (102.0 MiB/s) with 1 file(s) remaining \rCompleted 129.0 MiB/703.1 MiB (102.1 MiB/s) with 1 file(s) remaining \rCompleted 129.2 MiB/703.1 MiB (102.3 MiB/s) with 1 file(s) remaining \rCompleted 129.5 MiB/703.1 MiB (102.4 MiB/s) with 1 file(s) remaining \rCompleted 129.8 MiB/703.1 MiB (102.6 MiB/s) with 1 file(s) remaining \rCompleted 130.0 MiB/703.1 MiB (102.7 MiB/s) with 1 file(s) remaining \rCompleted 130.2 MiB/703.1 MiB (102.8 MiB/s) with 1 file(s) remaining \rCompleted 130.5 MiB/703.1 MiB (103.0 MiB/s) with 1 file(s) remaining \rCompleted 130.8 MiB/703.1 MiB (103.1 MiB/s) with 1 file(s) remaining \rCompleted 131.0 MiB/703.1 MiB (103.3 MiB/s) with 1 file(s) remaining \rCompleted 131.2 MiB/703.1 MiB (103.5 MiB/s) with 1 file(s) remaining \rCompleted 131.5 MiB/703.1 MiB (103.6 MiB/s) with 1 file(s) remaining \rCompleted 131.8 MiB/703.1 MiB (103.8 MiB/s) with 1 file(s) remaining \rCompleted 132.0 MiB/703.1 MiB (103.9 MiB/s) with 1 file(s) remaining \rCompleted 132.2 MiB/703.1 MiB (104.0 MiB/s) with 1 file(s) remaining \rCompleted 132.5 MiB/703.1 MiB (104.2 MiB/s) with 1 file(s) remaining \rCompleted 132.8 MiB/703.1 MiB (104.3 MiB/s) with 1 file(s) remaining \rCompleted 133.0 MiB/703.1 MiB (104.5 MiB/s) with 1 file(s) remaining \rCompleted 133.2 MiB/703.1 MiB (104.5 MiB/s) with 1 file(s) remaining \rCompleted 133.5 MiB/703.1 MiB (104.6 MiB/s) with 1 file(s) remaining \rCompleted 133.8 MiB/703.1 MiB (104.8 MiB/s) with 1 file(s) remaining \rCompleted 134.0 MiB/703.1 MiB (105.0 MiB/s) with 1 file(s) remaining \rCompleted 134.2 MiB/703.1 MiB (105.2 MiB/s) with 1 file(s) remaining \rCompleted 134.5 MiB/703.1 MiB (105.2 MiB/s) with 1 file(s) remaining \rCompleted 134.8 MiB/703.1 MiB (105.4 MiB/s) with 1 file(s) remaining \rCompleted 135.0 MiB/703.1 MiB (105.5 MiB/s) with 1 file(s) remaining \rCompleted 135.2 MiB/703.1 MiB (105.7 MiB/s) with 1 file(s) remaining \rCompleted 135.5 MiB/703.1 MiB (105.8 MiB/s) with 1 file(s) remaining \rCompleted 135.8 MiB/703.1 MiB (106.0 MiB/s) with 1 file(s) remaining \rCompleted 136.0 MiB/703.1 MiB (105.9 MiB/s) with 1 file(s) remaining \rCompleted 136.2 MiB/703.1 MiB (106.0 MiB/s) with 1 file(s) remaining \rCompleted 136.5 MiB/703.1 MiB (106.1 MiB/s) with 1 file(s) remaining \rCompleted 136.8 MiB/703.1 MiB (106.2 MiB/s) with 1 file(s) remaining \rCompleted 137.0 MiB/703.1 MiB (106.3 MiB/s) with 1 file(s) remaining \rCompleted 137.2 MiB/703.1 MiB (106.5 MiB/s) with 1 file(s) remaining \rCompleted 137.5 MiB/703.1 MiB (106.7 MiB/s) with 1 file(s) remaining \rCompleted 137.8 MiB/703.1 MiB (106.8 MiB/s) with 1 file(s) remaining \rCompleted 138.0 MiB/703.1 MiB (106.8 MiB/s) with 1 file(s) remaining \rCompleted 138.2 MiB/703.1 MiB (107.0 MiB/s) with 1 file(s) remaining \rCompleted 138.5 MiB/703.1 MiB (107.1 MiB/s) with 1 file(s) remaining \rCompleted 138.8 MiB/703.1 MiB (107.3 MiB/s) with 1 file(s) remaining \rCompleted 139.0 MiB/703.1 MiB (107.4 MiB/s) with 1 file(s) remaining \rCompleted 139.2 MiB/703.1 MiB (107.5 MiB/s) with 1 file(s) remaining \rCompleted 139.5 MiB/703.1 MiB (107.7 MiB/s) with 1 file(s) remaining \rCompleted 139.8 MiB/703.1 MiB (107.8 MiB/s) with 1 file(s) remaining \rCompleted 140.0 MiB/703.1 MiB (107.9 MiB/s) with 1 file(s) remaining \rCompleted 140.2 MiB/703.1 MiB (108.0 MiB/s) with 1 file(s) remaining \rCompleted 140.5 MiB/703.1 MiB (108.2 MiB/s) with 1 file(s) remaining \rCompleted 140.8 MiB/703.1 MiB (108.3 MiB/s) with 1 file(s) remaining \rCompleted 141.0 MiB/703.1 MiB (108.5 MiB/s) with 1 file(s) remaining \rCompleted 141.2 MiB/703.1 MiB (108.6 MiB/s) with 1 file(s) remaining \rCompleted 141.5 MiB/703.1 MiB (108.7 MiB/s) with 1 file(s) remaining \rCompleted 141.8 MiB/703.1 MiB (108.8 MiB/s) with 1 file(s) remaining \rCompleted 142.0 MiB/703.1 MiB (109.0 MiB/s) with 1 file(s) remaining \rCompleted 142.2 MiB/703.1 MiB (109.1 MiB/s) with 1 file(s) remaining \rCompleted 142.5 MiB/703.1 MiB (109.3 MiB/s) with 1 file(s) remaining \rCompleted 142.8 MiB/703.1 MiB (109.4 MiB/s) with 1 file(s) remaining \rCompleted 143.0 MiB/703.1 MiB (109.6 MiB/s) with 1 file(s) remaining \rCompleted 143.2 MiB/703.1 MiB (109.7 MiB/s) with 1 file(s) remaining \rCompleted 143.5 MiB/703.1 MiB (109.8 MiB/s) with 1 file(s) remaining \rCompleted 143.8 MiB/703.1 MiB (109.9 MiB/s) with 1 file(s) remaining \rCompleted 144.0 MiB/703.1 MiB (110.1 MiB/s) with 1 file(s) remaining \rCompleted 144.2 MiB/703.1 MiB (110.3 MiB/s) with 1 file(s) remaining \rCompleted 144.5 MiB/703.1 MiB (110.4 MiB/s) with 1 file(s) remaining \rCompleted 144.8 MiB/703.1 MiB (110.6 MiB/s) with 1 file(s) remaining \rCompleted 145.0 MiB/703.1 MiB (110.6 MiB/s) with 1 file(s) remaining \rCompleted 145.2 MiB/703.1 MiB (110.8 MiB/s) with 1 file(s) remaining \rCompleted 145.5 MiB/703.1 MiB (110.9 MiB/s) with 1 file(s) remaining \rCompleted 145.8 MiB/703.1 MiB (111.1 MiB/s) with 1 file(s) remaining \rCompleted 146.0 MiB/703.1 MiB (111.2 MiB/s) with 1 file(s) remaining \rCompleted 146.2 MiB/703.1 MiB (111.3 MiB/s) with 1 file(s) remaining \rCompleted 146.5 MiB/703.1 MiB (111.5 MiB/s) with 1 file(s) remaining \rCompleted 146.8 MiB/703.1 MiB (111.7 MiB/s) with 1 file(s) remaining \rCompleted 147.0 MiB/703.1 MiB (111.6 MiB/s) with 1 file(s) remaining \rCompleted 147.2 MiB/703.1 MiB (111.8 MiB/s) with 1 file(s) remaining \rCompleted 147.5 MiB/703.1 MiB (111.9 MiB/s) with 1 file(s) remaining \rCompleted 147.8 MiB/703.1 MiB (112.0 MiB/s) with 1 file(s) remaining \rCompleted 148.0 MiB/703.1 MiB (112.2 MiB/s) with 1 file(s) remaining \rCompleted 148.2 MiB/703.1 MiB (112.3 MiB/s) with 1 file(s) remaining \rCompleted 148.5 MiB/703.1 MiB (112.4 MiB/s) with 1 file(s) remaining \rCompleted 148.8 MiB/703.1 MiB (112.5 MiB/s) with 1 file(s) remaining \rCompleted 149.0 MiB/703.1 MiB (112.7 MiB/s) with 1 file(s) remaining \rCompleted 149.2 MiB/703.1 MiB (112.8 MiB/s) with 1 file(s) remaining \rCompleted 149.5 MiB/703.1 MiB (113.0 MiB/s) with 1 file(s) remaining \rCompleted 149.8 MiB/703.1 MiB (113.0 MiB/s) with 1 file(s) remaining \rCompleted 150.0 MiB/703.1 MiB (113.2 MiB/s) with 1 file(s) remaining \rCompleted 150.2 MiB/703.1 MiB (113.3 MiB/s) with 1 file(s) remaining \rCompleted 150.5 MiB/703.1 MiB (113.3 MiB/s) with 1 file(s) remaining \rCompleted 150.8 MiB/703.1 MiB (113.5 MiB/s) with 1 file(s) remaining \rCompleted 151.0 MiB/703.1 MiB (113.6 MiB/s) with 1 file(s) remaining \rCompleted 151.2 MiB/703.1 MiB (113.8 MiB/s) with 1 file(s) remaining \rCompleted 151.5 MiB/703.1 MiB (114.0 MiB/s) with 1 file(s) remaining \rCompleted 151.8 MiB/703.1 MiB (114.1 MiB/s) with 1 file(s) remaining \rCompleted 152.0 MiB/703.1 MiB (114.3 MiB/s) with 1 file(s) remaining \rCompleted 152.2 MiB/703.1 MiB (114.4 MiB/s) with 1 file(s) remaining \rCompleted 152.5 MiB/703.1 MiB (114.5 MiB/s) with 1 file(s) remaining \rCompleted 152.8 MiB/703.1 MiB (114.5 MiB/s) with 1 file(s) remaining \rCompleted 153.0 MiB/703.1 MiB (114.7 MiB/s) with 1 file(s) remaining \rCompleted 153.2 MiB/703.1 MiB (114.8 MiB/s) with 1 file(s) remaining \rCompleted 153.5 MiB/703.1 MiB (114.9 MiB/s) with 1 file(s) remaining \rCompleted 153.8 MiB/703.1 MiB (115.0 MiB/s) with 1 file(s) remaining \rCompleted 154.0 MiB/703.1 MiB (115.2 MiB/s) with 1 file(s) remaining \rCompleted 154.2 MiB/703.1 MiB (115.3 MiB/s) with 1 file(s) remaining \rCompleted 154.5 MiB/703.1 MiB (115.5 MiB/s) with 1 file(s) remaining \rCompleted 154.8 MiB/703.1 MiB (115.6 MiB/s) with 1 file(s) remaining \rCompleted 155.0 MiB/703.1 MiB (115.8 MiB/s) with 1 file(s) remaining \rCompleted 155.2 MiB/703.1 MiB (115.9 MiB/s) with 1 file(s) remaining \rCompleted 155.5 MiB/703.1 MiB (115.9 MiB/s) with 1 file(s) remaining \rCompleted 155.8 MiB/703.1 MiB (116.0 MiB/s) with 1 file(s) remaining \rCompleted 156.0 MiB/703.1 MiB (116.2 MiB/s) with 1 file(s) remaining \rCompleted 156.2 MiB/703.1 MiB (116.3 MiB/s) with 1 file(s) remaining \rCompleted 156.5 MiB/703.1 MiB (116.4 MiB/s) with 1 file(s) remaining \rCompleted 156.8 MiB/703.1 MiB (116.6 MiB/s) with 1 file(s) remaining \rCompleted 157.0 MiB/703.1 MiB (116.7 MiB/s) with 1 file(s) remaining \rCompleted 157.2 MiB/703.1 MiB (116.8 MiB/s) with 1 file(s) remaining \rCompleted 157.5 MiB/703.1 MiB (116.8 MiB/s) with 1 file(s) remaining \rCompleted 157.8 MiB/703.1 MiB (117.0 MiB/s) with 1 file(s) remaining \rCompleted 158.0 MiB/703.1 MiB (117.1 MiB/s) with 1 file(s) remaining \rCompleted 158.2 MiB/703.1 MiB (117.2 MiB/s) with 1 file(s) remaining \rCompleted 158.5 MiB/703.1 MiB (117.3 MiB/s) with 1 file(s) remaining \rCompleted 158.8 MiB/703.1 MiB (117.4 MiB/s) with 1 file(s) remaining \rCompleted 159.0 MiB/703.1 MiB (117.6 MiB/s) with 1 file(s) remaining \rCompleted 159.2 MiB/703.1 MiB (117.7 MiB/s) with 1 file(s) remaining \rCompleted 159.5 MiB/703.1 MiB (117.8 MiB/s) with 1 file(s) remaining \rCompleted 159.8 MiB/703.1 MiB (118.0 MiB/s) with 1 file(s) remaining \rCompleted 160.0 MiB/703.1 MiB (118.1 MiB/s) with 1 file(s) remaining \rCompleted 160.2 MiB/703.1 MiB (118.2 MiB/s) with 1 file(s) remaining \rCompleted 160.5 MiB/703.1 MiB (118.3 MiB/s) with 1 file(s) remaining \rCompleted 160.8 MiB/703.1 MiB (118.4 MiB/s) with 1 file(s) remaining \rCompleted 161.0 MiB/703.1 MiB (118.6 MiB/s) with 1 file(s) remaining \rCompleted 161.2 MiB/703.1 MiB (118.7 MiB/s) with 1 file(s) remaining \rCompleted 161.5 MiB/703.1 MiB (118.8 MiB/s) with 1 file(s) remaining \rCompleted 161.8 MiB/703.1 MiB (119.0 MiB/s) with 1 file(s) remaining \rCompleted 162.0 MiB/703.1 MiB (119.1 MiB/s) with 1 file(s) remaining \rCompleted 162.2 MiB/703.1 MiB (119.2 MiB/s) with 1 file(s) remaining \rCompleted 162.5 MiB/703.1 MiB (119.4 MiB/s) with 1 file(s) remaining \rCompleted 162.8 MiB/703.1 MiB (119.5 MiB/s) with 1 file(s) remaining \rCompleted 163.0 MiB/703.1 MiB (119.6 MiB/s) with 1 file(s) remaining \rCompleted 163.2 MiB/703.1 MiB (119.7 MiB/s) with 1 file(s) remaining \rCompleted 163.5 MiB/703.1 MiB (119.9 MiB/s) with 1 file(s) remaining \rCompleted 163.8 MiB/703.1 MiB (120.0 MiB/s) with 1 file(s) remaining \rCompleted 164.0 MiB/703.1 MiB (120.2 MiB/s) with 1 file(s) remaining \rCompleted 164.2 MiB/703.1 MiB (120.4 MiB/s) with 1 file(s) remaining \rCompleted 164.5 MiB/703.1 MiB (120.3 MiB/s) with 1 file(s) remaining \rCompleted 164.8 MiB/703.1 MiB (120.5 MiB/s) with 1 file(s) remaining \rCompleted 165.0 MiB/703.1 MiB (120.6 MiB/s) with 1 file(s) remaining \rCompleted 165.2 MiB/703.1 MiB (120.8 MiB/s) with 1 file(s) remaining \rCompleted 165.5 MiB/703.1 MiB (120.9 MiB/s) with 1 file(s) remaining \rCompleted 165.8 MiB/703.1 MiB (121.1 MiB/s) with 1 file(s) remaining \rCompleted 166.0 MiB/703.1 MiB (121.1 MiB/s) with 1 file(s) remaining \rCompleted 166.2 MiB/703.1 MiB (121.3 MiB/s) with 1 file(s) remaining \rCompleted 166.5 MiB/703.1 MiB (121.4 MiB/s) with 1 file(s) remaining \rCompleted 166.8 MiB/703.1 MiB (121.5 MiB/s) with 1 file(s) remaining \rCompleted 167.0 MiB/703.1 MiB (121.5 MiB/s) with 1 file(s) remaining \rCompleted 167.2 MiB/703.1 MiB (121.6 MiB/s) with 1 file(s) remaining \rCompleted 167.5 MiB/703.1 MiB (121.8 MiB/s) with 1 file(s) remaining \rCompleted 167.8 MiB/703.1 MiB (122.0 MiB/s) with 1 file(s) remaining \rCompleted 168.0 MiB/703.1 MiB (122.1 MiB/s) with 1 file(s) remaining \rCompleted 168.2 MiB/703.1 MiB (122.3 MiB/s) with 1 file(s) remaining \rCompleted 168.5 MiB/703.1 MiB (122.3 MiB/s) with 1 file(s) remaining \rCompleted 168.8 MiB/703.1 MiB (122.5 MiB/s) with 1 file(s) remaining \rCompleted 169.0 MiB/703.1 MiB (122.7 MiB/s) with 1 file(s) remaining \rCompleted 169.2 MiB/703.1 MiB (122.7 MiB/s) with 1 file(s) remaining \rCompleted 169.5 MiB/703.1 MiB (122.8 MiB/s) with 1 file(s) remaining \rCompleted 169.8 MiB/703.1 MiB (123.0 MiB/s) with 1 file(s) remaining \rCompleted 170.0 MiB/703.1 MiB (123.1 MiB/s) with 1 file(s) remaining \rCompleted 170.2 MiB/703.1 MiB (123.3 MiB/s) with 1 file(s) remaining \rCompleted 170.5 MiB/703.1 MiB (123.3 MiB/s) with 1 file(s) remaining \rCompleted 170.8 MiB/703.1 MiB (123.5 MiB/s) with 1 file(s) remaining \rCompleted 171.0 MiB/703.1 MiB (123.6 MiB/s) with 1 file(s) remaining \rCompleted 171.2 MiB/703.1 MiB (123.7 MiB/s) with 1 file(s) remaining \rCompleted 171.5 MiB/703.1 MiB (123.9 MiB/s) with 1 file(s) remaining \rCompleted 171.8 MiB/703.1 MiB (124.0 MiB/s) with 1 file(s) remaining \rCompleted 172.0 MiB/703.1 MiB (124.1 MiB/s) with 1 file(s) remaining \rCompleted 172.2 MiB/703.1 MiB (124.2 MiB/s) with 1 file(s) remaining \rCompleted 172.5 MiB/703.1 MiB (124.3 MiB/s) with 1 file(s) remaining \rCompleted 172.8 MiB/703.1 MiB (124.5 MiB/s) with 1 file(s) remaining \rCompleted 173.0 MiB/703.1 MiB (124.6 MiB/s) with 1 file(s) remaining \rCompleted 173.2 MiB/703.1 MiB (124.7 MiB/s) with 1 file(s) remaining \rCompleted 173.5 MiB/703.1 MiB (124.9 MiB/s) with 1 file(s) remaining \rCompleted 173.8 MiB/703.1 MiB (125.0 MiB/s) with 1 file(s) remaining \rCompleted 174.0 MiB/703.1 MiB (125.1 MiB/s) with 1 file(s) remaining \rCompleted 174.2 MiB/703.1 MiB (125.2 MiB/s) with 1 file(s) remaining \rCompleted 174.5 MiB/703.1 MiB (125.3 MiB/s) with 1 file(s) remaining \rCompleted 174.8 MiB/703.1 MiB (125.4 MiB/s) with 1 file(s) remaining \rCompleted 175.0 MiB/703.1 MiB (125.5 MiB/s) with 1 file(s) remaining \rCompleted 175.2 MiB/703.1 MiB (125.7 MiB/s) with 1 file(s) remaining \rCompleted 175.5 MiB/703.1 MiB (125.8 MiB/s) with 1 file(s) remaining \rCompleted 175.8 MiB/703.1 MiB (125.9 MiB/s) with 1 file(s) remaining \rCompleted 176.0 MiB/703.1 MiB (126.0 MiB/s) with 1 file(s) remaining \rCompleted 176.2 MiB/703.1 MiB (126.1 MiB/s) with 1 file(s) remaining \rCompleted 176.5 MiB/703.1 MiB (126.2 MiB/s) with 1 file(s) remaining \rCompleted 176.8 MiB/703.1 MiB (126.3 MiB/s) with 1 file(s) remaining \rCompleted 177.0 MiB/703.1 MiB (126.3 MiB/s) with 1 file(s) remaining \rCompleted 177.2 MiB/703.1 MiB (126.5 MiB/s) with 1 file(s) remaining \rCompleted 177.5 MiB/703.1 MiB (126.6 MiB/s) with 1 file(s) remaining \rCompleted 177.8 MiB/703.1 MiB (126.7 MiB/s) with 1 file(s) remaining \rCompleted 178.0 MiB/703.1 MiB (126.9 MiB/s) with 1 file(s) remaining \rCompleted 178.2 MiB/703.1 MiB (126.9 MiB/s) with 1 file(s) remaining \rCompleted 178.5 MiB/703.1 MiB (127.1 MiB/s) with 1 file(s) remaining \rCompleted 178.8 MiB/703.1 MiB (127.2 MiB/s) with 1 file(s) remaining \rCompleted 179.0 MiB/703.1 MiB (127.3 MiB/s) with 1 file(s) remaining \rCompleted 179.2 MiB/703.1 MiB (127.4 MiB/s) with 1 file(s) remaining \rCompleted 179.5 MiB/703.1 MiB (127.5 MiB/s) with 1 file(s) remaining \rCompleted 179.8 MiB/703.1 MiB (127.7 MiB/s) with 1 file(s) remaining \rCompleted 180.0 MiB/703.1 MiB (127.8 MiB/s) with 1 file(s) remaining \rCompleted 180.2 MiB/703.1 MiB (127.9 MiB/s) with 1 file(s) remaining \rCompleted 180.5 MiB/703.1 MiB (128.0 MiB/s) with 1 file(s) remaining \rCompleted 180.8 MiB/703.1 MiB (128.1 MiB/s) with 1 file(s) remaining \rCompleted 181.0 MiB/703.1 MiB (128.2 MiB/s) with 1 file(s) remaining \rCompleted 181.2 MiB/703.1 MiB (128.4 MiB/s) with 1 file(s) remaining \rCompleted 181.5 MiB/703.1 MiB (128.5 MiB/s) with 1 file(s) remaining \rCompleted 181.8 MiB/703.1 MiB (128.6 MiB/s) with 1 file(s) remaining \rCompleted 182.0 MiB/703.1 MiB (128.7 MiB/s) with 1 file(s) remaining \rCompleted 182.2 MiB/703.1 MiB (128.8 MiB/s) with 1 file(s) remaining \rCompleted 182.5 MiB/703.1 MiB (128.9 MiB/s) with 1 file(s) remaining \rCompleted 182.8 MiB/703.1 MiB (128.8 MiB/s) with 1 file(s) remaining \rCompleted 183.0 MiB/703.1 MiB (129.0 MiB/s) with 1 file(s) remaining \rCompleted 183.2 MiB/703.1 MiB (129.0 MiB/s) with 1 file(s) remaining \rCompleted 183.5 MiB/703.1 MiB (129.1 MiB/s) with 1 file(s) remaining \rCompleted 183.8 MiB/703.1 MiB (129.3 MiB/s) with 1 file(s) remaining \rCompleted 184.0 MiB/703.1 MiB (129.4 MiB/s) with 1 file(s) remaining \rCompleted 184.2 MiB/703.1 MiB (129.4 MiB/s) with 1 file(s) remaining \rCompleted 184.5 MiB/703.1 MiB (129.5 MiB/s) with 1 file(s) remaining \rCompleted 184.8 MiB/703.1 MiB (129.6 MiB/s) with 1 file(s) remaining \rCompleted 185.0 MiB/703.1 MiB (129.7 MiB/s) with 1 file(s) remaining \rCompleted 185.2 MiB/703.1 MiB (129.9 MiB/s) with 1 file(s) remaining \rCompleted 185.5 MiB/703.1 MiB (130.0 MiB/s) with 1 file(s) remaining \rCompleted 185.8 MiB/703.1 MiB (130.1 MiB/s) with 1 file(s) remaining \rCompleted 186.0 MiB/703.1 MiB (130.2 MiB/s) with 1 file(s) remaining \rCompleted 186.2 MiB/703.1 MiB (130.4 MiB/s) with 1 file(s) remaining \rCompleted 186.5 MiB/703.1 MiB (130.5 MiB/s) with 1 file(s) remaining \rCompleted 186.8 MiB/703.1 MiB (130.6 MiB/s) with 1 file(s) remaining \rCompleted 187.0 MiB/703.1 MiB (130.7 MiB/s) with 1 file(s) remaining \rCompleted 187.2 MiB/703.1 MiB (130.7 MiB/s) with 1 file(s) remaining \rCompleted 187.5 MiB/703.1 MiB (130.9 MiB/s) with 1 file(s) remaining \rCompleted 187.8 MiB/703.1 MiB (131.0 MiB/s) with 1 file(s) remaining \rCompleted 188.0 MiB/703.1 MiB (131.1 MiB/s) with 1 file(s) remaining \rCompleted 188.2 MiB/703.1 MiB (131.3 MiB/s) with 1 file(s) remaining \rCompleted 188.5 MiB/703.1 MiB (131.4 MiB/s) with 1 file(s) remaining \rCompleted 188.8 MiB/703.1 MiB (131.5 MiB/s) with 1 file(s) remaining \rCompleted 189.0 MiB/703.1 MiB (131.7 MiB/s) with 1 file(s) remaining \rCompleted 189.2 MiB/703.1 MiB (131.8 MiB/s) with 1 file(s) remaining \rCompleted 189.5 MiB/703.1 MiB (131.9 MiB/s) with 1 file(s) remaining \rCompleted 189.8 MiB/703.1 MiB (132.0 MiB/s) with 1 file(s) remaining \rCompleted 190.0 MiB/703.1 MiB (132.1 MiB/s) with 1 file(s) remaining \rCompleted 190.2 MiB/703.1 MiB (132.2 MiB/s) with 1 file(s) remaining \rCompleted 190.5 MiB/703.1 MiB (132.3 MiB/s) with 1 file(s) remaining \rCompleted 190.8 MiB/703.1 MiB (132.4 MiB/s) with 1 file(s) remaining \rCompleted 191.0 MiB/703.1 MiB (132.6 MiB/s) with 1 file(s) remaining \rCompleted 191.2 MiB/703.1 MiB (132.7 MiB/s) with 1 file(s) remaining \rCompleted 191.5 MiB/703.1 MiB (132.8 MiB/s) with 1 file(s) remaining \rCompleted 191.8 MiB/703.1 MiB (133.0 MiB/s) with 1 file(s) remaining \rCompleted 192.0 MiB/703.1 MiB (133.0 MiB/s) with 1 file(s) remaining \rCompleted 192.2 MiB/703.1 MiB (133.2 MiB/s) with 1 file(s) remaining \rCompleted 192.5 MiB/703.1 MiB (133.3 MiB/s) with 1 file(s) remaining \rCompleted 192.8 MiB/703.1 MiB (133.3 MiB/s) with 1 file(s) remaining \rCompleted 193.0 MiB/703.1 MiB (133.5 MiB/s) with 1 file(s) remaining \rCompleted 193.2 MiB/703.1 MiB (133.6 MiB/s) with 1 file(s) remaining \rCompleted 193.5 MiB/703.1 MiB (133.8 MiB/s) with 1 file(s) remaining \rCompleted 193.8 MiB/703.1 MiB (133.9 MiB/s) with 1 file(s) remaining \rCompleted 194.0 MiB/703.1 MiB (134.0 MiB/s) with 1 file(s) remaining \rCompleted 194.2 MiB/703.1 MiB (134.1 MiB/s) with 1 file(s) remaining \rCompleted 194.5 MiB/703.1 MiB (134.3 MiB/s) with 1 file(s) remaining \rCompleted 194.8 MiB/703.1 MiB (134.4 MiB/s) with 1 file(s) remaining \rCompleted 195.0 MiB/703.1 MiB (134.5 MiB/s) with 1 file(s) remaining \rCompleted 195.2 MiB/703.1 MiB (134.6 MiB/s) with 1 file(s) remaining \rCompleted 195.5 MiB/703.1 MiB (134.8 MiB/s) with 1 file(s) remaining \rCompleted 195.8 MiB/703.1 MiB (134.9 MiB/s) with 1 file(s) remaining \rCompleted 196.0 MiB/703.1 MiB (135.1 MiB/s) with 1 file(s) remaining \rCompleted 196.2 MiB/703.1 MiB (135.2 MiB/s) with 1 file(s) remaining \rCompleted 196.5 MiB/703.1 MiB (135.3 MiB/s) with 1 file(s) remaining \rCompleted 196.8 MiB/703.1 MiB (135.4 MiB/s) with 1 file(s) remaining \rCompleted 197.0 MiB/703.1 MiB (135.5 MiB/s) with 1 file(s) remaining \rCompleted 197.2 MiB/703.1 MiB (135.6 MiB/s) with 1 file(s) remaining \rCompleted 197.5 MiB/703.1 MiB (135.7 MiB/s) with 1 file(s) remaining \rCompleted 197.8 MiB/703.1 MiB (135.8 MiB/s) with 1 file(s) remaining \rCompleted 198.0 MiB/703.1 MiB (135.9 MiB/s) with 1 file(s) remaining \rCompleted 198.2 MiB/703.1 MiB (136.0 MiB/s) with 1 file(s) remaining \rCompleted 198.5 MiB/703.1 MiB (136.0 MiB/s) with 1 file(s) remaining \rCompleted 198.8 MiB/703.1 MiB (136.1 MiB/s) with 1 file(s) remaining \rCompleted 199.0 MiB/703.1 MiB (136.2 MiB/s) with 1 file(s) remaining \rCompleted 199.2 MiB/703.1 MiB (136.2 MiB/s) with 1 file(s) remaining \rCompleted 199.5 MiB/703.1 MiB (136.3 MiB/s) with 1 file(s) remaining \rCompleted 199.8 MiB/703.1 MiB (136.4 MiB/s) with 1 file(s) remaining \rCompleted 200.0 MiB/703.1 MiB (136.5 MiB/s) with 1 file(s) remaining \rCompleted 200.2 MiB/703.1 MiB (136.6 MiB/s) with 1 file(s) remaining \rCompleted 200.5 MiB/703.1 MiB (136.7 MiB/s) with 1 file(s) remaining \rCompleted 200.8 MiB/703.1 MiB (136.8 MiB/s) with 1 file(s) remaining \rCompleted 201.0 MiB/703.1 MiB (136.8 MiB/s) with 1 file(s) remaining \rCompleted 201.2 MiB/703.1 MiB (136.9 MiB/s) with 1 file(s) remaining \rCompleted 201.5 MiB/703.1 MiB (135.6 MiB/s) with 1 file(s) remaining \rCompleted 201.8 MiB/703.1 MiB (135.7 MiB/s) with 1 file(s) remaining \rCompleted 202.0 MiB/703.1 MiB (135.8 MiB/s) with 1 file(s) remaining \rCompleted 202.2 MiB/703.1 MiB (135.9 MiB/s) with 1 file(s) remaining \rCompleted 202.5 MiB/703.1 MiB (136.0 MiB/s) with 1 file(s) remaining \rCompleted 202.8 MiB/703.1 MiB (135.5 MiB/s) with 1 file(s) remaining \rCompleted 203.0 MiB/703.1 MiB (135.6 MiB/s) with 1 file(s) remaining \rCompleted 203.2 MiB/703.1 MiB (135.7 MiB/s) with 1 file(s) remaining \rCompleted 203.5 MiB/703.1 MiB (135.8 MiB/s) with 1 file(s) remaining \rCompleted 203.8 MiB/703.1 MiB (135.8 MiB/s) with 1 file(s) remaining \rCompleted 204.0 MiB/703.1 MiB (135.9 MiB/s) with 1 file(s) remaining \rCompleted 204.2 MiB/703.1 MiB (136.0 MiB/s) with 1 file(s) remaining \rCompleted 204.5 MiB/703.1 MiB (136.2 MiB/s) with 1 file(s) remaining \rCompleted 204.8 MiB/703.1 MiB (136.2 MiB/s) with 1 file(s) remaining \rCompleted 205.0 MiB/703.1 MiB (136.4 MiB/s) with 1 file(s) remaining \rCompleted 205.2 MiB/703.1 MiB (136.5 MiB/s) with 1 file(s) remaining \rCompleted 205.5 MiB/703.1 MiB (136.6 MiB/s) with 1 file(s) remaining \rCompleted 205.8 MiB/703.1 MiB (136.7 MiB/s) with 1 file(s) remaining \rCompleted 206.0 MiB/703.1 MiB (136.8 MiB/s) with 1 file(s) remaining \rCompleted 206.2 MiB/703.1 MiB (137.0 MiB/s) with 1 file(s) remaining \rCompleted 206.5 MiB/703.1 MiB (137.1 MiB/s) with 1 file(s) remaining \rCompleted 206.8 MiB/703.1 MiB (137.2 MiB/s) with 1 file(s) remaining \rCompleted 207.0 MiB/703.1 MiB (137.3 MiB/s) with 1 file(s) remaining \rCompleted 207.2 MiB/703.1 MiB (137.4 MiB/s) with 1 file(s) remaining \rCompleted 207.5 MiB/703.1 MiB (137.6 MiB/s) with 1 file(s) remaining \rCompleted 207.8 MiB/703.1 MiB (137.6 MiB/s) with 1 file(s) remaining \rCompleted 208.0 MiB/703.1 MiB (137.8 MiB/s) with 1 file(s) remaining \rCompleted 208.2 MiB/703.1 MiB (137.9 MiB/s) with 1 file(s) remaining \rCompleted 208.5 MiB/703.1 MiB (138.0 MiB/s) with 1 file(s) remaining \rCompleted 208.8 MiB/703.1 MiB (138.1 MiB/s) with 1 file(s) remaining \rCompleted 209.0 MiB/703.1 MiB (138.3 MiB/s) with 1 file(s) remaining \rCompleted 209.2 MiB/703.1 MiB (138.4 MiB/s) with 1 file(s) remaining \rCompleted 209.5 MiB/703.1 MiB (138.5 MiB/s) with 1 file(s) remaining \rCompleted 209.8 MiB/703.1 MiB (138.6 MiB/s) with 1 file(s) remaining \rCompleted 210.0 MiB/703.1 MiB (138.7 MiB/s) with 1 file(s) remaining \rCompleted 210.2 MiB/703.1 MiB (138.8 MiB/s) with 1 file(s) remaining \rCompleted 210.5 MiB/703.1 MiB (139.0 MiB/s) with 1 file(s) remaining \rCompleted 210.8 MiB/703.1 MiB (139.1 MiB/s) with 1 file(s) remaining \rCompleted 211.0 MiB/703.1 MiB (139.2 MiB/s) with 1 file(s) remaining \rCompleted 211.2 MiB/703.1 MiB (139.3 MiB/s) with 1 file(s) remaining \rCompleted 211.5 MiB/703.1 MiB (139.5 MiB/s) with 1 file(s) remaining \rCompleted 211.8 MiB/703.1 MiB (139.5 MiB/s) with 1 file(s) remaining \rCompleted 212.0 MiB/703.1 MiB (139.7 MiB/s) with 1 file(s) remaining \rCompleted 212.2 MiB/703.1 MiB (139.8 MiB/s) with 1 file(s) remaining \rCompleted 212.5 MiB/703.1 MiB (139.9 MiB/s) with 1 file(s) remaining \rCompleted 212.8 MiB/703.1 MiB (140.0 MiB/s) with 1 file(s) remaining \rCompleted 213.0 MiB/703.1 MiB (140.1 MiB/s) with 1 file(s) remaining \rCompleted 213.2 MiB/703.1 MiB (140.2 MiB/s) with 1 file(s) remaining \rCompleted 213.5 MiB/703.1 MiB (140.3 MiB/s) with 1 file(s) remaining \rCompleted 213.8 MiB/703.1 MiB (140.5 MiB/s) with 1 file(s) remaining \rCompleted 214.0 MiB/703.1 MiB (140.5 MiB/s) with 1 file(s) remaining \rCompleted 214.2 MiB/703.1 MiB (140.7 MiB/s) with 1 file(s) remaining \rCompleted 214.5 MiB/703.1 MiB (140.8 MiB/s) with 1 file(s) remaining \rCompleted 214.8 MiB/703.1 MiB (140.8 MiB/s) with 1 file(s) remaining \rCompleted 215.0 MiB/703.1 MiB (140.9 MiB/s) with 1 file(s) remaining \rCompleted 215.2 MiB/703.1 MiB (141.0 MiB/s) with 1 file(s) remaining \rCompleted 215.5 MiB/703.1 MiB (141.0 MiB/s) with 1 file(s) remaining \rCompleted 215.8 MiB/703.1 MiB (141.1 MiB/s) with 1 file(s) remaining \rCompleted 216.0 MiB/703.1 MiB (141.2 MiB/s) with 1 file(s) remaining \rCompleted 216.2 MiB/703.1 MiB (141.3 MiB/s) with 1 file(s) remaining \rCompleted 216.5 MiB/703.1 MiB (141.4 MiB/s) with 1 file(s) remaining \rCompleted 216.8 MiB/703.1 MiB (141.6 MiB/s) with 1 file(s) remaining \rCompleted 217.0 MiB/703.1 MiB (141.7 MiB/s) with 1 file(s) remaining \rCompleted 217.2 MiB/703.1 MiB (141.7 MiB/s) with 1 file(s) remaining \rCompleted 217.5 MiB/703.1 MiB (141.7 MiB/s) with 1 file(s) remaining \rCompleted 217.8 MiB/703.1 MiB (141.8 MiB/s) with 1 file(s) remaining \rCompleted 218.0 MiB/703.1 MiB (141.9 MiB/s) with 1 file(s) remaining \rCompleted 218.2 MiB/703.1 MiB (141.9 MiB/s) with 1 file(s) remaining \rCompleted 218.5 MiB/703.1 MiB (142.0 MiB/s) with 1 file(s) remaining \rCompleted 218.8 MiB/703.1 MiB (142.1 MiB/s) with 1 file(s) remaining \rCompleted 219.0 MiB/703.1 MiB (142.3 MiB/s) with 1 file(s) remaining \rCompleted 219.2 MiB/703.1 MiB (142.4 MiB/s) with 1 file(s) remaining \rCompleted 219.5 MiB/703.1 MiB (142.5 MiB/s) with 1 file(s) remaining \rCompleted 219.8 MiB/703.1 MiB (142.5 MiB/s) with 1 file(s) remaining \rCompleted 220.0 MiB/703.1 MiB (142.6 MiB/s) with 1 file(s) remaining \rCompleted 220.2 MiB/703.1 MiB (142.7 MiB/s) with 1 file(s) remaining \rCompleted 220.5 MiB/703.1 MiB (142.8 MiB/s) with 1 file(s) remaining \rCompleted 220.8 MiB/703.1 MiB (142.9 MiB/s) with 1 file(s) remaining \rCompleted 221.0 MiB/703.1 MiB (143.0 MiB/s) with 1 file(s) remaining \rCompleted 221.2 MiB/703.1 MiB (143.1 MiB/s) with 1 file(s) remaining \rCompleted 221.5 MiB/703.1 MiB (143.2 MiB/s) with 1 file(s) remaining \rCompleted 221.8 MiB/703.1 MiB (143.3 MiB/s) with 1 file(s) remaining \rCompleted 222.0 MiB/703.1 MiB (143.4 MiB/s) with 1 file(s) remaining \rCompleted 222.2 MiB/703.1 MiB (143.6 MiB/s) with 1 file(s) remaining \rCompleted 222.5 MiB/703.1 MiB (143.6 MiB/s) with 1 file(s) remaining \rCompleted 222.8 MiB/703.1 MiB (143.7 MiB/s) with 1 file(s) remaining \rCompleted 223.0 MiB/703.1 MiB (143.8 MiB/s) with 1 file(s) remaining \rCompleted 223.2 MiB/703.1 MiB (143.9 MiB/s) with 1 file(s) remaining \rCompleted 223.5 MiB/703.1 MiB (144.0 MiB/s) with 1 file(s) remaining \rCompleted 223.8 MiB/703.1 MiB (144.1 MiB/s) with 1 file(s) remaining \rCompleted 224.0 MiB/703.1 MiB (144.2 MiB/s) with 1 file(s) remaining \rCompleted 224.2 MiB/703.1 MiB (144.4 MiB/s) with 1 file(s) remaining \rCompleted 224.5 MiB/703.1 MiB (144.4 MiB/s) with 1 file(s) remaining \rCompleted 224.8 MiB/703.1 MiB (144.5 MiB/s) with 1 file(s) remaining \rCompleted 225.0 MiB/703.1 MiB (144.6 MiB/s) with 1 file(s) remaining \rCompleted 225.2 MiB/703.1 MiB (144.7 MiB/s) with 1 file(s) remaining \rCompleted 225.5 MiB/703.1 MiB (144.7 MiB/s) with 1 file(s) remaining \rCompleted 225.8 MiB/703.1 MiB (144.9 MiB/s) with 1 file(s) remaining \rCompleted 226.0 MiB/703.1 MiB (145.0 MiB/s) with 1 file(s) remaining \rCompleted 226.2 MiB/703.1 MiB (145.1 MiB/s) with 1 file(s) remaining \rCompleted 226.5 MiB/703.1 MiB (145.2 MiB/s) with 1 file(s) remaining \rCompleted 226.8 MiB/703.1 MiB (145.2 MiB/s) with 1 file(s) remaining \rCompleted 227.0 MiB/703.1 MiB (145.3 MiB/s) with 1 file(s) remaining \rCompleted 227.2 MiB/703.1 MiB (145.3 MiB/s) with 1 file(s) remaining \rCompleted 227.5 MiB/703.1 MiB (145.4 MiB/s) with 1 file(s) remaining \rCompleted 227.8 MiB/703.1 MiB (145.6 MiB/s) with 1 file(s) remaining \rCompleted 228.0 MiB/703.1 MiB (145.7 MiB/s) with 1 file(s) remaining \rCompleted 228.2 MiB/703.1 MiB (145.6 MiB/s) with 1 file(s) remaining \rCompleted 228.5 MiB/703.1 MiB (145.7 MiB/s) with 1 file(s) remaining \rCompleted 228.8 MiB/703.1 MiB (145.7 MiB/s) with 1 file(s) remaining \rCompleted 229.0 MiB/703.1 MiB (145.9 MiB/s) with 1 file(s) remaining \rCompleted 229.2 MiB/703.1 MiB (146.0 MiB/s) with 1 file(s) remaining \rCompleted 229.5 MiB/703.1 MiB (146.1 MiB/s) with 1 file(s) remaining \rCompleted 229.8 MiB/703.1 MiB (146.2 MiB/s) with 1 file(s) remaining \rCompleted 230.0 MiB/703.1 MiB (146.1 MiB/s) with 1 file(s) remaining \rCompleted 230.2 MiB/703.1 MiB (146.2 MiB/s) with 1 file(s) remaining \rCompleted 230.5 MiB/703.1 MiB (146.2 MiB/s) with 1 file(s) remaining \rCompleted 230.8 MiB/703.1 MiB (146.3 MiB/s) with 1 file(s) remaining \rCompleted 231.0 MiB/703.1 MiB (146.4 MiB/s) with 1 file(s) remaining \rCompleted 231.2 MiB/703.1 MiB (146.4 MiB/s) with 1 file(s) remaining \rCompleted 231.5 MiB/703.1 MiB (146.6 MiB/s) with 1 file(s) remaining \rCompleted 231.8 MiB/703.1 MiB (146.7 MiB/s) with 1 file(s) remaining \rCompleted 232.0 MiB/703.1 MiB (146.8 MiB/s) with 1 file(s) remaining \rCompleted 232.2 MiB/703.1 MiB (146.8 MiB/s) with 1 file(s) remaining \rCompleted 232.5 MiB/703.1 MiB (146.9 MiB/s) with 1 file(s) remaining \rCompleted 232.8 MiB/703.1 MiB (147.1 MiB/s) with 1 file(s) remaining \rCompleted 233.0 MiB/703.1 MiB (147.2 MiB/s) with 1 file(s) remaining \rCompleted 233.2 MiB/703.1 MiB (147.3 MiB/s) with 1 file(s) remaining \rCompleted 233.5 MiB/703.1 MiB (147.4 MiB/s) with 1 file(s) remaining \rCompleted 233.8 MiB/703.1 MiB (147.4 MiB/s) with 1 file(s) remaining \rCompleted 234.0 MiB/703.1 MiB (147.5 MiB/s) with 1 file(s) remaining \rCompleted 234.2 MiB/703.1 MiB (147.6 MiB/s) with 1 file(s) remaining \rCompleted 234.5 MiB/703.1 MiB (147.6 MiB/s) with 1 file(s) remaining \rCompleted 234.8 MiB/703.1 MiB (147.7 MiB/s) with 1 file(s) remaining \rCompleted 235.0 MiB/703.1 MiB (147.9 MiB/s) with 1 file(s) remaining \rCompleted 235.2 MiB/703.1 MiB (147.9 MiB/s) with 1 file(s) remaining \rCompleted 235.5 MiB/703.1 MiB (148.1 MiB/s) with 1 file(s) remaining \rCompleted 235.8 MiB/703.1 MiB (148.2 MiB/s) with 1 file(s) remaining \rCompleted 236.0 MiB/703.1 MiB (148.3 MiB/s) with 1 file(s) remaining \rCompleted 236.2 MiB/703.1 MiB (148.4 MiB/s) with 1 file(s) remaining \rCompleted 236.5 MiB/703.1 MiB (148.5 MiB/s) with 1 file(s) remaining \rCompleted 236.8 MiB/703.1 MiB (148.5 MiB/s) with 1 file(s) remaining \rCompleted 237.0 MiB/703.1 MiB (148.6 MiB/s) with 1 file(s) remaining \rCompleted 237.2 MiB/703.1 MiB (148.7 MiB/s) with 1 file(s) remaining \rCompleted 237.5 MiB/703.1 MiB (148.8 MiB/s) with 1 file(s) remaining \rCompleted 237.8 MiB/703.1 MiB (148.9 MiB/s) with 1 file(s) remaining \rCompleted 238.0 MiB/703.1 MiB (149.0 MiB/s) with 1 file(s) remaining \rCompleted 238.2 MiB/703.1 MiB (149.1 MiB/s) with 1 file(s) remaining \rCompleted 238.5 MiB/703.1 MiB (149.2 MiB/s) with 1 file(s) remaining \rCompleted 238.8 MiB/703.1 MiB (149.3 MiB/s) with 1 file(s) remaining \rCompleted 239.0 MiB/703.1 MiB (149.4 MiB/s) with 1 file(s) remaining \rCompleted 239.2 MiB/703.1 MiB (149.5 MiB/s) with 1 file(s) remaining \rCompleted 239.5 MiB/703.1 MiB (149.6 MiB/s) with 1 file(s) remaining \rCompleted 239.8 MiB/703.1 MiB (149.7 MiB/s) with 1 file(s) remaining \rCompleted 240.0 MiB/703.1 MiB (149.8 MiB/s) with 1 file(s) remaining \rCompleted 240.2 MiB/703.1 MiB (149.9 MiB/s) with 1 file(s) remaining \rCompleted 240.5 MiB/703.1 MiB (149.9 MiB/s) with 1 file(s) remaining \rCompleted 240.8 MiB/703.1 MiB (150.0 MiB/s) with 1 file(s) remaining \rCompleted 241.0 MiB/703.1 MiB (150.1 MiB/s) with 1 file(s) remaining \rCompleted 241.2 MiB/703.1 MiB (150.2 MiB/s) with 1 file(s) remaining \rCompleted 241.5 MiB/703.1 MiB (150.3 MiB/s) with 1 file(s) remaining \rCompleted 241.8 MiB/703.1 MiB (150.4 MiB/s) with 1 file(s) remaining \rCompleted 242.0 MiB/703.1 MiB (150.5 MiB/s) with 1 file(s) remaining \rCompleted 242.2 MiB/703.1 MiB (150.7 MiB/s) with 1 file(s) remaining \rCompleted 242.5 MiB/703.1 MiB (150.7 MiB/s) with 1 file(s) remaining \rCompleted 242.8 MiB/703.1 MiB (150.8 MiB/s) with 1 file(s) remaining \rCompleted 243.0 MiB/703.1 MiB (150.9 MiB/s) with 1 file(s) remaining \rCompleted 243.2 MiB/703.1 MiB (151.0 MiB/s) with 1 file(s) remaining \rCompleted 243.5 MiB/703.1 MiB (151.1 MiB/s) with 1 file(s) remaining \rCompleted 243.8 MiB/703.1 MiB (151.2 MiB/s) with 1 file(s) remaining \rCompleted 244.0 MiB/703.1 MiB (151.2 MiB/s) with 1 file(s) remaining \rCompleted 244.2 MiB/703.1 MiB (151.3 MiB/s) with 1 file(s) remaining \rCompleted 244.5 MiB/703.1 MiB (151.3 MiB/s) with 1 file(s) remaining \rCompleted 244.8 MiB/703.1 MiB (151.4 MiB/s) with 1 file(s) remaining \rCompleted 245.0 MiB/703.1 MiB (151.6 MiB/s) with 1 file(s) remaining \rCompleted 245.2 MiB/703.1 MiB (151.6 MiB/s) with 1 file(s) remaining \rCompleted 245.5 MiB/703.1 MiB (151.7 MiB/s) with 1 file(s) remaining \rCompleted 245.8 MiB/703.1 MiB (151.7 MiB/s) with 1 file(s) remaining \rCompleted 246.0 MiB/703.1 MiB (151.8 MiB/s) with 1 file(s) remaining \rCompleted 246.2 MiB/703.1 MiB (151.8 MiB/s) with 1 file(s) remaining \rCompleted 246.5 MiB/703.1 MiB (151.9 MiB/s) with 1 file(s) remaining \rCompleted 246.8 MiB/703.1 MiB (152.0 MiB/s) with 1 file(s) remaining \rCompleted 247.0 MiB/703.1 MiB (152.1 MiB/s) with 1 file(s) remaining \rCompleted 247.2 MiB/703.1 MiB (152.2 MiB/s) with 1 file(s) remaining \rCompleted 247.5 MiB/703.1 MiB (152.3 MiB/s) with 1 file(s) remaining \rCompleted 247.8 MiB/703.1 MiB (152.4 MiB/s) with 1 file(s) remaining \rCompleted 248.0 MiB/703.1 MiB (152.4 MiB/s) with 1 file(s) remaining \rCompleted 248.2 MiB/703.1 MiB (152.5 MiB/s) with 1 file(s) remaining \rCompleted 248.5 MiB/703.1 MiB (152.7 MiB/s) with 1 file(s) remaining \rCompleted 248.8 MiB/703.1 MiB (152.8 MiB/s) with 1 file(s) remaining \rCompleted 249.0 MiB/703.1 MiB (152.8 MiB/s) with 1 file(s) remaining \rCompleted 249.2 MiB/703.1 MiB (153.0 MiB/s) with 1 file(s) remaining \rCompleted 249.5 MiB/703.1 MiB (153.0 MiB/s) with 1 file(s) remaining \rCompleted 249.8 MiB/703.1 MiB (153.1 MiB/s) with 1 file(s) remaining \rCompleted 250.0 MiB/703.1 MiB (153.2 MiB/s) with 1 file(s) remaining \rCompleted 250.2 MiB/703.1 MiB (153.4 MiB/s) with 1 file(s) remaining \rCompleted 250.5 MiB/703.1 MiB (153.4 MiB/s) with 1 file(s) remaining \rCompleted 250.8 MiB/703.1 MiB (153.5 MiB/s) with 1 file(s) remaining \rCompleted 251.0 MiB/703.1 MiB (153.6 MiB/s) with 1 file(s) remaining \rCompleted 251.2 MiB/703.1 MiB (153.7 MiB/s) with 1 file(s) remaining \rCompleted 251.5 MiB/703.1 MiB (153.8 MiB/s) with 1 file(s) remaining \rCompleted 251.8 MiB/703.1 MiB (153.9 MiB/s) with 1 file(s) remaining \rCompleted 252.0 MiB/703.1 MiB (154.0 MiB/s) with 1 file(s) remaining \rCompleted 252.2 MiB/703.1 MiB (154.1 MiB/s) with 1 file(s) remaining \rCompleted 252.5 MiB/703.1 MiB (154.2 MiB/s) with 1 file(s) remaining \rCompleted 252.8 MiB/703.1 MiB (154.3 MiB/s) with 1 file(s) remaining \rCompleted 253.0 MiB/703.1 MiB (154.4 MiB/s) with 1 file(s) remaining \rCompleted 253.2 MiB/703.1 MiB (154.5 MiB/s) with 1 file(s) remaining \rCompleted 253.5 MiB/703.1 MiB (154.6 MiB/s) with 1 file(s) remaining \rCompleted 253.8 MiB/703.1 MiB (154.7 MiB/s) with 1 file(s) remaining \rCompleted 254.0 MiB/703.1 MiB (154.8 MiB/s) with 1 file(s) remaining \rCompleted 254.2 MiB/703.1 MiB (154.9 MiB/s) with 1 file(s) remaining \rCompleted 254.5 MiB/703.1 MiB (155.0 MiB/s) with 1 file(s) remaining \rCompleted 254.8 MiB/703.1 MiB (155.1 MiB/s) with 1 file(s) remaining \rCompleted 255.0 MiB/703.1 MiB (155.2 MiB/s) with 1 file(s) remaining \rCompleted 255.2 MiB/703.1 MiB (155.3 MiB/s) with 1 file(s) remaining \rCompleted 255.5 MiB/703.1 MiB (155.4 MiB/s) with 1 file(s) remaining \rCompleted 255.8 MiB/703.1 MiB (155.5 MiB/s) with 1 file(s) remaining \rCompleted 256.0 MiB/703.1 MiB (155.6 MiB/s) with 1 file(s) remaining \rCompleted 256.2 MiB/703.1 MiB (155.6 MiB/s) with 1 file(s) remaining \rCompleted 256.5 MiB/703.1 MiB (155.7 MiB/s) with 1 file(s) remaining \rCompleted 256.8 MiB/703.1 MiB (155.9 MiB/s) with 1 file(s) remaining \rCompleted 257.0 MiB/703.1 MiB (156.0 MiB/s) with 1 file(s) remaining \rCompleted 257.2 MiB/703.1 MiB (156.0 MiB/s) with 1 file(s) remaining \rCompleted 257.5 MiB/703.1 MiB (156.0 MiB/s) with 1 file(s) remaining \rCompleted 257.8 MiB/703.1 MiB (156.1 MiB/s) with 1 file(s) remaining \rCompleted 258.0 MiB/703.1 MiB (156.2 MiB/s) with 1 file(s) remaining \rCompleted 258.2 MiB/703.1 MiB (156.3 MiB/s) with 1 file(s) remaining \rCompleted 258.5 MiB/703.1 MiB (156.4 MiB/s) with 1 file(s) remaining \rCompleted 258.8 MiB/703.1 MiB (156.5 MiB/s) with 1 file(s) remaining \rCompleted 259.0 MiB/703.1 MiB (156.6 MiB/s) with 1 file(s) remaining \rCompleted 259.2 MiB/703.1 MiB (156.6 MiB/s) with 1 file(s) remaining \rCompleted 259.5 MiB/703.1 MiB (156.8 MiB/s) with 1 file(s) remaining \rCompleted 259.8 MiB/703.1 MiB (156.9 MiB/s) with 1 file(s) remaining \rCompleted 260.0 MiB/703.1 MiB (157.0 MiB/s) with 1 file(s) remaining \rCompleted 260.2 MiB/703.1 MiB (157.1 MiB/s) with 1 file(s) remaining \rCompleted 260.5 MiB/703.1 MiB (157.2 MiB/s) with 1 file(s) remaining \rCompleted 260.8 MiB/703.1 MiB (157.3 MiB/s) with 1 file(s) remaining \rCompleted 261.0 MiB/703.1 MiB (157.4 MiB/s) with 1 file(s) remaining \rCompleted 261.2 MiB/703.1 MiB (157.5 MiB/s) with 1 file(s) remaining \rCompleted 261.5 MiB/703.1 MiB (157.6 MiB/s) with 1 file(s) remaining \rCompleted 261.8 MiB/703.1 MiB (157.6 MiB/s) with 1 file(s) remaining \rCompleted 262.0 MiB/703.1 MiB (157.8 MiB/s) with 1 file(s) remaining \rCompleted 262.2 MiB/703.1 MiB (157.8 MiB/s) with 1 file(s) remaining \rCompleted 262.5 MiB/703.1 MiB (157.9 MiB/s) with 1 file(s) remaining \rCompleted 262.8 MiB/703.1 MiB (158.0 MiB/s) with 1 file(s) remaining \rCompleted 263.0 MiB/703.1 MiB (158.1 MiB/s) with 1 file(s) remaining \rCompleted 263.2 MiB/703.1 MiB (158.3 MiB/s) with 1 file(s) remaining \rCompleted 263.5 MiB/703.1 MiB (158.3 MiB/s) with 1 file(s) remaining \rCompleted 263.8 MiB/703.1 MiB (158.4 MiB/s) with 1 file(s) remaining \rCompleted 264.0 MiB/703.1 MiB (158.5 MiB/s) with 1 file(s) remaining \rCompleted 264.2 MiB/703.1 MiB (158.6 MiB/s) with 1 file(s) remaining \rCompleted 264.5 MiB/703.1 MiB (158.8 MiB/s) with 1 file(s) remaining \rCompleted 264.8 MiB/703.1 MiB (158.8 MiB/s) with 1 file(s) remaining \rCompleted 265.0 MiB/703.1 MiB (158.9 MiB/s) with 1 file(s) remaining \rCompleted 265.2 MiB/703.1 MiB (159.0 MiB/s) with 1 file(s) remaining \rCompleted 265.5 MiB/703.1 MiB (159.1 MiB/s) with 1 file(s) remaining \rCompleted 265.8 MiB/703.1 MiB (159.2 MiB/s) with 1 file(s) remaining \rCompleted 266.0 MiB/703.1 MiB (159.3 MiB/s) with 1 file(s) remaining \rCompleted 266.2 MiB/703.1 MiB (159.3 MiB/s) with 1 file(s) remaining \rCompleted 266.5 MiB/703.1 MiB (159.4 MiB/s) with 1 file(s) remaining \rCompleted 266.8 MiB/703.1 MiB (159.5 MiB/s) with 1 file(s) remaining \rCompleted 267.0 MiB/703.1 MiB (159.6 MiB/s) with 1 file(s) remaining \rCompleted 267.2 MiB/703.1 MiB (159.6 MiB/s) with 1 file(s) remaining \rCompleted 267.5 MiB/703.1 MiB (159.7 MiB/s) with 1 file(s) remaining \rCompleted 267.8 MiB/703.1 MiB (159.7 MiB/s) with 1 file(s) remaining \rCompleted 268.0 MiB/703.1 MiB (159.9 MiB/s) with 1 file(s) remaining \rCompleted 268.2 MiB/703.1 MiB (160.0 MiB/s) with 1 file(s) remaining \rCompleted 268.5 MiB/703.1 MiB (160.1 MiB/s) with 1 file(s) remaining \rCompleted 268.8 MiB/703.1 MiB (160.2 MiB/s) with 1 file(s) remaining \rCompleted 269.0 MiB/703.1 MiB (160.2 MiB/s) with 1 file(s) remaining \rCompleted 269.2 MiB/703.1 MiB (160.3 MiB/s) with 1 file(s) remaining \rCompleted 269.5 MiB/703.1 MiB (160.5 MiB/s) with 1 file(s) remaining \rCompleted 269.8 MiB/703.1 MiB (160.5 MiB/s) with 1 file(s) remaining \rCompleted 270.0 MiB/703.1 MiB (160.6 MiB/s) with 1 file(s) remaining \rCompleted 270.2 MiB/703.1 MiB (160.7 MiB/s) with 1 file(s) remaining \rCompleted 270.5 MiB/703.1 MiB (160.9 MiB/s) with 1 file(s) remaining \rCompleted 270.8 MiB/703.1 MiB (160.9 MiB/s) with 1 file(s) remaining \rCompleted 271.0 MiB/703.1 MiB (161.0 MiB/s) with 1 file(s) remaining \rCompleted 271.2 MiB/703.1 MiB (161.1 MiB/s) with 1 file(s) remaining \rCompleted 271.5 MiB/703.1 MiB (161.2 MiB/s) with 1 file(s) remaining \rCompleted 271.8 MiB/703.1 MiB (161.3 MiB/s) with 1 file(s) remaining \rCompleted 272.0 MiB/703.1 MiB (161.3 MiB/s) with 1 file(s) remaining \rCompleted 272.2 MiB/703.1 MiB (161.5 MiB/s) with 1 file(s) remaining \rCompleted 272.5 MiB/703.1 MiB (161.6 MiB/s) with 1 file(s) remaining \rCompleted 272.8 MiB/703.1 MiB (161.7 MiB/s) with 1 file(s) remaining \rCompleted 273.0 MiB/703.1 MiB (161.6 MiB/s) with 1 file(s) remaining \rCompleted 273.2 MiB/703.1 MiB (161.7 MiB/s) with 1 file(s) remaining \rCompleted 273.5 MiB/703.1 MiB (161.8 MiB/s) with 1 file(s) remaining \rCompleted 273.8 MiB/703.1 MiB (161.9 MiB/s) with 1 file(s) remaining \rCompleted 274.0 MiB/703.1 MiB (162.0 MiB/s) with 1 file(s) remaining \rCompleted 274.2 MiB/703.1 MiB (162.1 MiB/s) with 1 file(s) remaining \rCompleted 274.5 MiB/703.1 MiB (162.2 MiB/s) with 1 file(s) remaining \rCompleted 274.8 MiB/703.1 MiB (162.1 MiB/s) with 1 file(s) remaining \rCompleted 275.0 MiB/703.1 MiB (162.2 MiB/s) with 1 file(s) remaining \rCompleted 275.2 MiB/703.1 MiB (162.3 MiB/s) with 1 file(s) remaining \rCompleted 275.5 MiB/703.1 MiB (162.5 MiB/s) with 1 file(s) remaining \rCompleted 275.8 MiB/703.1 MiB (162.6 MiB/s) with 1 file(s) remaining \rCompleted 276.0 MiB/703.1 MiB (162.6 MiB/s) with 1 file(s) remaining \rCompleted 276.2 MiB/703.1 MiB (162.6 MiB/s) with 1 file(s) remaining \rCompleted 276.5 MiB/703.1 MiB (162.7 MiB/s) with 1 file(s) remaining \rCompleted 276.8 MiB/703.1 MiB (162.8 MiB/s) with 1 file(s) remaining \rCompleted 277.0 MiB/703.1 MiB (162.9 MiB/s) with 1 file(s) remaining \rCompleted 277.2 MiB/703.1 MiB (163.0 MiB/s) with 1 file(s) remaining \rCompleted 277.5 MiB/703.1 MiB (163.1 MiB/s) with 1 file(s) remaining \rCompleted 277.8 MiB/703.1 MiB (163.2 MiB/s) with 1 file(s) remaining \rCompleted 278.0 MiB/703.1 MiB (163.3 MiB/s) with 1 file(s) remaining \rCompleted 278.2 MiB/703.1 MiB (163.4 MiB/s) with 1 file(s) remaining \rCompleted 278.5 MiB/703.1 MiB (163.5 MiB/s) with 1 file(s) remaining \rCompleted 278.8 MiB/703.1 MiB (163.6 MiB/s) with 1 file(s) remaining \rCompleted 279.0 MiB/703.1 MiB (163.7 MiB/s) with 1 file(s) remaining \rCompleted 279.2 MiB/703.1 MiB (163.8 MiB/s) with 1 file(s) remaining \rCompleted 279.5 MiB/703.1 MiB (163.8 MiB/s) with 1 file(s) remaining \rCompleted 279.8 MiB/703.1 MiB (163.8 MiB/s) with 1 file(s) remaining \rCompleted 280.0 MiB/703.1 MiB (163.8 MiB/s) with 1 file(s) remaining \rCompleted 280.2 MiB/703.1 MiB (163.9 MiB/s) with 1 file(s) remaining \rCompleted 280.5 MiB/703.1 MiB (164.1 MiB/s) with 1 file(s) remaining \rCompleted 280.8 MiB/703.1 MiB (164.1 MiB/s) with 1 file(s) remaining \rCompleted 281.0 MiB/703.1 MiB (164.2 MiB/s) with 1 file(s) remaining \rCompleted 281.2 MiB/703.1 MiB (164.3 MiB/s) with 1 file(s) remaining \rCompleted 281.5 MiB/703.1 MiB (164.4 MiB/s) with 1 file(s) remaining \rCompleted 281.8 MiB/703.1 MiB (164.4 MiB/s) with 1 file(s) remaining \rCompleted 282.0 MiB/703.1 MiB (164.6 MiB/s) with 1 file(s) remaining \rCompleted 282.2 MiB/703.1 MiB (164.7 MiB/s) with 1 file(s) remaining \rCompleted 282.5 MiB/703.1 MiB (164.8 MiB/s) with 1 file(s) remaining \rCompleted 282.8 MiB/703.1 MiB (164.8 MiB/s) with 1 file(s) remaining \rCompleted 283.0 MiB/703.1 MiB (164.9 MiB/s) with 1 file(s) remaining \rCompleted 283.2 MiB/703.1 MiB (165.0 MiB/s) with 1 file(s) remaining \rCompleted 283.5 MiB/703.1 MiB (165.0 MiB/s) with 1 file(s) remaining \rCompleted 283.8 MiB/703.1 MiB (165.0 MiB/s) with 1 file(s) remaining \rCompleted 284.0 MiB/703.1 MiB (164.9 MiB/s) with 1 file(s) remaining \rCompleted 284.2 MiB/703.1 MiB (165.0 MiB/s) with 1 file(s) remaining \rCompleted 284.5 MiB/703.1 MiB (165.1 MiB/s) with 1 file(s) remaining \rCompleted 284.8 MiB/703.1 MiB (165.2 MiB/s) with 1 file(s) remaining \rCompleted 285.0 MiB/703.1 MiB (165.3 MiB/s) with 1 file(s) remaining \rCompleted 285.2 MiB/703.1 MiB (165.3 MiB/s) with 1 file(s) remaining \rCompleted 285.5 MiB/703.1 MiB (165.5 MiB/s) with 1 file(s) remaining \rCompleted 285.8 MiB/703.1 MiB (165.6 MiB/s) with 1 file(s) remaining \rCompleted 286.0 MiB/703.1 MiB (165.6 MiB/s) with 1 file(s) remaining \rCompleted 286.2 MiB/703.1 MiB (165.7 MiB/s) with 1 file(s) remaining \rCompleted 286.5 MiB/703.1 MiB (165.9 MiB/s) with 1 file(s) remaining \rCompleted 286.8 MiB/703.1 MiB (165.9 MiB/s) with 1 file(s) remaining \rCompleted 287.0 MiB/703.1 MiB (166.0 MiB/s) with 1 file(s) remaining \rCompleted 287.2 MiB/703.1 MiB (166.2 MiB/s) with 1 file(s) remaining \rCompleted 287.5 MiB/703.1 MiB (166.2 MiB/s) with 1 file(s) remaining \rCompleted 287.8 MiB/703.1 MiB (166.3 MiB/s) with 1 file(s) remaining \rCompleted 288.0 MiB/703.1 MiB (166.4 MiB/s) with 1 file(s) remaining \rCompleted 288.2 MiB/703.1 MiB (166.5 MiB/s) with 1 file(s) remaining \rCompleted 288.5 MiB/703.1 MiB (166.6 MiB/s) with 1 file(s) remaining \rCompleted 288.8 MiB/703.1 MiB (166.7 MiB/s) with 1 file(s) remaining \rCompleted 289.0 MiB/703.1 MiB (166.8 MiB/s) with 1 file(s) remaining \rCompleted 289.2 MiB/703.1 MiB (166.9 MiB/s) with 1 file(s) remaining \rCompleted 289.5 MiB/703.1 MiB (167.0 MiB/s) with 1 file(s) remaining \rCompleted 289.8 MiB/703.1 MiB (167.1 MiB/s) with 1 file(s) remaining \rCompleted 290.0 MiB/703.1 MiB (167.2 MiB/s) with 1 file(s) remaining \rCompleted 290.2 MiB/703.1 MiB (167.2 MiB/s) with 1 file(s) remaining \rCompleted 290.5 MiB/703.1 MiB (167.2 MiB/s) with 1 file(s) remaining \rCompleted 290.8 MiB/703.1 MiB (167.3 MiB/s) with 1 file(s) remaining \rCompleted 291.0 MiB/703.1 MiB (167.3 MiB/s) with 1 file(s) remaining \rCompleted 291.2 MiB/703.1 MiB (167.3 MiB/s) with 1 file(s) remaining \rCompleted 291.5 MiB/703.1 MiB (167.4 MiB/s) with 1 file(s) remaining \rCompleted 291.8 MiB/703.1 MiB (167.4 MiB/s) with 1 file(s) remaining \rCompleted 292.0 MiB/703.1 MiB (167.5 MiB/s) with 1 file(s) remaining \rCompleted 292.2 MiB/703.1 MiB (167.5 MiB/s) with 1 file(s) remaining \rCompleted 292.5 MiB/703.1 MiB (167.7 MiB/s) with 1 file(s) remaining \rCompleted 292.8 MiB/703.1 MiB (167.8 MiB/s) with 1 file(s) remaining \rCompleted 293.0 MiB/703.1 MiB (167.8 MiB/s) with 1 file(s) remaining \rCompleted 293.2 MiB/703.1 MiB (168.0 MiB/s) with 1 file(s) remaining \rCompleted 293.5 MiB/703.1 MiB (168.0 MiB/s) with 1 file(s) remaining \rCompleted 293.8 MiB/703.1 MiB (168.2 MiB/s) with 1 file(s) remaining \rCompleted 294.0 MiB/703.1 MiB (168.2 MiB/s) with 1 file(s) remaining \rCompleted 294.2 MiB/703.1 MiB (168.4 MiB/s) with 1 file(s) remaining \rCompleted 294.5 MiB/703.1 MiB (168.4 MiB/s) with 1 file(s) remaining \rCompleted 294.8 MiB/703.1 MiB (168.5 MiB/s) with 1 file(s) remaining \rCompleted 295.0 MiB/703.1 MiB (168.6 MiB/s) with 1 file(s) remaining \rCompleted 295.2 MiB/703.1 MiB (168.7 MiB/s) with 1 file(s) remaining \rCompleted 295.5 MiB/703.1 MiB (168.4 MiB/s) with 1 file(s) remaining \rCompleted 295.8 MiB/703.1 MiB (168.5 MiB/s) with 1 file(s) remaining \rCompleted 296.0 MiB/703.1 MiB (168.5 MiB/s) with 1 file(s) remaining \rCompleted 296.2 MiB/703.1 MiB (168.5 MiB/s) with 1 file(s) remaining \rCompleted 296.5 MiB/703.1 MiB (168.6 MiB/s) with 1 file(s) remaining \rCompleted 296.8 MiB/703.1 MiB (168.6 MiB/s) with 1 file(s) remaining \rCompleted 297.0 MiB/703.1 MiB (168.7 MiB/s) with 1 file(s) remaining \rCompleted 297.2 MiB/703.1 MiB (168.8 MiB/s) with 1 file(s) remaining \rCompleted 297.5 MiB/703.1 MiB (168.7 MiB/s) with 1 file(s) remaining \rCompleted 297.8 MiB/703.1 MiB (168.8 MiB/s) with 1 file(s) remaining \rCompleted 298.0 MiB/703.1 MiB (168.8 MiB/s) with 1 file(s) remaining \rCompleted 298.2 MiB/703.1 MiB (168.9 MiB/s) with 1 file(s) remaining \rCompleted 298.5 MiB/703.1 MiB (169.1 MiB/s) with 1 file(s) remaining \rCompleted 298.8 MiB/703.1 MiB (169.1 MiB/s) with 1 file(s) remaining \rCompleted 299.0 MiB/703.1 MiB (169.2 MiB/s) with 1 file(s) remaining \rCompleted 299.2 MiB/703.1 MiB (169.4 MiB/s) with 1 file(s) remaining \rCompleted 299.5 MiB/703.1 MiB (169.4 MiB/s) with 1 file(s) remaining \rCompleted 299.8 MiB/703.1 MiB (169.5 MiB/s) with 1 file(s) remaining \rCompleted 300.0 MiB/703.1 MiB (169.6 MiB/s) with 1 file(s) remaining \rCompleted 300.2 MiB/703.1 MiB (169.6 MiB/s) with 1 file(s) remaining \rCompleted 300.5 MiB/703.1 MiB (169.7 MiB/s) with 1 file(s) remaining \rCompleted 300.8 MiB/703.1 MiB (169.7 MiB/s) with 1 file(s) remaining \rCompleted 301.0 MiB/703.1 MiB (169.7 MiB/s) with 1 file(s) remaining \rCompleted 301.2 MiB/703.1 MiB (169.9 MiB/s) with 1 file(s) remaining \rCompleted 301.5 MiB/703.1 MiB (170.0 MiB/s) with 1 file(s) remaining \rCompleted 301.8 MiB/703.1 MiB (170.0 MiB/s) with 1 file(s) remaining \rCompleted 302.0 MiB/703.1 MiB (170.1 MiB/s) with 1 file(s) remaining \rCompleted 302.2 MiB/703.1 MiB (170.1 MiB/s) with 1 file(s) remaining \rCompleted 302.5 MiB/703.1 MiB (170.2 MiB/s) with 1 file(s) remaining \rCompleted 302.8 MiB/703.1 MiB (170.2 MiB/s) with 1 file(s) remaining \rCompleted 303.0 MiB/703.1 MiB (170.3 MiB/s) with 1 file(s) remaining \rCompleted 303.2 MiB/703.1 MiB (170.4 MiB/s) with 1 file(s) remaining \rCompleted 303.5 MiB/703.1 MiB (170.5 MiB/s) with 1 file(s) remaining \rCompleted 303.8 MiB/703.1 MiB (170.6 MiB/s) with 1 file(s) remaining \rCompleted 304.0 MiB/703.1 MiB (170.6 MiB/s) with 1 file(s) remaining \rCompleted 304.2 MiB/703.1 MiB (170.7 MiB/s) with 1 file(s) remaining \rCompleted 304.5 MiB/703.1 MiB (170.9 MiB/s) with 1 file(s) remaining \rCompleted 304.8 MiB/703.1 MiB (171.0 MiB/s) with 1 file(s) remaining \rCompleted 305.0 MiB/703.1 MiB (171.0 MiB/s) with 1 file(s) remaining \rCompleted 305.2 MiB/703.1 MiB (171.1 MiB/s) with 1 file(s) remaining \rCompleted 305.5 MiB/703.1 MiB (171.2 MiB/s) with 1 file(s) remaining \rCompleted 305.8 MiB/703.1 MiB (171.1 MiB/s) with 1 file(s) remaining \rCompleted 306.0 MiB/703.1 MiB (171.2 MiB/s) with 1 file(s) remaining \rCompleted 306.2 MiB/703.1 MiB (171.3 MiB/s) with 1 file(s) remaining \rCompleted 306.5 MiB/703.1 MiB (171.3 MiB/s) with 1 file(s) remaining \rCompleted 306.8 MiB/703.1 MiB (171.4 MiB/s) with 1 file(s) remaining \rCompleted 307.0 MiB/703.1 MiB (171.4 MiB/s) with 1 file(s) remaining \rCompleted 307.2 MiB/703.1 MiB (171.5 MiB/s) with 1 file(s) remaining \rCompleted 307.5 MiB/703.1 MiB (171.7 MiB/s) with 1 file(s) remaining \rCompleted 307.8 MiB/703.1 MiB (171.7 MiB/s) with 1 file(s) remaining \rCompleted 308.0 MiB/703.1 MiB (171.8 MiB/s) with 1 file(s) remaining \rCompleted 308.2 MiB/703.1 MiB (171.8 MiB/s) with 1 file(s) remaining \rCompleted 308.5 MiB/703.1 MiB (171.9 MiB/s) with 1 file(s) remaining \rCompleted 308.8 MiB/703.1 MiB (172.0 MiB/s) with 1 file(s) remaining \rCompleted 309.0 MiB/703.1 MiB (172.1 MiB/s) with 1 file(s) remaining \rCompleted 309.2 MiB/703.1 MiB (172.2 MiB/s) with 1 file(s) remaining \rCompleted 309.5 MiB/703.1 MiB (172.2 MiB/s) with 1 file(s) remaining \rCompleted 309.8 MiB/703.1 MiB (172.3 MiB/s) with 1 file(s) remaining \rCompleted 310.0 MiB/703.1 MiB (172.4 MiB/s) with 1 file(s) remaining \rCompleted 310.2 MiB/703.1 MiB (172.4 MiB/s) with 1 file(s) remaining \rCompleted 310.5 MiB/703.1 MiB (172.4 MiB/s) with 1 file(s) remaining \rCompleted 310.8 MiB/703.1 MiB (172.5 MiB/s) with 1 file(s) remaining \rCompleted 311.0 MiB/703.1 MiB (172.6 MiB/s) with 1 file(s) remaining \rCompleted 311.2 MiB/703.1 MiB (172.4 MiB/s) with 1 file(s) remaining \rCompleted 311.5 MiB/703.1 MiB (172.5 MiB/s) with 1 file(s) remaining \rCompleted 311.8 MiB/703.1 MiB (172.6 MiB/s) with 1 file(s) remaining \rCompleted 312.0 MiB/703.1 MiB (172.6 MiB/s) with 1 file(s) remaining \rCompleted 312.2 MiB/703.1 MiB (172.6 MiB/s) with 1 file(s) remaining \rCompleted 312.5 MiB/703.1 MiB (172.7 MiB/s) with 1 file(s) remaining \rCompleted 312.8 MiB/703.1 MiB (172.8 MiB/s) with 1 file(s) remaining \rCompleted 313.0 MiB/703.1 MiB (172.7 MiB/s) with 1 file(s) remaining \rCompleted 313.2 MiB/703.1 MiB (172.7 MiB/s) with 1 file(s) remaining \rCompleted 313.5 MiB/703.1 MiB (172.8 MiB/s) with 1 file(s) remaining \rCompleted 313.8 MiB/703.1 MiB (172.8 MiB/s) with 1 file(s) remaining \rCompleted 314.0 MiB/703.1 MiB (172.9 MiB/s) with 1 file(s) remaining \rCompleted 314.2 MiB/703.1 MiB (173.1 MiB/s) with 1 file(s) remaining \rCompleted 314.5 MiB/703.1 MiB (173.2 MiB/s) with 1 file(s) remaining \rCompleted 314.8 MiB/703.1 MiB (173.2 MiB/s) with 1 file(s) remaining \rCompleted 315.0 MiB/703.1 MiB (173.2 MiB/s) with 1 file(s) remaining \rCompleted 315.2 MiB/703.1 MiB (173.3 MiB/s) with 1 file(s) remaining \rCompleted 315.5 MiB/703.1 MiB (173.3 MiB/s) with 1 file(s) remaining \rCompleted 315.8 MiB/703.1 MiB (173.5 MiB/s) with 1 file(s) remaining \rCompleted 316.0 MiB/703.1 MiB (173.6 MiB/s) with 1 file(s) remaining \rCompleted 316.2 MiB/703.1 MiB (173.6 MiB/s) with 1 file(s) remaining \rCompleted 316.5 MiB/703.1 MiB (173.6 MiB/s) with 1 file(s) remaining \rCompleted 316.8 MiB/703.1 MiB (173.7 MiB/s) with 1 file(s) remaining \rCompleted 317.0 MiB/703.1 MiB (173.8 MiB/s) with 1 file(s) remaining \rCompleted 317.2 MiB/703.1 MiB (173.9 MiB/s) with 1 file(s) remaining \rCompleted 317.5 MiB/703.1 MiB (174.0 MiB/s) with 1 file(s) remaining \rCompleted 317.8 MiB/703.1 MiB (174.0 MiB/s) with 1 file(s) remaining \rCompleted 318.0 MiB/703.1 MiB (174.1 MiB/s) with 1 file(s) remaining \rCompleted 318.2 MiB/703.1 MiB (174.1 MiB/s) with 1 file(s) remaining \rCompleted 318.5 MiB/703.1 MiB (174.3 MiB/s) with 1 file(s) remaining \rCompleted 318.8 MiB/703.1 MiB (174.3 MiB/s) with 1 file(s) remaining \rCompleted 319.0 MiB/703.1 MiB (174.4 MiB/s) with 1 file(s) remaining \rCompleted 319.2 MiB/703.1 MiB (174.5 MiB/s) with 1 file(s) remaining \rCompleted 319.5 MiB/703.1 MiB (174.5 MiB/s) with 1 file(s) remaining \rCompleted 319.8 MiB/703.1 MiB (174.6 MiB/s) with 1 file(s) remaining \rCompleted 320.0 MiB/703.1 MiB (174.7 MiB/s) with 1 file(s) remaining \rCompleted 320.2 MiB/703.1 MiB (174.7 MiB/s) with 1 file(s) remaining \rCompleted 320.5 MiB/703.1 MiB (174.8 MiB/s) with 1 file(s) remaining \rCompleted 320.8 MiB/703.1 MiB (174.9 MiB/s) with 1 file(s) remaining \rCompleted 321.0 MiB/703.1 MiB (175.0 MiB/s) with 1 file(s) remaining \rCompleted 321.2 MiB/703.1 MiB (175.1 MiB/s) with 1 file(s) remaining \rCompleted 321.5 MiB/703.1 MiB (175.2 MiB/s) with 1 file(s) remaining \rCompleted 321.8 MiB/703.1 MiB (175.3 MiB/s) with 1 file(s) remaining \rCompleted 322.0 MiB/703.1 MiB (175.4 MiB/s) with 1 file(s) remaining \rCompleted 322.2 MiB/703.1 MiB (175.4 MiB/s) with 1 file(s) remaining \rCompleted 322.5 MiB/703.1 MiB (175.5 MiB/s) with 1 file(s) remaining \rCompleted 322.8 MiB/703.1 MiB (175.5 MiB/s) with 1 file(s) remaining \rCompleted 323.0 MiB/703.1 MiB (175.6 MiB/s) with 1 file(s) remaining \rCompleted 323.2 MiB/703.1 MiB (175.7 MiB/s) with 1 file(s) remaining \rCompleted 323.5 MiB/703.1 MiB (175.8 MiB/s) with 1 file(s) remaining \rCompleted 323.8 MiB/703.1 MiB (175.9 MiB/s) with 1 file(s) remaining \rCompleted 324.0 MiB/703.1 MiB (176.0 MiB/s) with 1 file(s) remaining \rCompleted 324.2 MiB/703.1 MiB (176.0 MiB/s) with 1 file(s) remaining \rCompleted 324.5 MiB/703.1 MiB (176.0 MiB/s) with 1 file(s) remaining \rCompleted 324.8 MiB/703.1 MiB (176.1 MiB/s) with 1 file(s) remaining \rCompleted 325.0 MiB/703.1 MiB (176.3 MiB/s) with 1 file(s) remaining \rCompleted 325.2 MiB/703.1 MiB (176.4 MiB/s) with 1 file(s) remaining \rCompleted 325.5 MiB/703.1 MiB (176.4 MiB/s) with 1 file(s) remaining \rCompleted 325.8 MiB/703.1 MiB (176.5 MiB/s) with 1 file(s) remaining \rCompleted 326.0 MiB/703.1 MiB (176.6 MiB/s) with 1 file(s) remaining \rCompleted 326.2 MiB/703.1 MiB (176.6 MiB/s) with 1 file(s) remaining \rCompleted 326.5 MiB/703.1 MiB (176.7 MiB/s) with 1 file(s) remaining \rCompleted 326.8 MiB/703.1 MiB (176.8 MiB/s) with 1 file(s) remaining \rCompleted 327.0 MiB/703.1 MiB (176.8 MiB/s) with 1 file(s) remaining \rCompleted 327.2 MiB/703.1 MiB (176.9 MiB/s) with 1 file(s) remaining \rCompleted 327.5 MiB/703.1 MiB (176.9 MiB/s) with 1 file(s) remaining \rCompleted 327.8 MiB/703.1 MiB (177.0 MiB/s) with 1 file(s) remaining \rCompleted 328.0 MiB/703.1 MiB (177.1 MiB/s) with 1 file(s) remaining \rCompleted 328.2 MiB/703.1 MiB (177.3 MiB/s) with 1 file(s) remaining \rCompleted 328.5 MiB/703.1 MiB (177.4 MiB/s) with 1 file(s) remaining \rCompleted 328.8 MiB/703.1 MiB (177.5 MiB/s) with 1 file(s) remaining \rCompleted 329.0 MiB/703.1 MiB (177.5 MiB/s) with 1 file(s) remaining \rCompleted 329.2 MiB/703.1 MiB (177.5 MiB/s) with 1 file(s) remaining \rCompleted 329.5 MiB/703.1 MiB (177.6 MiB/s) with 1 file(s) remaining \rCompleted 329.8 MiB/703.1 MiB (177.7 MiB/s) with 1 file(s) remaining \rCompleted 330.0 MiB/703.1 MiB (177.8 MiB/s) with 1 file(s) remaining \rCompleted 330.2 MiB/703.1 MiB (177.9 MiB/s) with 1 file(s) remaining \rCompleted 330.5 MiB/703.1 MiB (177.9 MiB/s) with 1 file(s) remaining \rCompleted 330.8 MiB/703.1 MiB (178.0 MiB/s) with 1 file(s) remaining \rCompleted 331.0 MiB/703.1 MiB (178.1 MiB/s) with 1 file(s) remaining \rCompleted 331.2 MiB/703.1 MiB (178.1 MiB/s) with 1 file(s) remaining \rCompleted 331.5 MiB/703.1 MiB (178.2 MiB/s) with 1 file(s) remaining \rCompleted 331.8 MiB/703.1 MiB (178.3 MiB/s) with 1 file(s) remaining \rCompleted 332.0 MiB/703.1 MiB (178.3 MiB/s) with 1 file(s) remaining \rCompleted 332.2 MiB/703.1 MiB (178.4 MiB/s) with 1 file(s) remaining \rCompleted 332.5 MiB/703.1 MiB (178.4 MiB/s) with 1 file(s) remaining \rCompleted 332.8 MiB/703.1 MiB (178.5 MiB/s) with 1 file(s) remaining \rCompleted 333.0 MiB/703.1 MiB (178.5 MiB/s) with 1 file(s) remaining \rCompleted 333.2 MiB/703.1 MiB (178.5 MiB/s) with 1 file(s) remaining \rCompleted 333.5 MiB/703.1 MiB (178.7 MiB/s) with 1 file(s) remaining \rCompleted 333.8 MiB/703.1 MiB (178.8 MiB/s) with 1 file(s) remaining \rCompleted 334.0 MiB/703.1 MiB (178.9 MiB/s) with 1 file(s) remaining \rCompleted 334.2 MiB/703.1 MiB (178.9 MiB/s) with 1 file(s) remaining \rCompleted 334.5 MiB/703.1 MiB (178.9 MiB/s) with 1 file(s) remaining \rCompleted 334.8 MiB/703.1 MiB (178.9 MiB/s) with 1 file(s) remaining \rCompleted 335.0 MiB/703.1 MiB (179.0 MiB/s) with 1 file(s) remaining \rCompleted 335.2 MiB/703.1 MiB (179.1 MiB/s) with 1 file(s) remaining \rCompleted 335.5 MiB/703.1 MiB (179.2 MiB/s) with 1 file(s) remaining \rCompleted 335.8 MiB/703.1 MiB (179.3 MiB/s) with 1 file(s) remaining \rCompleted 336.0 MiB/703.1 MiB (179.4 MiB/s) with 1 file(s) remaining \rCompleted 336.2 MiB/703.1 MiB (179.5 MiB/s) with 1 file(s) remaining \rCompleted 336.5 MiB/703.1 MiB (179.4 MiB/s) with 1 file(s) remaining \rCompleted 336.8 MiB/703.1 MiB (179.5 MiB/s) with 1 file(s) remaining \rCompleted 337.0 MiB/703.1 MiB (179.6 MiB/s) with 1 file(s) remaining \rCompleted 337.2 MiB/703.1 MiB (179.7 MiB/s) with 1 file(s) remaining \rCompleted 337.5 MiB/703.1 MiB (179.8 MiB/s) with 1 file(s) remaining \rCompleted 337.8 MiB/703.1 MiB (179.8 MiB/s) with 1 file(s) remaining \rCompleted 338.0 MiB/703.1 MiB (179.9 MiB/s) with 1 file(s) remaining \rCompleted 338.2 MiB/703.1 MiB (179.9 MiB/s) with 1 file(s) remaining \rCompleted 338.5 MiB/703.1 MiB (180.0 MiB/s) with 1 file(s) remaining \rCompleted 338.8 MiB/703.1 MiB (180.1 MiB/s) with 1 file(s) remaining \rCompleted 339.0 MiB/703.1 MiB (180.2 MiB/s) with 1 file(s) remaining \rCompleted 339.2 MiB/703.1 MiB (180.3 MiB/s) with 1 file(s) remaining \rCompleted 339.5 MiB/703.1 MiB (180.3 MiB/s) with 1 file(s) remaining \rCompleted 339.8 MiB/703.1 MiB (180.3 MiB/s) with 1 file(s) remaining \rCompleted 340.0 MiB/703.1 MiB (180.4 MiB/s) with 1 file(s) remaining \rCompleted 340.2 MiB/703.1 MiB (180.4 MiB/s) with 1 file(s) remaining \rCompleted 340.5 MiB/703.1 MiB (180.5 MiB/s) with 1 file(s) remaining \rCompleted 340.8 MiB/703.1 MiB (180.6 MiB/s) with 1 file(s) remaining \rCompleted 341.0 MiB/703.1 MiB (180.7 MiB/s) with 1 file(s) remaining \rCompleted 341.2 MiB/703.1 MiB (180.8 MiB/s) with 1 file(s) remaining \rCompleted 341.5 MiB/703.1 MiB (180.9 MiB/s) with 1 file(s) remaining \rCompleted 341.8 MiB/703.1 MiB (180.9 MiB/s) with 1 file(s) remaining \rCompleted 342.0 MiB/703.1 MiB (180.9 MiB/s) with 1 file(s) remaining \rCompleted 342.2 MiB/703.1 MiB (181.0 MiB/s) with 1 file(s) remaining \rCompleted 342.5 MiB/703.1 MiB (181.1 MiB/s) with 1 file(s) remaining \rCompleted 342.8 MiB/703.1 MiB (181.2 MiB/s) with 1 file(s) remaining \rCompleted 343.0 MiB/703.1 MiB (181.3 MiB/s) with 1 file(s) remaining \rCompleted 343.2 MiB/703.1 MiB (181.3 MiB/s) with 1 file(s) remaining \rCompleted 343.5 MiB/703.1 MiB (181.4 MiB/s) with 1 file(s) remaining \rCompleted 343.8 MiB/703.1 MiB (181.5 MiB/s) with 1 file(s) remaining \rCompleted 344.0 MiB/703.1 MiB (181.4 MiB/s) with 1 file(s) remaining \rCompleted 344.2 MiB/703.1 MiB (181.5 MiB/s) with 1 file(s) remaining \rCompleted 344.5 MiB/703.1 MiB (181.5 MiB/s) with 1 file(s) remaining \rCompleted 344.8 MiB/703.1 MiB (181.6 MiB/s) with 1 file(s) remaining \rCompleted 345.0 MiB/703.1 MiB (181.7 MiB/s) with 1 file(s) remaining \rCompleted 345.2 MiB/703.1 MiB (181.8 MiB/s) with 1 file(s) remaining \rCompleted 345.5 MiB/703.1 MiB (181.8 MiB/s) with 1 file(s) remaining \rCompleted 345.8 MiB/703.1 MiB (181.9 MiB/s) with 1 file(s) remaining \rCompleted 346.0 MiB/703.1 MiB (181.9 MiB/s) with 1 file(s) remaining \rCompleted 346.2 MiB/703.1 MiB (181.9 MiB/s) with 1 file(s) remaining \rCompleted 346.5 MiB/703.1 MiB (182.0 MiB/s) with 1 file(s) remaining \rCompleted 346.8 MiB/703.1 MiB (182.1 MiB/s) with 1 file(s) remaining \rCompleted 347.0 MiB/703.1 MiB (182.2 MiB/s) with 1 file(s) remaining \rCompleted 347.2 MiB/703.1 MiB (182.4 MiB/s) with 1 file(s) remaining \rCompleted 347.5 MiB/703.1 MiB (182.5 MiB/s) with 1 file(s) remaining \rCompleted 347.8 MiB/703.1 MiB (182.6 MiB/s) with 1 file(s) remaining \rCompleted 348.0 MiB/703.1 MiB (182.7 MiB/s) with 1 file(s) remaining \rCompleted 348.2 MiB/703.1 MiB (182.6 MiB/s) with 1 file(s) remaining \rCompleted 348.5 MiB/703.1 MiB (182.6 MiB/s) with 1 file(s) remaining \rCompleted 348.8 MiB/703.1 MiB (182.7 MiB/s) with 1 file(s) remaining \rCompleted 349.0 MiB/703.1 MiB (182.8 MiB/s) with 1 file(s) remaining \rCompleted 349.2 MiB/703.1 MiB (182.9 MiB/s) with 1 file(s) remaining \rCompleted 349.5 MiB/703.1 MiB (183.0 MiB/s) with 1 file(s) remaining \rCompleted 349.8 MiB/703.1 MiB (183.1 MiB/s) with 1 file(s) remaining \rCompleted 350.0 MiB/703.1 MiB (183.2 MiB/s) with 1 file(s) remaining \rCompleted 350.2 MiB/703.1 MiB (183.2 MiB/s) with 1 file(s) remaining \rCompleted 350.5 MiB/703.1 MiB (183.1 MiB/s) with 1 file(s) remaining \rCompleted 350.8 MiB/703.1 MiB (183.2 MiB/s) with 1 file(s) remaining \rCompleted 351.0 MiB/703.1 MiB (183.3 MiB/s) with 1 file(s) remaining \rCompleted 351.2 MiB/703.1 MiB (183.3 MiB/s) with 1 file(s) remaining \rCompleted 351.5 MiB/703.1 MiB (183.4 MiB/s) with 1 file(s) remaining \rCompleted 351.8 MiB/703.1 MiB (183.5 MiB/s) with 1 file(s) remaining \rCompleted 352.0 MiB/703.1 MiB (183.5 MiB/s) with 1 file(s) remaining \rCompleted 352.2 MiB/703.1 MiB (183.5 MiB/s) with 1 file(s) remaining \rCompleted 352.5 MiB/703.1 MiB (183.6 MiB/s) with 1 file(s) remaining \rCompleted 352.8 MiB/703.1 MiB (183.7 MiB/s) with 1 file(s) remaining \rCompleted 353.0 MiB/703.1 MiB (183.8 MiB/s) with 1 file(s) remaining \rCompleted 353.2 MiB/703.1 MiB (183.7 MiB/s) with 1 file(s) remaining \rCompleted 353.5 MiB/703.1 MiB (183.6 MiB/s) with 1 file(s) remaining \rCompleted 353.8 MiB/703.1 MiB (183.7 MiB/s) with 1 file(s) remaining \rCompleted 354.0 MiB/703.1 MiB (183.8 MiB/s) with 1 file(s) remaining \rCompleted 354.2 MiB/703.1 MiB (183.8 MiB/s) with 1 file(s) remaining \rCompleted 354.5 MiB/703.1 MiB (183.9 MiB/s) with 1 file(s) remaining \rCompleted 354.8 MiB/703.1 MiB (183.9 MiB/s) with 1 file(s) remaining \rCompleted 355.0 MiB/703.1 MiB (184.0 MiB/s) with 1 file(s) remaining \rCompleted 355.2 MiB/703.1 MiB (184.1 MiB/s) with 1 file(s) remaining \rCompleted 355.5 MiB/703.1 MiB (184.2 MiB/s) with 1 file(s) remaining \rCompleted 355.8 MiB/703.1 MiB (184.2 MiB/s) with 1 file(s) remaining \rCompleted 356.0 MiB/703.1 MiB (184.3 MiB/s) with 1 file(s) remaining \rCompleted 356.2 MiB/703.1 MiB (184.4 MiB/s) with 1 file(s) remaining \rCompleted 356.5 MiB/703.1 MiB (184.3 MiB/s) with 1 file(s) remaining \rCompleted 356.8 MiB/703.1 MiB (184.4 MiB/s) with 1 file(s) remaining \rCompleted 357.0 MiB/703.1 MiB (184.5 MiB/s) with 1 file(s) remaining \rCompleted 357.2 MiB/703.1 MiB (184.5 MiB/s) with 1 file(s) remaining \rCompleted 357.5 MiB/703.1 MiB (184.6 MiB/s) with 1 file(s) remaining \rCompleted 357.8 MiB/703.1 MiB (184.5 MiB/s) with 1 file(s) remaining \rCompleted 358.0 MiB/703.1 MiB (184.5 MiB/s) with 1 file(s) remaining \rCompleted 358.2 MiB/703.1 MiB (184.6 MiB/s) with 1 file(s) remaining \rCompleted 358.5 MiB/703.1 MiB (184.7 MiB/s) with 1 file(s) remaining \rCompleted 358.8 MiB/703.1 MiB (184.8 MiB/s) with 1 file(s) remaining \rCompleted 359.0 MiB/703.1 MiB (184.9 MiB/s) with 1 file(s) remaining \rCompleted 359.2 MiB/703.1 MiB (184.9 MiB/s) with 1 file(s) remaining \rCompleted 359.5 MiB/703.1 MiB (185.0 MiB/s) with 1 file(s) remaining \rCompleted 359.8 MiB/703.1 MiB (185.0 MiB/s) with 1 file(s) remaining \rCompleted 360.0 MiB/703.1 MiB (185.1 MiB/s) with 1 file(s) remaining \rCompleted 360.2 MiB/703.1 MiB (185.2 MiB/s) with 1 file(s) remaining \rCompleted 360.5 MiB/703.1 MiB (185.3 MiB/s) with 1 file(s) remaining \rCompleted 360.8 MiB/703.1 MiB (185.3 MiB/s) with 1 file(s) remaining \rCompleted 361.0 MiB/703.1 MiB (185.4 MiB/s) with 1 file(s) remaining \rCompleted 361.2 MiB/703.1 MiB (185.4 MiB/s) with 1 file(s) remaining \rCompleted 361.5 MiB/703.1 MiB (185.5 MiB/s) with 1 file(s) remaining \rCompleted 361.8 MiB/703.1 MiB (185.6 MiB/s) with 1 file(s) remaining \rCompleted 362.0 MiB/703.1 MiB (185.7 MiB/s) with 1 file(s) remaining \rCompleted 362.2 MiB/703.1 MiB (185.8 MiB/s) with 1 file(s) remaining \rCompleted 362.5 MiB/703.1 MiB (185.9 MiB/s) with 1 file(s) remaining \rCompleted 362.8 MiB/703.1 MiB (186.0 MiB/s) with 1 file(s) remaining \rCompleted 363.0 MiB/703.1 MiB (185.9 MiB/s) with 1 file(s) remaining \rCompleted 363.2 MiB/703.1 MiB (185.8 MiB/s) with 1 file(s) remaining \rCompleted 363.5 MiB/703.1 MiB (185.9 MiB/s) with 1 file(s) remaining \rCompleted 363.8 MiB/703.1 MiB (186.0 MiB/s) with 1 file(s) remaining \rCompleted 364.0 MiB/703.1 MiB (186.1 MiB/s) with 1 file(s) remaining \rCompleted 364.2 MiB/703.1 MiB (186.2 MiB/s) with 1 file(s) remaining \rCompleted 364.5 MiB/703.1 MiB (186.1 MiB/s) with 1 file(s) remaining \rCompleted 364.8 MiB/703.1 MiB (186.2 MiB/s) with 1 file(s) remaining \rCompleted 365.0 MiB/703.1 MiB (186.2 MiB/s) with 1 file(s) remaining \rCompleted 365.2 MiB/703.1 MiB (186.3 MiB/s) with 1 file(s) remaining \rCompleted 365.5 MiB/703.1 MiB (186.4 MiB/s) with 1 file(s) remaining \rCompleted 365.8 MiB/703.1 MiB (186.4 MiB/s) with 1 file(s) remaining \rCompleted 366.0 MiB/703.1 MiB (186.4 MiB/s) with 1 file(s) remaining \rCompleted 366.2 MiB/703.1 MiB (186.6 MiB/s) with 1 file(s) remaining \rCompleted 366.5 MiB/703.1 MiB (186.6 MiB/s) with 1 file(s) remaining \rCompleted 366.8 MiB/703.1 MiB (186.7 MiB/s) with 1 file(s) remaining \rCompleted 367.0 MiB/703.1 MiB (186.7 MiB/s) with 1 file(s) remaining \rCompleted 367.2 MiB/703.1 MiB (186.8 MiB/s) with 1 file(s) remaining \rCompleted 367.5 MiB/703.1 MiB (186.7 MiB/s) with 1 file(s) remaining \rCompleted 367.8 MiB/703.1 MiB (186.8 MiB/s) with 1 file(s) remaining \rCompleted 368.0 MiB/703.1 MiB (186.9 MiB/s) with 1 file(s) remaining \rCompleted 368.2 MiB/703.1 MiB (187.0 MiB/s) with 1 file(s) remaining \rCompleted 368.5 MiB/703.1 MiB (187.0 MiB/s) with 1 file(s) remaining \rCompleted 368.8 MiB/703.1 MiB (187.1 MiB/s) with 1 file(s) remaining \rCompleted 369.0 MiB/703.1 MiB (187.2 MiB/s) with 1 file(s) remaining \rCompleted 369.2 MiB/703.1 MiB (187.3 MiB/s) with 1 file(s) remaining \rCompleted 369.5 MiB/703.1 MiB (187.3 MiB/s) with 1 file(s) remaining \rCompleted 369.8 MiB/703.1 MiB (187.4 MiB/s) with 1 file(s) remaining \rCompleted 370.0 MiB/703.1 MiB (187.5 MiB/s) with 1 file(s) remaining \rCompleted 370.2 MiB/703.1 MiB (187.5 MiB/s) with 1 file(s) remaining \rCompleted 370.5 MiB/703.1 MiB (187.6 MiB/s) with 1 file(s) remaining \rCompleted 370.8 MiB/703.1 MiB (187.7 MiB/s) with 1 file(s) remaining \rCompleted 371.0 MiB/703.1 MiB (187.8 MiB/s) with 1 file(s) remaining \rCompleted 371.2 MiB/703.1 MiB (187.8 MiB/s) with 1 file(s) remaining \rCompleted 371.5 MiB/703.1 MiB (187.9 MiB/s) with 1 file(s) remaining \rCompleted 371.8 MiB/703.1 MiB (188.0 MiB/s) with 1 file(s) remaining \rCompleted 372.0 MiB/703.1 MiB (188.0 MiB/s) with 1 file(s) remaining \rCompleted 372.2 MiB/703.1 MiB (188.0 MiB/s) with 1 file(s) remaining \rCompleted 372.5 MiB/703.1 MiB (188.1 MiB/s) with 1 file(s) remaining \rCompleted 372.8 MiB/703.1 MiB (188.3 MiB/s) with 1 file(s) remaining \rCompleted 373.0 MiB/703.1 MiB (188.3 MiB/s) with 1 file(s) remaining \rCompleted 373.2 MiB/703.1 MiB (188.4 MiB/s) with 1 file(s) remaining \rCompleted 373.5 MiB/703.1 MiB (188.5 MiB/s) with 1 file(s) remaining \rCompleted 373.8 MiB/703.1 MiB (188.6 MiB/s) with 1 file(s) remaining \rCompleted 374.0 MiB/703.1 MiB (188.7 MiB/s) with 1 file(s) remaining \rCompleted 374.2 MiB/703.1 MiB (188.8 MiB/s) with 1 file(s) remaining \rCompleted 374.5 MiB/703.1 MiB (188.8 MiB/s) with 1 file(s) remaining \rCompleted 374.8 MiB/703.1 MiB (188.9 MiB/s) with 1 file(s) remaining \rCompleted 375.0 MiB/703.1 MiB (189.0 MiB/s) with 1 file(s) remaining \rCompleted 375.2 MiB/703.1 MiB (189.0 MiB/s) with 1 file(s) remaining \rCompleted 375.5 MiB/703.1 MiB (189.2 MiB/s) with 1 file(s) remaining \rCompleted 375.8 MiB/703.1 MiB (189.3 MiB/s) with 1 file(s) remaining \rCompleted 376.0 MiB/703.1 MiB (189.3 MiB/s) with 1 file(s) remaining \rCompleted 376.2 MiB/703.1 MiB (189.4 MiB/s) with 1 file(s) remaining \rCompleted 376.5 MiB/703.1 MiB (189.5 MiB/s) with 1 file(s) remaining \rCompleted 376.8 MiB/703.1 MiB (189.6 MiB/s) with 1 file(s) remaining \rCompleted 377.0 MiB/703.1 MiB (189.7 MiB/s) with 1 file(s) remaining \rCompleted 377.2 MiB/703.1 MiB (189.8 MiB/s) with 1 file(s) remaining \rCompleted 377.5 MiB/703.1 MiB (189.8 MiB/s) with 1 file(s) remaining \rCompleted 377.8 MiB/703.1 MiB (189.9 MiB/s) with 1 file(s) remaining \rCompleted 378.0 MiB/703.1 MiB (190.0 MiB/s) with 1 file(s) remaining \rCompleted 378.2 MiB/703.1 MiB (190.0 MiB/s) with 1 file(s) remaining \rCompleted 378.5 MiB/703.1 MiB (190.0 MiB/s) with 1 file(s) remaining \rCompleted 378.8 MiB/703.1 MiB (190.1 MiB/s) with 1 file(s) remaining \rCompleted 379.0 MiB/703.1 MiB (190.2 MiB/s) with 1 file(s) remaining \rCompleted 379.2 MiB/703.1 MiB (190.3 MiB/s) with 1 file(s) remaining \rCompleted 379.5 MiB/703.1 MiB (190.3 MiB/s) with 1 file(s) remaining \rCompleted 379.8 MiB/703.1 MiB (190.5 MiB/s) with 1 file(s) remaining \rCompleted 380.0 MiB/703.1 MiB (190.5 MiB/s) with 1 file(s) remaining \rCompleted 380.2 MiB/703.1 MiB (190.6 MiB/s) with 1 file(s) remaining \rCompleted 380.5 MiB/703.1 MiB (190.7 MiB/s) with 1 file(s) remaining \rCompleted 380.8 MiB/703.1 MiB (190.8 MiB/s) with 1 file(s) remaining \rCompleted 381.0 MiB/703.1 MiB (190.8 MiB/s) with 1 file(s) remaining \rCompleted 381.2 MiB/703.1 MiB (191.0 MiB/s) with 1 file(s) remaining \rCompleted 381.5 MiB/703.1 MiB (191.0 MiB/s) with 1 file(s) remaining \rCompleted 381.8 MiB/703.1 MiB (191.1 MiB/s) with 1 file(s) remaining \rCompleted 382.0 MiB/703.1 MiB (190.5 MiB/s) with 1 file(s) remaining \rCompleted 382.2 MiB/703.1 MiB (190.6 MiB/s) with 1 file(s) remaining \rCompleted 382.5 MiB/703.1 MiB (190.6 MiB/s) with 1 file(s) remaining \rCompleted 382.8 MiB/703.1 MiB (190.7 MiB/s) with 1 file(s) remaining \rCompleted 383.0 MiB/703.1 MiB (190.8 MiB/s) with 1 file(s) remaining \rCompleted 383.2 MiB/703.1 MiB (190.7 MiB/s) with 1 file(s) remaining \rCompleted 383.5 MiB/703.1 MiB (190.7 MiB/s) with 1 file(s) remaining \rCompleted 383.8 MiB/703.1 MiB (190.8 MiB/s) with 1 file(s) remaining \rCompleted 384.0 MiB/703.1 MiB (190.9 MiB/s) with 1 file(s) remaining \rCompleted 384.2 MiB/703.1 MiB (190.9 MiB/s) with 1 file(s) remaining \rCompleted 384.5 MiB/703.1 MiB (191.0 MiB/s) with 1 file(s) remaining \rCompleted 384.8 MiB/703.1 MiB (191.1 MiB/s) with 1 file(s) remaining \rCompleted 385.0 MiB/703.1 MiB (191.1 MiB/s) with 1 file(s) remaining \rCompleted 385.2 MiB/703.1 MiB (191.3 MiB/s) with 1 file(s) remaining \rCompleted 385.5 MiB/703.1 MiB (191.3 MiB/s) with 1 file(s) remaining \rCompleted 385.8 MiB/703.1 MiB (190.7 MiB/s) with 1 file(s) remaining \rCompleted 386.0 MiB/703.1 MiB (190.8 MiB/s) with 1 file(s) remaining \rCompleted 386.2 MiB/703.1 MiB (190.8 MiB/s) with 1 file(s) remaining \rCompleted 386.5 MiB/703.1 MiB (190.9 MiB/s) with 1 file(s) remaining \rCompleted 386.8 MiB/703.1 MiB (190.8 MiB/s) with 1 file(s) remaining \rCompleted 387.0 MiB/703.1 MiB (190.9 MiB/s) with 1 file(s) remaining \rCompleted 387.2 MiB/703.1 MiB (190.9 MiB/s) with 1 file(s) remaining \rCompleted 387.5 MiB/703.1 MiB (191.0 MiB/s) with 1 file(s) remaining \rCompleted 387.8 MiB/703.1 MiB (191.1 MiB/s) with 1 file(s) remaining \rCompleted 388.0 MiB/703.1 MiB (191.2 MiB/s) with 1 file(s) remaining \rCompleted 388.2 MiB/703.1 MiB (191.2 MiB/s) with 1 file(s) remaining \rCompleted 388.5 MiB/703.1 MiB (191.3 MiB/s) with 1 file(s) remaining \rCompleted 388.8 MiB/703.1 MiB (190.9 MiB/s) with 1 file(s) remaining \rCompleted 389.0 MiB/703.1 MiB (190.9 MiB/s) with 1 file(s) remaining \rCompleted 389.2 MiB/703.1 MiB (190.9 MiB/s) with 1 file(s) remaining \rCompleted 389.5 MiB/703.1 MiB (191.0 MiB/s) with 1 file(s) remaining \rCompleted 389.8 MiB/703.1 MiB (191.0 MiB/s) with 1 file(s) remaining \rCompleted 390.0 MiB/703.1 MiB (191.0 MiB/s) with 1 file(s) remaining \rCompleted 390.2 MiB/703.1 MiB (191.1 MiB/s) with 1 file(s) remaining \rCompleted 390.5 MiB/703.1 MiB (191.1 MiB/s) with 1 file(s) remaining \rCompleted 390.8 MiB/703.1 MiB (191.2 MiB/s) with 1 file(s) remaining \rCompleted 391.0 MiB/703.1 MiB (191.1 MiB/s) with 1 file(s) remaining \rCompleted 391.2 MiB/703.1 MiB (191.2 MiB/s) with 1 file(s) remaining \rCompleted 391.5 MiB/703.1 MiB (191.2 MiB/s) with 1 file(s) remaining \rCompleted 391.8 MiB/703.1 MiB (191.3 MiB/s) with 1 file(s) remaining \rCompleted 392.0 MiB/703.1 MiB (191.4 MiB/s) with 1 file(s) remaining \rCompleted 392.2 MiB/703.1 MiB (191.5 MiB/s) with 1 file(s) remaining \rCompleted 392.5 MiB/703.1 MiB (191.5 MiB/s) with 1 file(s) remaining \rCompleted 392.8 MiB/703.1 MiB (191.6 MiB/s) with 1 file(s) remaining \rCompleted 393.0 MiB/703.1 MiB (191.7 MiB/s) with 1 file(s) remaining \rCompleted 393.2 MiB/703.1 MiB (191.7 MiB/s) with 1 file(s) remaining \rCompleted 393.5 MiB/703.1 MiB (191.8 MiB/s) with 1 file(s) remaining \rCompleted 393.8 MiB/703.1 MiB (191.9 MiB/s) with 1 file(s) remaining \rCompleted 394.0 MiB/703.1 MiB (191.9 MiB/s) with 1 file(s) remaining \rCompleted 394.2 MiB/703.1 MiB (192.0 MiB/s) with 1 file(s) remaining \rCompleted 394.5 MiB/703.1 MiB (192.1 MiB/s) with 1 file(s) remaining \rCompleted 394.8 MiB/703.1 MiB (192.1 MiB/s) with 1 file(s) remaining \rCompleted 395.0 MiB/703.1 MiB (192.2 MiB/s) with 1 file(s) remaining \rCompleted 395.2 MiB/703.1 MiB (192.2 MiB/s) with 1 file(s) remaining \rCompleted 395.5 MiB/703.1 MiB (192.3 MiB/s) with 1 file(s) remaining \rCompleted 395.8 MiB/703.1 MiB (192.3 MiB/s) with 1 file(s) remaining \rCompleted 396.0 MiB/703.1 MiB (192.3 MiB/s) with 1 file(s) remaining \rCompleted 396.2 MiB/703.1 MiB (192.4 MiB/s) with 1 file(s) remaining \rCompleted 396.5 MiB/703.1 MiB (192.5 MiB/s) with 1 file(s) remaining \rCompleted 396.8 MiB/703.1 MiB (192.5 MiB/s) with 1 file(s) remaining \rCompleted 397.0 MiB/703.1 MiB (192.6 MiB/s) with 1 file(s) remaining \rCompleted 397.2 MiB/703.1 MiB (192.6 MiB/s) with 1 file(s) remaining \rCompleted 397.5 MiB/703.1 MiB (192.7 MiB/s) with 1 file(s) remaining \rCompleted 397.8 MiB/703.1 MiB (192.7 MiB/s) with 1 file(s) remaining \rCompleted 398.0 MiB/703.1 MiB (192.7 MiB/s) with 1 file(s) remaining \rCompleted 398.2 MiB/703.1 MiB (192.8 MiB/s) with 1 file(s) remaining \rCompleted 398.5 MiB/703.1 MiB (192.8 MiB/s) with 1 file(s) remaining \rCompleted 398.8 MiB/703.1 MiB (192.9 MiB/s) with 1 file(s) remaining \rCompleted 399.0 MiB/703.1 MiB (193.0 MiB/s) with 1 file(s) remaining \rCompleted 399.2 MiB/703.1 MiB (193.1 MiB/s) with 1 file(s) remaining \rCompleted 399.5 MiB/703.1 MiB (193.0 MiB/s) with 1 file(s) remaining \rCompleted 399.8 MiB/703.1 MiB (193.1 MiB/s) with 1 file(s) remaining \rCompleted 400.0 MiB/703.1 MiB (193.2 MiB/s) with 1 file(s) remaining \rCompleted 400.2 MiB/703.1 MiB (193.2 MiB/s) with 1 file(s) remaining \rCompleted 400.5 MiB/703.1 MiB (193.3 MiB/s) with 1 file(s) remaining \rCompleted 400.8 MiB/703.1 MiB (193.3 MiB/s) with 1 file(s) remaining \rCompleted 401.0 MiB/703.1 MiB (193.5 MiB/s) with 1 file(s) remaining \rCompleted 401.2 MiB/703.1 MiB (193.5 MiB/s) with 1 file(s) remaining \rCompleted 401.5 MiB/703.1 MiB (193.6 MiB/s) with 1 file(s) remaining \rCompleted 401.8 MiB/703.1 MiB (193.6 MiB/s) with 1 file(s) remaining \rCompleted 402.0 MiB/703.1 MiB (193.7 MiB/s) with 1 file(s) remaining \rCompleted 402.2 MiB/703.1 MiB (193.8 MiB/s) with 1 file(s) remaining \rCompleted 402.5 MiB/703.1 MiB (193.7 MiB/s) with 1 file(s) remaining \rCompleted 402.8 MiB/703.1 MiB (193.8 MiB/s) with 1 file(s) remaining \rCompleted 403.0 MiB/703.1 MiB (193.9 MiB/s) with 1 file(s) remaining \rCompleted 403.2 MiB/703.1 MiB (193.9 MiB/s) with 1 file(s) remaining \rCompleted 403.5 MiB/703.1 MiB (194.0 MiB/s) with 1 file(s) remaining \rCompleted 403.8 MiB/703.1 MiB (194.1 MiB/s) with 1 file(s) remaining \rCompleted 404.0 MiB/703.1 MiB (194.2 MiB/s) with 1 file(s) remaining \rCompleted 404.2 MiB/703.1 MiB (194.2 MiB/s) with 1 file(s) remaining \rCompleted 404.5 MiB/703.1 MiB (194.2 MiB/s) with 1 file(s) remaining \rCompleted 404.8 MiB/703.1 MiB (194.3 MiB/s) with 1 file(s) remaining \rCompleted 405.0 MiB/703.1 MiB (194.4 MiB/s) with 1 file(s) remaining \rCompleted 405.2 MiB/703.1 MiB (194.4 MiB/s) with 1 file(s) remaining \rCompleted 405.5 MiB/703.1 MiB (194.5 MiB/s) with 1 file(s) remaining \rCompleted 405.8 MiB/703.1 MiB (194.6 MiB/s) with 1 file(s) remaining \rCompleted 406.0 MiB/703.1 MiB (194.7 MiB/s) with 1 file(s) remaining \rCompleted 406.2 MiB/703.1 MiB (194.8 MiB/s) with 1 file(s) remaining \rCompleted 406.5 MiB/703.1 MiB (194.7 MiB/s) with 1 file(s) remaining \rCompleted 406.8 MiB/703.1 MiB (194.8 MiB/s) with 1 file(s) remaining \rCompleted 407.0 MiB/703.1 MiB (194.9 MiB/s) with 1 file(s) remaining \rCompleted 407.2 MiB/703.1 MiB (194.9 MiB/s) with 1 file(s) remaining \rCompleted 407.5 MiB/703.1 MiB (195.1 MiB/s) with 1 file(s) remaining \rCompleted 407.8 MiB/703.1 MiB (195.1 MiB/s) with 1 file(s) remaining \rCompleted 408.0 MiB/703.1 MiB (195.2 MiB/s) with 1 file(s) remaining \rCompleted 408.2 MiB/703.1 MiB (195.1 MiB/s) with 1 file(s) remaining \rCompleted 408.5 MiB/703.1 MiB (195.2 MiB/s) with 1 file(s) remaining \rCompleted 408.8 MiB/703.1 MiB (195.3 MiB/s) with 1 file(s) remaining \rCompleted 409.0 MiB/703.1 MiB (195.3 MiB/s) with 1 file(s) remaining \rCompleted 409.2 MiB/703.1 MiB (195.3 MiB/s) with 1 file(s) remaining \rCompleted 409.5 MiB/703.1 MiB (195.4 MiB/s) with 1 file(s) remaining \rCompleted 409.8 MiB/703.1 MiB (195.5 MiB/s) with 1 file(s) remaining \rCompleted 410.0 MiB/703.1 MiB (195.6 MiB/s) with 1 file(s) remaining \rCompleted 410.2 MiB/703.1 MiB (195.6 MiB/s) with 1 file(s) remaining \rCompleted 410.5 MiB/703.1 MiB (195.7 MiB/s) with 1 file(s) remaining \rCompleted 410.8 MiB/703.1 MiB (195.7 MiB/s) with 1 file(s) remaining \rCompleted 411.0 MiB/703.1 MiB (195.8 MiB/s) with 1 file(s) remaining \rCompleted 411.2 MiB/703.1 MiB (195.8 MiB/s) with 1 file(s) remaining \rCompleted 411.5 MiB/703.1 MiB (195.9 MiB/s) with 1 file(s) remaining \rCompleted 411.8 MiB/703.1 MiB (195.9 MiB/s) with 1 file(s) remaining \rCompleted 412.0 MiB/703.1 MiB (196.0 MiB/s) with 1 file(s) remaining \rCompleted 412.2 MiB/703.1 MiB (196.1 MiB/s) with 1 file(s) remaining \rCompleted 412.5 MiB/703.1 MiB (196.0 MiB/s) with 1 file(s) remaining \rCompleted 412.8 MiB/703.1 MiB (196.1 MiB/s) with 1 file(s) remaining \rCompleted 413.0 MiB/703.1 MiB (196.2 MiB/s) with 1 file(s) remaining \rCompleted 413.2 MiB/703.1 MiB (196.2 MiB/s) with 1 file(s) remaining \rCompleted 413.5 MiB/703.1 MiB (196.2 MiB/s) with 1 file(s) remaining \rCompleted 413.8 MiB/703.1 MiB (196.3 MiB/s) with 1 file(s) remaining \rCompleted 414.0 MiB/703.1 MiB (196.4 MiB/s) with 1 file(s) remaining \rCompleted 414.2 MiB/703.1 MiB (196.4 MiB/s) with 1 file(s) remaining \rCompleted 414.5 MiB/703.1 MiB (196.5 MiB/s) with 1 file(s) remaining \rCompleted 414.8 MiB/703.1 MiB (196.6 MiB/s) with 1 file(s) remaining \rCompleted 415.0 MiB/703.1 MiB (196.7 MiB/s) with 1 file(s) remaining \rCompleted 415.2 MiB/703.1 MiB (196.8 MiB/s) with 1 file(s) remaining \rCompleted 415.5 MiB/703.1 MiB (196.8 MiB/s) with 1 file(s) remaining \rCompleted 415.8 MiB/703.1 MiB (196.9 MiB/s) with 1 file(s) remaining \rCompleted 416.0 MiB/703.1 MiB (196.9 MiB/s) with 1 file(s) remaining \rCompleted 416.2 MiB/703.1 MiB (196.9 MiB/s) with 1 file(s) remaining \rCompleted 416.5 MiB/703.1 MiB (196.8 MiB/s) with 1 file(s) remaining \rCompleted 416.8 MiB/703.1 MiB (196.9 MiB/s) with 1 file(s) remaining \rCompleted 417.0 MiB/703.1 MiB (196.9 MiB/s) with 1 file(s) remaining \rCompleted 417.2 MiB/703.1 MiB (196.8 MiB/s) with 1 file(s) remaining \rCompleted 417.5 MiB/703.1 MiB (196.9 MiB/s) with 1 file(s) remaining \rCompleted 417.8 MiB/703.1 MiB (197.0 MiB/s) with 1 file(s) remaining \rCompleted 418.0 MiB/703.1 MiB (197.0 MiB/s) with 1 file(s) remaining \rCompleted 418.2 MiB/703.1 MiB (196.9 MiB/s) with 1 file(s) remaining \rCompleted 418.5 MiB/703.1 MiB (196.9 MiB/s) with 1 file(s) remaining \rCompleted 418.8 MiB/703.1 MiB (196.9 MiB/s) with 1 file(s) remaining \rCompleted 419.0 MiB/703.1 MiB (197.0 MiB/s) with 1 file(s) remaining \rCompleted 419.2 MiB/703.1 MiB (197.0 MiB/s) with 1 file(s) remaining \rCompleted 419.5 MiB/703.1 MiB (197.1 MiB/s) with 1 file(s) remaining \rCompleted 419.8 MiB/703.1 MiB (197.2 MiB/s) with 1 file(s) remaining \rCompleted 420.0 MiB/703.1 MiB (197.2 MiB/s) with 1 file(s) remaining \rCompleted 420.2 MiB/703.1 MiB (197.3 MiB/s) with 1 file(s) remaining \rCompleted 420.5 MiB/703.1 MiB (197.4 MiB/s) with 1 file(s) remaining \rCompleted 420.8 MiB/703.1 MiB (197.4 MiB/s) with 1 file(s) remaining \rCompleted 421.0 MiB/703.1 MiB (197.4 MiB/s) with 1 file(s) remaining \rCompleted 421.2 MiB/703.1 MiB (197.4 MiB/s) with 1 file(s) remaining \rCompleted 421.5 MiB/703.1 MiB (197.5 MiB/s) with 1 file(s) remaining \rCompleted 421.8 MiB/703.1 MiB (197.6 MiB/s) with 1 file(s) remaining \rCompleted 422.0 MiB/703.1 MiB (197.6 MiB/s) with 1 file(s) remaining \rCompleted 422.2 MiB/703.1 MiB (197.6 MiB/s) with 1 file(s) remaining \rCompleted 422.5 MiB/703.1 MiB (197.6 MiB/s) with 1 file(s) remaining \rCompleted 422.8 MiB/703.1 MiB (197.7 MiB/s) with 1 file(s) remaining \rCompleted 423.0 MiB/703.1 MiB (197.8 MiB/s) with 1 file(s) remaining \rCompleted 423.2 MiB/703.1 MiB (197.8 MiB/s) with 1 file(s) remaining \rCompleted 423.5 MiB/703.1 MiB (197.9 MiB/s) with 1 file(s) remaining \rCompleted 423.8 MiB/703.1 MiB (198.0 MiB/s) with 1 file(s) remaining \rCompleted 424.0 MiB/703.1 MiB (198.0 MiB/s) with 1 file(s) remaining \rCompleted 424.2 MiB/703.1 MiB (198.0 MiB/s) with 1 file(s) remaining \rCompleted 424.5 MiB/703.1 MiB (198.0 MiB/s) with 1 file(s) remaining \rCompleted 424.8 MiB/703.1 MiB (198.1 MiB/s) with 1 file(s) remaining \rCompleted 425.0 MiB/703.1 MiB (198.1 MiB/s) with 1 file(s) remaining \rCompleted 425.2 MiB/703.1 MiB (198.1 MiB/s) with 1 file(s) remaining \rCompleted 425.5 MiB/703.1 MiB (198.2 MiB/s) with 1 file(s) remaining \rCompleted 425.8 MiB/703.1 MiB (198.3 MiB/s) with 1 file(s) remaining \rCompleted 426.0 MiB/703.1 MiB (198.4 MiB/s) with 1 file(s) remaining \rCompleted 426.2 MiB/703.1 MiB (198.5 MiB/s) with 1 file(s) remaining \rCompleted 426.5 MiB/703.1 MiB (198.4 MiB/s) with 1 file(s) remaining \rCompleted 426.8 MiB/703.1 MiB (198.4 MiB/s) with 1 file(s) remaining \rCompleted 427.0 MiB/703.1 MiB (198.5 MiB/s) with 1 file(s) remaining \rCompleted 427.2 MiB/703.1 MiB (198.6 MiB/s) with 1 file(s) remaining \rCompleted 427.5 MiB/703.1 MiB (198.7 MiB/s) with 1 file(s) remaining \rCompleted 427.8 MiB/703.1 MiB (198.8 MiB/s) with 1 file(s) remaining \rCompleted 428.0 MiB/703.1 MiB (198.8 MiB/s) with 1 file(s) remaining \rCompleted 428.2 MiB/703.1 MiB (198.8 MiB/s) with 1 file(s) remaining \rCompleted 428.5 MiB/703.1 MiB (198.9 MiB/s) with 1 file(s) remaining \rCompleted 428.8 MiB/703.1 MiB (198.9 MiB/s) with 1 file(s) remaining \rCompleted 429.0 MiB/703.1 MiB (198.9 MiB/s) with 1 file(s) remaining \rCompleted 429.2 MiB/703.1 MiB (199.0 MiB/s) with 1 file(s) remaining \rCompleted 429.5 MiB/703.1 MiB (199.1 MiB/s) with 1 file(s) remaining \rCompleted 429.8 MiB/703.1 MiB (199.1 MiB/s) with 1 file(s) remaining \rCompleted 430.0 MiB/703.1 MiB (199.2 MiB/s) with 1 file(s) remaining \rCompleted 430.2 MiB/703.1 MiB (199.2 MiB/s) with 1 file(s) remaining \rCompleted 430.5 MiB/703.1 MiB (199.3 MiB/s) with 1 file(s) remaining \rCompleted 430.8 MiB/703.1 MiB (199.3 MiB/s) with 1 file(s) remaining \rCompleted 431.0 MiB/703.1 MiB (199.4 MiB/s) with 1 file(s) remaining \rCompleted 431.2 MiB/703.1 MiB (199.5 MiB/s) with 1 file(s) remaining \rCompleted 431.5 MiB/703.1 MiB (199.5 MiB/s) with 1 file(s) remaining \rCompleted 431.8 MiB/703.1 MiB (199.5 MiB/s) with 1 file(s) remaining \rCompleted 432.0 MiB/703.1 MiB (199.6 MiB/s) with 1 file(s) remaining \rCompleted 432.2 MiB/703.1 MiB (199.6 MiB/s) with 1 file(s) remaining \rCompleted 432.5 MiB/703.1 MiB (199.7 MiB/s) with 1 file(s) remaining \rCompleted 432.8 MiB/703.1 MiB (199.8 MiB/s) with 1 file(s) remaining \rCompleted 433.0 MiB/703.1 MiB (199.7 MiB/s) with 1 file(s) remaining \rCompleted 433.2 MiB/703.1 MiB (199.7 MiB/s) with 1 file(s) remaining \rCompleted 433.5 MiB/703.1 MiB (199.8 MiB/s) with 1 file(s) remaining \rCompleted 433.8 MiB/703.1 MiB (199.7 MiB/s) with 1 file(s) remaining \rCompleted 434.0 MiB/703.1 MiB (199.8 MiB/s) with 1 file(s) remaining \rCompleted 434.2 MiB/703.1 MiB (199.8 MiB/s) with 1 file(s) remaining \rCompleted 434.5 MiB/703.1 MiB (199.8 MiB/s) with 1 file(s) remaining \rCompleted 434.8 MiB/703.1 MiB (199.9 MiB/s) with 1 file(s) remaining \rCompleted 435.0 MiB/703.1 MiB (200.0 MiB/s) with 1 file(s) remaining \rCompleted 435.2 MiB/703.1 MiB (200.0 MiB/s) with 1 file(s) remaining \rCompleted 435.5 MiB/703.1 MiB (200.1 MiB/s) with 1 file(s) remaining \rCompleted 435.8 MiB/703.1 MiB (200.2 MiB/s) with 1 file(s) remaining \rCompleted 436.0 MiB/703.1 MiB (200.2 MiB/s) with 1 file(s) remaining \rCompleted 436.2 MiB/703.1 MiB (200.2 MiB/s) with 1 file(s) remaining \rCompleted 436.5 MiB/703.1 MiB (200.2 MiB/s) with 1 file(s) remaining \rCompleted 436.8 MiB/703.1 MiB (200.2 MiB/s) with 1 file(s) remaining \rCompleted 437.0 MiB/703.1 MiB (200.2 MiB/s) with 1 file(s) remaining \rCompleted 437.2 MiB/703.1 MiB (200.3 MiB/s) with 1 file(s) remaining \rCompleted 437.5 MiB/703.1 MiB (200.4 MiB/s) with 1 file(s) remaining \rCompleted 437.8 MiB/703.1 MiB (200.5 MiB/s) with 1 file(s) remaining \rCompleted 438.0 MiB/703.1 MiB (200.5 MiB/s) with 1 file(s) remaining \rCompleted 438.2 MiB/703.1 MiB (200.5 MiB/s) with 1 file(s) remaining \rCompleted 438.5 MiB/703.1 MiB (200.5 MiB/s) with 1 file(s) remaining \rCompleted 438.8 MiB/703.1 MiB (200.6 MiB/s) with 1 file(s) remaining \rCompleted 439.0 MiB/703.1 MiB (200.7 MiB/s) with 1 file(s) remaining \rCompleted 439.2 MiB/703.1 MiB (200.8 MiB/s) with 1 file(s) remaining \rCompleted 439.5 MiB/703.1 MiB (200.9 MiB/s) with 1 file(s) remaining \rCompleted 439.8 MiB/703.1 MiB (200.8 MiB/s) with 1 file(s) remaining \rCompleted 440.0 MiB/703.1 MiB (200.8 MiB/s) with 1 file(s) remaining \rCompleted 440.2 MiB/703.1 MiB (200.9 MiB/s) with 1 file(s) remaining \rCompleted 440.5 MiB/703.1 MiB (201.0 MiB/s) with 1 file(s) remaining \rCompleted 440.8 MiB/703.1 MiB (201.1 MiB/s) with 1 file(s) remaining \rCompleted 441.0 MiB/703.1 MiB (201.2 MiB/s) with 1 file(s) remaining \rCompleted 441.2 MiB/703.1 MiB (201.2 MiB/s) with 1 file(s) remaining \rCompleted 441.5 MiB/703.1 MiB (201.2 MiB/s) with 1 file(s) remaining \rCompleted 441.8 MiB/703.1 MiB (201.2 MiB/s) with 1 file(s) remaining \rCompleted 442.0 MiB/703.1 MiB (201.3 MiB/s) with 1 file(s) remaining \rCompleted 442.2 MiB/703.1 MiB (201.3 MiB/s) with 1 file(s) remaining \rCompleted 442.5 MiB/703.1 MiB (201.4 MiB/s) with 1 file(s) remaining \rCompleted 442.8 MiB/703.1 MiB (201.4 MiB/s) with 1 file(s) remaining \rCompleted 443.0 MiB/703.1 MiB (201.4 MiB/s) with 1 file(s) remaining \rCompleted 443.2 MiB/703.1 MiB (201.5 MiB/s) with 1 file(s) remaining \rCompleted 443.5 MiB/703.1 MiB (201.4 MiB/s) with 1 file(s) remaining \rCompleted 443.8 MiB/703.1 MiB (201.5 MiB/s) with 1 file(s) remaining \rCompleted 444.0 MiB/703.1 MiB (201.6 MiB/s) with 1 file(s) remaining \rCompleted 444.2 MiB/703.1 MiB (201.6 MiB/s) with 1 file(s) remaining \rCompleted 444.5 MiB/703.1 MiB (201.7 MiB/s) with 1 file(s) remaining \rCompleted 444.8 MiB/703.1 MiB (201.7 MiB/s) with 1 file(s) remaining \rCompleted 445.0 MiB/703.1 MiB (201.8 MiB/s) with 1 file(s) remaining \rCompleted 445.2 MiB/703.1 MiB (201.8 MiB/s) with 1 file(s) remaining \rCompleted 445.5 MiB/703.1 MiB (201.9 MiB/s) with 1 file(s) remaining \rCompleted 445.8 MiB/703.1 MiB (201.9 MiB/s) with 1 file(s) remaining \rCompleted 446.0 MiB/703.1 MiB (202.0 MiB/s) with 1 file(s) remaining \rCompleted 446.2 MiB/703.1 MiB (202.0 MiB/s) with 1 file(s) remaining \rCompleted 446.5 MiB/703.1 MiB (202.1 MiB/s) with 1 file(s) remaining \rCompleted 446.8 MiB/703.1 MiB (202.2 MiB/s) with 1 file(s) remaining \rCompleted 447.0 MiB/703.1 MiB (202.1 MiB/s) with 1 file(s) remaining \rCompleted 447.2 MiB/703.1 MiB (202.2 MiB/s) with 1 file(s) remaining \rCompleted 447.5 MiB/703.1 MiB (202.3 MiB/s) with 1 file(s) remaining \rCompleted 447.8 MiB/703.1 MiB (202.3 MiB/s) with 1 file(s) remaining \rCompleted 448.0 MiB/703.1 MiB (202.3 MiB/s) with 1 file(s) remaining \rCompleted 448.2 MiB/703.1 MiB (202.3 MiB/s) with 1 file(s) remaining \rCompleted 448.5 MiB/703.1 MiB (202.4 MiB/s) with 1 file(s) remaining \rCompleted 448.8 MiB/703.1 MiB (202.5 MiB/s) with 1 file(s) remaining \rCompleted 449.0 MiB/703.1 MiB (202.6 MiB/s) with 1 file(s) remaining \rCompleted 449.2 MiB/703.1 MiB (202.5 MiB/s) with 1 file(s) remaining \rCompleted 449.5 MiB/703.1 MiB (202.6 MiB/s) with 1 file(s) remaining \rCompleted 449.8 MiB/703.1 MiB (202.6 MiB/s) with 1 file(s) remaining \rCompleted 450.0 MiB/703.1 MiB (202.7 MiB/s) with 1 file(s) remaining \rCompleted 450.2 MiB/703.1 MiB (202.7 MiB/s) with 1 file(s) remaining \rCompleted 450.5 MiB/703.1 MiB (202.8 MiB/s) with 1 file(s) remaining \rCompleted 450.8 MiB/703.1 MiB (202.8 MiB/s) with 1 file(s) remaining \rCompleted 451.0 MiB/703.1 MiB (202.9 MiB/s) with 1 file(s) remaining \rCompleted 451.2 MiB/703.1 MiB (202.9 MiB/s) with 1 file(s) remaining \rCompleted 451.5 MiB/703.1 MiB (203.0 MiB/s) with 1 file(s) remaining \rCompleted 451.8 MiB/703.1 MiB (203.1 MiB/s) with 1 file(s) remaining \rCompleted 452.0 MiB/703.1 MiB (203.0 MiB/s) with 1 file(s) remaining \rCompleted 452.2 MiB/703.1 MiB (203.1 MiB/s) with 1 file(s) remaining \rCompleted 452.5 MiB/703.1 MiB (203.2 MiB/s) with 1 file(s) remaining \rCompleted 452.8 MiB/703.1 MiB (203.2 MiB/s) with 1 file(s) remaining \rCompleted 453.0 MiB/703.1 MiB (203.3 MiB/s) with 1 file(s) remaining \rCompleted 453.2 MiB/703.1 MiB (203.3 MiB/s) with 1 file(s) remaining \rCompleted 453.5 MiB/703.1 MiB (203.4 MiB/s) with 1 file(s) remaining \rCompleted 453.8 MiB/703.1 MiB (203.4 MiB/s) with 1 file(s) remaining \rCompleted 454.0 MiB/703.1 MiB (203.4 MiB/s) with 1 file(s) remaining \rCompleted 454.2 MiB/703.1 MiB (203.5 MiB/s) with 1 file(s) remaining \rCompleted 454.5 MiB/703.1 MiB (203.6 MiB/s) with 1 file(s) remaining \rCompleted 454.8 MiB/703.1 MiB (203.7 MiB/s) with 1 file(s) remaining \rCompleted 455.0 MiB/703.1 MiB (203.7 MiB/s) with 1 file(s) remaining \rCompleted 455.2 MiB/703.1 MiB (203.7 MiB/s) with 1 file(s) remaining \rCompleted 455.5 MiB/703.1 MiB (203.8 MiB/s) with 1 file(s) remaining \rCompleted 455.8 MiB/703.1 MiB (203.8 MiB/s) with 1 file(s) remaining \rCompleted 456.0 MiB/703.1 MiB (203.8 MiB/s) with 1 file(s) remaining \rCompleted 456.2 MiB/703.1 MiB (203.9 MiB/s) with 1 file(s) remaining \rCompleted 456.5 MiB/703.1 MiB (203.9 MiB/s) with 1 file(s) remaining \rCompleted 456.8 MiB/703.1 MiB (203.9 MiB/s) with 1 file(s) remaining \rCompleted 457.0 MiB/703.1 MiB (204.0 MiB/s) with 1 file(s) remaining \rCompleted 457.2 MiB/703.1 MiB (204.1 MiB/s) with 1 file(s) remaining \rCompleted 457.5 MiB/703.1 MiB (204.2 MiB/s) with 1 file(s) remaining \rCompleted 457.8 MiB/703.1 MiB (204.2 MiB/s) with 1 file(s) remaining \rCompleted 458.0 MiB/703.1 MiB (204.3 MiB/s) with 1 file(s) remaining \rCompleted 458.2 MiB/703.1 MiB (204.3 MiB/s) with 1 file(s) remaining \rCompleted 458.5 MiB/703.1 MiB (204.4 MiB/s) with 1 file(s) remaining \rCompleted 458.8 MiB/703.1 MiB (204.5 MiB/s) with 1 file(s) remaining \rCompleted 459.0 MiB/703.1 MiB (204.5 MiB/s) with 1 file(s) remaining \rCompleted 459.2 MiB/703.1 MiB (204.6 MiB/s) with 1 file(s) remaining \rCompleted 459.5 MiB/703.1 MiB (204.6 MiB/s) with 1 file(s) remaining \rCompleted 459.8 MiB/703.1 MiB (204.7 MiB/s) with 1 file(s) remaining \rCompleted 460.0 MiB/703.1 MiB (204.7 MiB/s) with 1 file(s) remaining \rCompleted 460.2 MiB/703.1 MiB (204.8 MiB/s) with 1 file(s) remaining \rCompleted 460.5 MiB/703.1 MiB (204.8 MiB/s) with 1 file(s) remaining \rCompleted 460.8 MiB/703.1 MiB (204.9 MiB/s) with 1 file(s) remaining \rCompleted 461.0 MiB/703.1 MiB (204.9 MiB/s) with 1 file(s) remaining \rCompleted 461.2 MiB/703.1 MiB (205.0 MiB/s) with 1 file(s) remaining \rCompleted 461.5 MiB/703.1 MiB (205.1 MiB/s) with 1 file(s) remaining \rCompleted 461.8 MiB/703.1 MiB (205.1 MiB/s) with 1 file(s) remaining \rCompleted 462.0 MiB/703.1 MiB (205.2 MiB/s) with 1 file(s) remaining \rCompleted 462.2 MiB/703.1 MiB (205.2 MiB/s) with 1 file(s) remaining \rCompleted 462.5 MiB/703.1 MiB (205.3 MiB/s) with 1 file(s) remaining \rCompleted 462.8 MiB/703.1 MiB (205.3 MiB/s) with 1 file(s) remaining \rCompleted 463.0 MiB/703.1 MiB (205.4 MiB/s) with 1 file(s) remaining \rCompleted 463.2 MiB/703.1 MiB (205.5 MiB/s) with 1 file(s) remaining \rCompleted 463.5 MiB/703.1 MiB (205.6 MiB/s) with 1 file(s) remaining \rCompleted 463.8 MiB/703.1 MiB (205.7 MiB/s) with 1 file(s) remaining \rCompleted 464.0 MiB/703.1 MiB (205.6 MiB/s) with 1 file(s) remaining \rCompleted 464.2 MiB/703.1 MiB (205.6 MiB/s) with 1 file(s) remaining \rCompleted 464.5 MiB/703.1 MiB (205.7 MiB/s) with 1 file(s) remaining \rCompleted 464.8 MiB/703.1 MiB (205.7 MiB/s) with 1 file(s) remaining \rCompleted 465.0 MiB/703.1 MiB (205.8 MiB/s) with 1 file(s) remaining \rCompleted 465.2 MiB/703.1 MiB (205.8 MiB/s) with 1 file(s) remaining \rCompleted 465.5 MiB/703.1 MiB (205.7 MiB/s) with 1 file(s) remaining \rCompleted 465.8 MiB/703.1 MiB (205.8 MiB/s) with 1 file(s) remaining \rCompleted 466.0 MiB/703.1 MiB (205.9 MiB/s) with 1 file(s) remaining \rCompleted 466.2 MiB/703.1 MiB (205.9 MiB/s) with 1 file(s) remaining \rCompleted 466.5 MiB/703.1 MiB (206.0 MiB/s) with 1 file(s) remaining \rCompleted 466.8 MiB/703.1 MiB (206.1 MiB/s) with 1 file(s) remaining \rCompleted 467.0 MiB/703.1 MiB (206.2 MiB/s) with 1 file(s) remaining \rCompleted 467.2 MiB/703.1 MiB (206.1 MiB/s) with 1 file(s) remaining \rCompleted 467.5 MiB/703.1 MiB (206.2 MiB/s) with 1 file(s) remaining \rCompleted 467.8 MiB/703.1 MiB (206.3 MiB/s) with 1 file(s) remaining \rCompleted 468.0 MiB/703.1 MiB (206.3 MiB/s) with 1 file(s) remaining \rCompleted 468.2 MiB/703.1 MiB (206.3 MiB/s) with 1 file(s) remaining \rCompleted 468.5 MiB/703.1 MiB (206.3 MiB/s) with 1 file(s) remaining \rCompleted 468.8 MiB/703.1 MiB (206.4 MiB/s) with 1 file(s) remaining \rCompleted 469.0 MiB/703.1 MiB (206.5 MiB/s) with 1 file(s) remaining \rCompleted 469.2 MiB/703.1 MiB (206.5 MiB/s) with 1 file(s) remaining \rCompleted 469.5 MiB/703.1 MiB (206.6 MiB/s) with 1 file(s) remaining \rCompleted 469.8 MiB/703.1 MiB (206.7 MiB/s) with 1 file(s) remaining \rCompleted 470.0 MiB/703.1 MiB (206.7 MiB/s) with 1 file(s) remaining \rCompleted 470.2 MiB/703.1 MiB (206.8 MiB/s) with 1 file(s) remaining \rCompleted 470.5 MiB/703.1 MiB (206.9 MiB/s) with 1 file(s) remaining \rCompleted 470.8 MiB/703.1 MiB (206.9 MiB/s) with 1 file(s) remaining \rCompleted 471.0 MiB/703.1 MiB (206.9 MiB/s) with 1 file(s) remaining \rCompleted 471.2 MiB/703.1 MiB (207.0 MiB/s) with 1 file(s) remaining \rCompleted 471.5 MiB/703.1 MiB (207.0 MiB/s) with 1 file(s) remaining \rCompleted 471.8 MiB/703.1 MiB (207.1 MiB/s) with 1 file(s) remaining \rCompleted 472.0 MiB/703.1 MiB (207.2 MiB/s) with 1 file(s) remaining \rCompleted 472.2 MiB/703.1 MiB (207.3 MiB/s) with 1 file(s) remaining \rCompleted 472.5 MiB/703.1 MiB (207.3 MiB/s) with 1 file(s) remaining \rCompleted 472.8 MiB/703.1 MiB (207.4 MiB/s) with 1 file(s) remaining \rCompleted 473.0 MiB/703.1 MiB (207.5 MiB/s) with 1 file(s) remaining \rCompleted 473.2 MiB/703.1 MiB (207.5 MiB/s) with 1 file(s) remaining \rCompleted 473.5 MiB/703.1 MiB (207.6 MiB/s) with 1 file(s) remaining \rCompleted 473.8 MiB/703.1 MiB (207.7 MiB/s) with 1 file(s) remaining \rCompleted 474.0 MiB/703.1 MiB (207.8 MiB/s) with 1 file(s) remaining \rCompleted 474.2 MiB/703.1 MiB (207.7 MiB/s) with 1 file(s) remaining \rCompleted 474.5 MiB/703.1 MiB (207.8 MiB/s) with 1 file(s) remaining \rCompleted 474.8 MiB/703.1 MiB (207.9 MiB/s) with 1 file(s) remaining \rCompleted 475.0 MiB/703.1 MiB (208.0 MiB/s) with 1 file(s) remaining \rCompleted 475.2 MiB/703.1 MiB (208.0 MiB/s) with 1 file(s) remaining \rCompleted 475.5 MiB/703.1 MiB (208.1 MiB/s) with 1 file(s) remaining \rCompleted 475.8 MiB/703.1 MiB (208.2 MiB/s) with 1 file(s) remaining \rCompleted 476.0 MiB/703.1 MiB (208.2 MiB/s) with 1 file(s) remaining \rCompleted 476.2 MiB/703.1 MiB (208.3 MiB/s) with 1 file(s) remaining \rCompleted 476.5 MiB/703.1 MiB (208.4 MiB/s) with 1 file(s) remaining \rCompleted 476.8 MiB/703.1 MiB (208.3 MiB/s) with 1 file(s) remaining \rCompleted 477.0 MiB/703.1 MiB (208.4 MiB/s) with 1 file(s) remaining \rCompleted 477.2 MiB/703.1 MiB (208.4 MiB/s) with 1 file(s) remaining \rCompleted 477.5 MiB/703.1 MiB (208.4 MiB/s) with 1 file(s) remaining \rCompleted 477.8 MiB/703.1 MiB (208.5 MiB/s) with 1 file(s) remaining \rCompleted 478.0 MiB/703.1 MiB (208.6 MiB/s) with 1 file(s) remaining \rCompleted 478.2 MiB/703.1 MiB (208.6 MiB/s) with 1 file(s) remaining \rCompleted 478.5 MiB/703.1 MiB (208.7 MiB/s) with 1 file(s) remaining \rCompleted 478.8 MiB/703.1 MiB (208.8 MiB/s) with 1 file(s) remaining \rCompleted 479.0 MiB/703.1 MiB (208.7 MiB/s) with 1 file(s) remaining \rCompleted 479.2 MiB/703.1 MiB (208.8 MiB/s) with 1 file(s) remaining \rCompleted 479.5 MiB/703.1 MiB (208.9 MiB/s) with 1 file(s) remaining \rCompleted 479.8 MiB/703.1 MiB (209.0 MiB/s) with 1 file(s) remaining \rCompleted 480.0 MiB/703.1 MiB (209.1 MiB/s) with 1 file(s) remaining \rCompleted 480.2 MiB/703.1 MiB (209.0 MiB/s) with 1 file(s) remaining \rCompleted 480.5 MiB/703.1 MiB (209.1 MiB/s) with 1 file(s) remaining \rCompleted 480.8 MiB/703.1 MiB (209.1 MiB/s) with 1 file(s) remaining \rCompleted 481.0 MiB/703.1 MiB (209.2 MiB/s) with 1 file(s) remaining \rCompleted 481.2 MiB/703.1 MiB (209.3 MiB/s) with 1 file(s) remaining \rCompleted 481.5 MiB/703.1 MiB (209.4 MiB/s) with 1 file(s) remaining \rCompleted 481.8 MiB/703.1 MiB (209.2 MiB/s) with 1 file(s) remaining \rCompleted 482.0 MiB/703.1 MiB (209.2 MiB/s) with 1 file(s) remaining \rCompleted 482.2 MiB/703.1 MiB (209.3 MiB/s) with 1 file(s) remaining \rCompleted 482.5 MiB/703.1 MiB (209.4 MiB/s) with 1 file(s) remaining \rCompleted 482.8 MiB/703.1 MiB (209.4 MiB/s) with 1 file(s) remaining \rCompleted 483.0 MiB/703.1 MiB (209.5 MiB/s) with 1 file(s) remaining \rCompleted 483.2 MiB/703.1 MiB (209.5 MiB/s) with 1 file(s) remaining \rCompleted 483.5 MiB/703.1 MiB (209.6 MiB/s) with 1 file(s) remaining \rCompleted 483.8 MiB/703.1 MiB (209.6 MiB/s) with 1 file(s) remaining \rCompleted 484.0 MiB/703.1 MiB (209.7 MiB/s) with 1 file(s) remaining \rCompleted 484.2 MiB/703.1 MiB (209.7 MiB/s) with 1 file(s) remaining \rCompleted 484.5 MiB/703.1 MiB (209.8 MiB/s) with 1 file(s) remaining \rCompleted 484.8 MiB/703.1 MiB (209.8 MiB/s) with 1 file(s) remaining \rCompleted 485.0 MiB/703.1 MiB (209.9 MiB/s) with 1 file(s) remaining \rCompleted 485.2 MiB/703.1 MiB (209.9 MiB/s) with 1 file(s) remaining \rCompleted 485.5 MiB/703.1 MiB (209.9 MiB/s) with 1 file(s) remaining \rCompleted 485.8 MiB/703.1 MiB (210.0 MiB/s) with 1 file(s) remaining \rCompleted 486.0 MiB/703.1 MiB (210.1 MiB/s) with 1 file(s) remaining \rCompleted 486.2 MiB/703.1 MiB (210.0 MiB/s) with 1 file(s) remaining \rCompleted 486.5 MiB/703.1 MiB (210.1 MiB/s) with 1 file(s) remaining \rCompleted 486.8 MiB/703.1 MiB (210.2 MiB/s) with 1 file(s) remaining \rCompleted 487.0 MiB/703.1 MiB (210.2 MiB/s) with 1 file(s) remaining \rCompleted 487.2 MiB/703.1 MiB (210.3 MiB/s) with 1 file(s) remaining \rCompleted 487.5 MiB/703.1 MiB (210.4 MiB/s) with 1 file(s) remaining \rCompleted 487.8 MiB/703.1 MiB (210.4 MiB/s) with 1 file(s) remaining \rCompleted 488.0 MiB/703.1 MiB (210.4 MiB/s) with 1 file(s) remaining \rCompleted 488.2 MiB/703.1 MiB (210.4 MiB/s) with 1 file(s) remaining \rCompleted 488.5 MiB/703.1 MiB (210.4 MiB/s) with 1 file(s) remaining \rCompleted 488.8 MiB/703.1 MiB (210.5 MiB/s) with 1 file(s) remaining \rCompleted 489.0 MiB/703.1 MiB (210.6 MiB/s) with 1 file(s) remaining \rCompleted 489.2 MiB/703.1 MiB (210.6 MiB/s) with 1 file(s) remaining \rCompleted 489.5 MiB/703.1 MiB (210.5 MiB/s) with 1 file(s) remaining \rCompleted 489.8 MiB/703.1 MiB (210.6 MiB/s) with 1 file(s) remaining \rCompleted 490.0 MiB/703.1 MiB (210.6 MiB/s) with 1 file(s) remaining \rCompleted 490.2 MiB/703.1 MiB (210.7 MiB/s) with 1 file(s) remaining \rCompleted 490.5 MiB/703.1 MiB (210.7 MiB/s) with 1 file(s) remaining \rCompleted 490.8 MiB/703.1 MiB (210.8 MiB/s) with 1 file(s) remaining \rCompleted 491.0 MiB/703.1 MiB (210.8 MiB/s) with 1 file(s) remaining \rCompleted 491.2 MiB/703.1 MiB (210.9 MiB/s) with 1 file(s) remaining \rCompleted 491.5 MiB/703.1 MiB (211.0 MiB/s) with 1 file(s) remaining \rCompleted 491.8 MiB/703.1 MiB (211.0 MiB/s) with 1 file(s) remaining \rCompleted 492.0 MiB/703.1 MiB (211.0 MiB/s) with 1 file(s) remaining \rCompleted 492.2 MiB/703.1 MiB (211.1 MiB/s) with 1 file(s) remaining \rCompleted 492.5 MiB/703.1 MiB (211.2 MiB/s) with 1 file(s) remaining \rCompleted 492.8 MiB/703.1 MiB (211.1 MiB/s) with 1 file(s) remaining \rCompleted 493.0 MiB/703.1 MiB (211.2 MiB/s) with 1 file(s) remaining \rCompleted 493.2 MiB/703.1 MiB (211.3 MiB/s) with 1 file(s) remaining \rCompleted 493.5 MiB/703.1 MiB (211.4 MiB/s) with 1 file(s) remaining \rCompleted 493.8 MiB/703.1 MiB (211.3 MiB/s) with 1 file(s) remaining \rCompleted 494.0 MiB/703.1 MiB (211.4 MiB/s) with 1 file(s) remaining \rCompleted 494.2 MiB/703.1 MiB (211.5 MiB/s) with 1 file(s) remaining \rCompleted 494.5 MiB/703.1 MiB (211.6 MiB/s) with 1 file(s) remaining \rCompleted 494.8 MiB/703.1 MiB (211.7 MiB/s) with 1 file(s) remaining \rCompleted 495.0 MiB/703.1 MiB (211.7 MiB/s) with 1 file(s) remaining \rCompleted 495.2 MiB/703.1 MiB (211.7 MiB/s) with 1 file(s) remaining \rCompleted 495.5 MiB/703.1 MiB (211.8 MiB/s) with 1 file(s) remaining \rCompleted 495.8 MiB/703.1 MiB (211.8 MiB/s) with 1 file(s) remaining \rCompleted 496.0 MiB/703.1 MiB (211.9 MiB/s) with 1 file(s) remaining \rCompleted 496.2 MiB/703.1 MiB (211.9 MiB/s) with 1 file(s) remaining \rCompleted 496.5 MiB/703.1 MiB (212.0 MiB/s) with 1 file(s) remaining \rCompleted 496.8 MiB/703.1 MiB (212.1 MiB/s) with 1 file(s) remaining \rCompleted 497.0 MiB/703.1 MiB (212.1 MiB/s) with 1 file(s) remaining \rCompleted 497.2 MiB/703.1 MiB (212.2 MiB/s) with 1 file(s) remaining \rCompleted 497.5 MiB/703.1 MiB (212.3 MiB/s) with 1 file(s) remaining \rCompleted 497.8 MiB/703.1 MiB (212.3 MiB/s) with 1 file(s) remaining \rCompleted 498.0 MiB/703.1 MiB (212.4 MiB/s) with 1 file(s) remaining \rCompleted 498.2 MiB/703.1 MiB (212.5 MiB/s) with 1 file(s) remaining \rCompleted 498.5 MiB/703.1 MiB (212.4 MiB/s) with 1 file(s) remaining \rCompleted 498.8 MiB/703.1 MiB (212.5 MiB/s) with 1 file(s) remaining \rCompleted 499.0 MiB/703.1 MiB (212.6 MiB/s) with 1 file(s) remaining \rCompleted 499.2 MiB/703.1 MiB (212.5 MiB/s) with 1 file(s) remaining \rCompleted 499.5 MiB/703.1 MiB (212.6 MiB/s) with 1 file(s) remaining \rCompleted 499.8 MiB/703.1 MiB (212.6 MiB/s) with 1 file(s) remaining \rCompleted 500.0 MiB/703.1 MiB (212.7 MiB/s) with 1 file(s) remaining \rCompleted 500.2 MiB/703.1 MiB (212.8 MiB/s) with 1 file(s) remaining \rCompleted 500.5 MiB/703.1 MiB (212.8 MiB/s) with 1 file(s) remaining \rCompleted 500.8 MiB/703.1 MiB (212.8 MiB/s) with 1 file(s) remaining \rCompleted 501.0 MiB/703.1 MiB (212.7 MiB/s) with 1 file(s) remaining \rCompleted 501.2 MiB/703.1 MiB (212.8 MiB/s) with 1 file(s) remaining \rCompleted 501.5 MiB/703.1 MiB (212.9 MiB/s) with 1 file(s) remaining \rCompleted 501.8 MiB/703.1 MiB (212.9 MiB/s) with 1 file(s) remaining \rCompleted 502.0 MiB/703.1 MiB (212.9 MiB/s) with 1 file(s) remaining \rCompleted 502.2 MiB/703.1 MiB (213.0 MiB/s) with 1 file(s) remaining \rCompleted 502.5 MiB/703.1 MiB (213.1 MiB/s) with 1 file(s) remaining \rCompleted 502.8 MiB/703.1 MiB (213.1 MiB/s) with 1 file(s) remaining \rCompleted 503.0 MiB/703.1 MiB (213.1 MiB/s) with 1 file(s) remaining \rCompleted 503.2 MiB/703.1 MiB (213.1 MiB/s) with 1 file(s) remaining \rCompleted 503.5 MiB/703.1 MiB (213.2 MiB/s) with 1 file(s) remaining \rCompleted 503.8 MiB/703.1 MiB (213.2 MiB/s) with 1 file(s) remaining \rCompleted 504.0 MiB/703.1 MiB (213.2 MiB/s) with 1 file(s) remaining \rCompleted 504.2 MiB/703.1 MiB (213.3 MiB/s) with 1 file(s) remaining \rCompleted 504.5 MiB/703.1 MiB (213.4 MiB/s) with 1 file(s) remaining \rCompleted 504.8 MiB/703.1 MiB (213.4 MiB/s) with 1 file(s) remaining \rCompleted 505.0 MiB/703.1 MiB (213.4 MiB/s) with 1 file(s) remaining \rCompleted 505.2 MiB/703.1 MiB (213.4 MiB/s) with 1 file(s) remaining \rCompleted 505.5 MiB/703.1 MiB (213.4 MiB/s) with 1 file(s) remaining \rCompleted 505.8 MiB/703.1 MiB (213.5 MiB/s) with 1 file(s) remaining \rCompleted 506.0 MiB/703.1 MiB (213.6 MiB/s) with 1 file(s) remaining \rCompleted 506.2 MiB/703.1 MiB (213.5 MiB/s) with 1 file(s) remaining \rCompleted 506.5 MiB/703.1 MiB (213.6 MiB/s) with 1 file(s) remaining \rCompleted 506.8 MiB/703.1 MiB (213.6 MiB/s) with 1 file(s) remaining \rCompleted 507.0 MiB/703.1 MiB (213.7 MiB/s) with 1 file(s) remaining \rCompleted 507.2 MiB/703.1 MiB (213.7 MiB/s) with 1 file(s) remaining \rCompleted 507.5 MiB/703.1 MiB (213.7 MiB/s) with 1 file(s) remaining \rCompleted 507.8 MiB/703.1 MiB (213.7 MiB/s) with 1 file(s) remaining \rCompleted 508.0 MiB/703.1 MiB (213.7 MiB/s) with 1 file(s) remaining \rCompleted 508.2 MiB/703.1 MiB (213.8 MiB/s) with 1 file(s) remaining \rCompleted 508.5 MiB/703.1 MiB (213.9 MiB/s) with 1 file(s) remaining \rCompleted 508.8 MiB/703.1 MiB (214.0 MiB/s) with 1 file(s) remaining \rCompleted 509.0 MiB/703.1 MiB (214.1 MiB/s) with 1 file(s) remaining \rCompleted 509.2 MiB/703.1 MiB (214.0 MiB/s) with 1 file(s) remaining \rCompleted 509.5 MiB/703.1 MiB (214.1 MiB/s) with 1 file(s) remaining \rCompleted 509.8 MiB/703.1 MiB (214.1 MiB/s) with 1 file(s) remaining \rCompleted 510.0 MiB/703.1 MiB (214.2 MiB/s) with 1 file(s) remaining \rCompleted 510.2 MiB/703.1 MiB (214.1 MiB/s) with 1 file(s) remaining \rCompleted 510.5 MiB/703.1 MiB (214.2 MiB/s) with 1 file(s) remaining \rCompleted 510.8 MiB/703.1 MiB (214.3 MiB/s) with 1 file(s) remaining \rCompleted 511.0 MiB/703.1 MiB (214.3 MiB/s) with 1 file(s) remaining \rCompleted 511.2 MiB/703.1 MiB (214.4 MiB/s) with 1 file(s) remaining \rCompleted 511.5 MiB/703.1 MiB (214.3 MiB/s) with 1 file(s) remaining \rCompleted 511.8 MiB/703.1 MiB (214.3 MiB/s) with 1 file(s) remaining \rCompleted 512.0 MiB/703.1 MiB (214.3 MiB/s) with 1 file(s) remaining \rCompleted 512.2 MiB/703.1 MiB (214.4 MiB/s) with 1 file(s) remaining \rCompleted 512.5 MiB/703.1 MiB (214.4 MiB/s) with 1 file(s) remaining \rCompleted 512.8 MiB/703.1 MiB (214.5 MiB/s) with 1 file(s) remaining \rCompleted 513.0 MiB/703.1 MiB (214.5 MiB/s) with 1 file(s) remaining \rCompleted 513.2 MiB/703.1 MiB (214.6 MiB/s) with 1 file(s) remaining \rCompleted 513.5 MiB/703.1 MiB (214.7 MiB/s) with 1 file(s) remaining \rCompleted 513.8 MiB/703.1 MiB (214.7 MiB/s) with 1 file(s) remaining \rCompleted 514.0 MiB/703.1 MiB (214.7 MiB/s) with 1 file(s) remaining \rCompleted 514.2 MiB/703.1 MiB (214.8 MiB/s) with 1 file(s) remaining \rCompleted 514.5 MiB/703.1 MiB (214.8 MiB/s) with 1 file(s) remaining \rCompleted 514.8 MiB/703.1 MiB (214.8 MiB/s) with 1 file(s) remaining \rCompleted 515.0 MiB/703.1 MiB (214.8 MiB/s) with 1 file(s) remaining \rCompleted 515.2 MiB/703.1 MiB (214.9 MiB/s) with 1 file(s) remaining \rCompleted 515.5 MiB/703.1 MiB (215.0 MiB/s) with 1 file(s) remaining \rCompleted 515.8 MiB/703.1 MiB (215.1 MiB/s) with 1 file(s) remaining \rCompleted 516.0 MiB/703.1 MiB (215.1 MiB/s) with 1 file(s) remaining \rCompleted 516.2 MiB/703.1 MiB (215.1 MiB/s) with 1 file(s) remaining \rCompleted 516.5 MiB/703.1 MiB (215.1 MiB/s) with 1 file(s) remaining \rCompleted 516.8 MiB/703.1 MiB (215.2 MiB/s) with 1 file(s) remaining \rCompleted 517.0 MiB/703.1 MiB (215.2 MiB/s) with 1 file(s) remaining \rCompleted 517.2 MiB/703.1 MiB (215.3 MiB/s) with 1 file(s) remaining \rCompleted 517.5 MiB/703.1 MiB (215.1 MiB/s) with 1 file(s) remaining \rCompleted 517.8 MiB/703.1 MiB (215.2 MiB/s) with 1 file(s) remaining \rCompleted 518.0 MiB/703.1 MiB (215.1 MiB/s) with 1 file(s) remaining \rCompleted 518.2 MiB/703.1 MiB (215.2 MiB/s) with 1 file(s) remaining \rCompleted 518.5 MiB/703.1 MiB (215.3 MiB/s) with 1 file(s) remaining \rCompleted 518.8 MiB/703.1 MiB (215.3 MiB/s) with 1 file(s) remaining \rCompleted 519.0 MiB/703.1 MiB (215.4 MiB/s) with 1 file(s) remaining \rCompleted 519.2 MiB/703.1 MiB (215.4 MiB/s) with 1 file(s) remaining \rCompleted 519.5 MiB/703.1 MiB (215.4 MiB/s) with 1 file(s) remaining \rCompleted 519.8 MiB/703.1 MiB (215.5 MiB/s) with 1 file(s) remaining \rCompleted 520.0 MiB/703.1 MiB (215.6 MiB/s) with 1 file(s) remaining \rCompleted 520.2 MiB/703.1 MiB (215.6 MiB/s) with 1 file(s) remaining \rCompleted 520.5 MiB/703.1 MiB (215.7 MiB/s) with 1 file(s) remaining \rCompleted 520.8 MiB/703.1 MiB (215.6 MiB/s) with 1 file(s) remaining \rCompleted 521.0 MiB/703.1 MiB (215.6 MiB/s) with 1 file(s) remaining \rCompleted 521.2 MiB/703.1 MiB (215.7 MiB/s) with 1 file(s) remaining \rCompleted 521.5 MiB/703.1 MiB (215.7 MiB/s) with 1 file(s) remaining \rCompleted 521.8 MiB/703.1 MiB (215.8 MiB/s) with 1 file(s) remaining \rCompleted 522.0 MiB/703.1 MiB (215.8 MiB/s) with 1 file(s) remaining \rCompleted 522.2 MiB/703.1 MiB (215.9 MiB/s) with 1 file(s) remaining \rCompleted 522.5 MiB/703.1 MiB (215.8 MiB/s) with 1 file(s) remaining \rCompleted 522.8 MiB/703.1 MiB (215.9 MiB/s) with 1 file(s) remaining \rCompleted 523.0 MiB/703.1 MiB (215.9 MiB/s) with 1 file(s) remaining \rCompleted 523.2 MiB/703.1 MiB (216.0 MiB/s) with 1 file(s) remaining \rCompleted 523.5 MiB/703.1 MiB (216.0 MiB/s) with 1 file(s) remaining \rCompleted 523.8 MiB/703.1 MiB (216.0 MiB/s) with 1 file(s) remaining \rCompleted 524.0 MiB/703.1 MiB (216.1 MiB/s) with 1 file(s) remaining \rCompleted 524.2 MiB/703.1 MiB (216.1 MiB/s) with 1 file(s) remaining \rCompleted 524.5 MiB/703.1 MiB (216.2 MiB/s) with 1 file(s) remaining \rCompleted 524.8 MiB/703.1 MiB (216.3 MiB/s) with 1 file(s) remaining \rCompleted 525.0 MiB/703.1 MiB (216.3 MiB/s) with 1 file(s) remaining \rCompleted 525.2 MiB/703.1 MiB (216.3 MiB/s) with 1 file(s) remaining \rCompleted 525.5 MiB/703.1 MiB (216.3 MiB/s) with 1 file(s) remaining \rCompleted 525.8 MiB/703.1 MiB (216.4 MiB/s) with 1 file(s) remaining \rCompleted 526.0 MiB/703.1 MiB (216.5 MiB/s) with 1 file(s) remaining \rCompleted 526.2 MiB/703.1 MiB (216.4 MiB/s) with 1 file(s) remaining \rCompleted 526.5 MiB/703.1 MiB (216.5 MiB/s) with 1 file(s) remaining \rCompleted 526.8 MiB/703.1 MiB (216.6 MiB/s) with 1 file(s) remaining \rCompleted 527.0 MiB/703.1 MiB (216.6 MiB/s) with 1 file(s) remaining \rCompleted 527.2 MiB/703.1 MiB (216.7 MiB/s) with 1 file(s) remaining \rCompleted 527.5 MiB/703.1 MiB (216.8 MiB/s) with 1 file(s) remaining \rCompleted 527.8 MiB/703.1 MiB (216.8 MiB/s) with 1 file(s) remaining \rCompleted 528.0 MiB/703.1 MiB (216.6 MiB/s) with 1 file(s) remaining \rCompleted 528.2 MiB/703.1 MiB (216.7 MiB/s) with 1 file(s) remaining \rCompleted 528.5 MiB/703.1 MiB (216.8 MiB/s) with 1 file(s) remaining \rCompleted 528.8 MiB/703.1 MiB (216.8 MiB/s) with 1 file(s) remaining \rCompleted 529.0 MiB/703.1 MiB (216.9 MiB/s) with 1 file(s) remaining \rCompleted 529.2 MiB/703.1 MiB (216.9 MiB/s) with 1 file(s) remaining \rCompleted 529.5 MiB/703.1 MiB (216.9 MiB/s) with 1 file(s) remaining \rCompleted 529.8 MiB/703.1 MiB (217.0 MiB/s) with 1 file(s) remaining \rCompleted 530.0 MiB/703.1 MiB (217.1 MiB/s) with 1 file(s) remaining \rCompleted 530.2 MiB/703.1 MiB (217.1 MiB/s) with 1 file(s) remaining \rCompleted 530.5 MiB/703.1 MiB (217.1 MiB/s) with 1 file(s) remaining \rCompleted 530.8 MiB/703.1 MiB (217.1 MiB/s) with 1 file(s) remaining \rCompleted 531.0 MiB/703.1 MiB (217.2 MiB/s) with 1 file(s) remaining \rCompleted 531.2 MiB/703.1 MiB (217.3 MiB/s) with 1 file(s) remaining \rCompleted 531.5 MiB/703.1 MiB (217.3 MiB/s) with 1 file(s) remaining \rCompleted 531.8 MiB/703.1 MiB (217.3 MiB/s) with 1 file(s) remaining \rCompleted 532.0 MiB/703.1 MiB (217.4 MiB/s) with 1 file(s) remaining \rCompleted 532.2 MiB/703.1 MiB (217.4 MiB/s) with 1 file(s) remaining \rCompleted 532.5 MiB/703.1 MiB (217.5 MiB/s) with 1 file(s) remaining \rCompleted 532.8 MiB/703.1 MiB (217.5 MiB/s) with 1 file(s) remaining \rCompleted 533.0 MiB/703.1 MiB (217.5 MiB/s) with 1 file(s) remaining \rCompleted 533.2 MiB/703.1 MiB (217.6 MiB/s) with 1 file(s) remaining \rCompleted 533.5 MiB/703.1 MiB (217.6 MiB/s) with 1 file(s) remaining \rCompleted 533.8 MiB/703.1 MiB (217.7 MiB/s) with 1 file(s) remaining \rCompleted 534.0 MiB/703.1 MiB (217.7 MiB/s) with 1 file(s) remaining \rCompleted 534.2 MiB/703.1 MiB (217.8 MiB/s) with 1 file(s) remaining \rCompleted 534.5 MiB/703.1 MiB (217.8 MiB/s) with 1 file(s) remaining \rCompleted 534.8 MiB/703.1 MiB (217.8 MiB/s) with 1 file(s) remaining \rCompleted 535.0 MiB/703.1 MiB (217.9 MiB/s) with 1 file(s) remaining \rCompleted 535.2 MiB/703.1 MiB (218.0 MiB/s) with 1 file(s) remaining \rCompleted 535.5 MiB/703.1 MiB (218.1 MiB/s) with 1 file(s) remaining \rCompleted 535.8 MiB/703.1 MiB (218.1 MiB/s) with 1 file(s) remaining \rCompleted 536.0 MiB/703.1 MiB (218.1 MiB/s) with 1 file(s) remaining \rCompleted 536.2 MiB/703.1 MiB (218.1 MiB/s) with 1 file(s) remaining \rCompleted 536.5 MiB/703.1 MiB (218.2 MiB/s) with 1 file(s) remaining \rCompleted 536.8 MiB/703.1 MiB (218.2 MiB/s) with 1 file(s) remaining \rCompleted 537.0 MiB/703.1 MiB (218.3 MiB/s) with 1 file(s) remaining \rCompleted 537.2 MiB/703.1 MiB (218.3 MiB/s) with 1 file(s) remaining \rCompleted 537.5 MiB/703.1 MiB (218.3 MiB/s) with 1 file(s) remaining \rCompleted 537.8 MiB/703.1 MiB (218.4 MiB/s) with 1 file(s) remaining \rCompleted 538.0 MiB/703.1 MiB (218.4 MiB/s) with 1 file(s) remaining \rCompleted 538.2 MiB/703.1 MiB (218.5 MiB/s) with 1 file(s) remaining \rCompleted 538.5 MiB/703.1 MiB (218.5 MiB/s) with 1 file(s) remaining \rCompleted 538.8 MiB/703.1 MiB (218.6 MiB/s) with 1 file(s) remaining \rCompleted 539.0 MiB/703.1 MiB (218.6 MiB/s) with 1 file(s) remaining \rCompleted 539.2 MiB/703.1 MiB (218.7 MiB/s) with 1 file(s) remaining \rCompleted 539.5 MiB/703.1 MiB (218.8 MiB/s) with 1 file(s) remaining \rCompleted 539.8 MiB/703.1 MiB (218.7 MiB/s) with 1 file(s) remaining \rCompleted 540.0 MiB/703.1 MiB (218.8 MiB/s) with 1 file(s) remaining \rCompleted 540.2 MiB/703.1 MiB (218.8 MiB/s) with 1 file(s) remaining \rCompleted 540.5 MiB/703.1 MiB (218.9 MiB/s) with 1 file(s) remaining \rCompleted 540.8 MiB/703.1 MiB (219.0 MiB/s) with 1 file(s) remaining \rCompleted 541.0 MiB/703.1 MiB (218.9 MiB/s) with 1 file(s) remaining \rCompleted 541.2 MiB/703.1 MiB (219.0 MiB/s) with 1 file(s) remaining \rCompleted 541.5 MiB/703.1 MiB (219.1 MiB/s) with 1 file(s) remaining \rCompleted 541.8 MiB/703.1 MiB (219.1 MiB/s) with 1 file(s) remaining \rCompleted 542.0 MiB/703.1 MiB (219.2 MiB/s) with 1 file(s) remaining \rCompleted 542.2 MiB/703.1 MiB (219.2 MiB/s) with 1 file(s) remaining \rCompleted 542.5 MiB/703.1 MiB (219.1 MiB/s) with 1 file(s) remaining \rCompleted 542.8 MiB/703.1 MiB (219.1 MiB/s) with 1 file(s) remaining \rCompleted 543.0 MiB/703.1 MiB (219.1 MiB/s) with 1 file(s) remaining \rCompleted 543.2 MiB/703.1 MiB (219.1 MiB/s) with 1 file(s) remaining \rCompleted 543.5 MiB/703.1 MiB (219.1 MiB/s) with 1 file(s) remaining \rCompleted 543.8 MiB/703.1 MiB (219.2 MiB/s) with 1 file(s) remaining \rCompleted 544.0 MiB/703.1 MiB (219.3 MiB/s) with 1 file(s) remaining \rCompleted 544.2 MiB/703.1 MiB (219.3 MiB/s) with 1 file(s) remaining \rCompleted 544.5 MiB/703.1 MiB (219.3 MiB/s) with 1 file(s) remaining \rCompleted 544.8 MiB/703.1 MiB (219.4 MiB/s) with 1 file(s) remaining \rCompleted 545.0 MiB/703.1 MiB (219.4 MiB/s) with 1 file(s) remaining \rCompleted 545.2 MiB/703.1 MiB (219.4 MiB/s) with 1 file(s) remaining \rCompleted 545.5 MiB/703.1 MiB (219.5 MiB/s) with 1 file(s) remaining \rCompleted 545.8 MiB/703.1 MiB (219.5 MiB/s) with 1 file(s) remaining \rCompleted 546.0 MiB/703.1 MiB (219.6 MiB/s) with 1 file(s) remaining \rCompleted 546.2 MiB/703.1 MiB (219.7 MiB/s) with 1 file(s) remaining \rCompleted 546.5 MiB/703.1 MiB (219.6 MiB/s) with 1 file(s) remaining \rCompleted 546.8 MiB/703.1 MiB (219.6 MiB/s) with 1 file(s) remaining \rCompleted 547.0 MiB/703.1 MiB (219.7 MiB/s) with 1 file(s) remaining \rCompleted 547.2 MiB/703.1 MiB (219.8 MiB/s) with 1 file(s) remaining \rCompleted 547.5 MiB/703.1 MiB (219.8 MiB/s) with 1 file(s) remaining \rCompleted 547.8 MiB/703.1 MiB (219.9 MiB/s) with 1 file(s) remaining \rCompleted 548.0 MiB/703.1 MiB (219.8 MiB/s) with 1 file(s) remaining \rCompleted 548.2 MiB/703.1 MiB (219.8 MiB/s) with 1 file(s) remaining \rCompleted 548.5 MiB/703.1 MiB (219.9 MiB/s) with 1 file(s) remaining \rCompleted 548.8 MiB/703.1 MiB (219.9 MiB/s) with 1 file(s) remaining \rCompleted 549.0 MiB/703.1 MiB (219.9 MiB/s) with 1 file(s) remaining \rCompleted 549.2 MiB/703.1 MiB (219.9 MiB/s) with 1 file(s) remaining \rCompleted 549.5 MiB/703.1 MiB (220.0 MiB/s) with 1 file(s) remaining \rCompleted 549.8 MiB/703.1 MiB (220.1 MiB/s) with 1 file(s) remaining \rCompleted 550.0 MiB/703.1 MiB (220.1 MiB/s) with 1 file(s) remaining \rCompleted 550.2 MiB/703.1 MiB (220.2 MiB/s) with 1 file(s) remaining \rCompleted 550.5 MiB/703.1 MiB (220.2 MiB/s) with 1 file(s) remaining \rCompleted 550.8 MiB/703.1 MiB (220.1 MiB/s) with 1 file(s) remaining \rCompleted 551.0 MiB/703.1 MiB (220.2 MiB/s) with 1 file(s) remaining \rCompleted 551.2 MiB/703.1 MiB (220.1 MiB/s) with 1 file(s) remaining \rCompleted 551.5 MiB/703.1 MiB (220.1 MiB/s) with 1 file(s) remaining \rCompleted 551.8 MiB/703.1 MiB (220.1 MiB/s) with 1 file(s) remaining \rCompleted 552.0 MiB/703.1 MiB (220.2 MiB/s) with 1 file(s) remaining \rCompleted 552.2 MiB/703.1 MiB (220.3 MiB/s) with 1 file(s) remaining \rCompleted 552.5 MiB/703.1 MiB (220.3 MiB/s) with 1 file(s) remaining \rCompleted 552.8 MiB/703.1 MiB (220.3 MiB/s) with 1 file(s) remaining \rCompleted 553.0 MiB/703.1 MiB (220.4 MiB/s) with 1 file(s) remaining \rCompleted 553.2 MiB/703.1 MiB (220.3 MiB/s) with 1 file(s) remaining \rCompleted 553.5 MiB/703.1 MiB (220.3 MiB/s) with 1 file(s) remaining \rCompleted 553.8 MiB/703.1 MiB (220.3 MiB/s) with 1 file(s) remaining \rCompleted 554.0 MiB/703.1 MiB (220.3 MiB/s) with 1 file(s) remaining \rCompleted 554.2 MiB/703.1 MiB (220.4 MiB/s) with 1 file(s) remaining \rCompleted 554.5 MiB/703.1 MiB (220.5 MiB/s) with 1 file(s) remaining \rCompleted 554.8 MiB/703.1 MiB (220.4 MiB/s) with 1 file(s) remaining \rCompleted 555.0 MiB/703.1 MiB (220.5 MiB/s) with 1 file(s) remaining \rCompleted 555.2 MiB/703.1 MiB (220.5 MiB/s) with 1 file(s) remaining \rCompleted 555.5 MiB/703.1 MiB (220.5 MiB/s) with 1 file(s) remaining \rCompleted 555.8 MiB/703.1 MiB (220.5 MiB/s) with 1 file(s) remaining \rCompleted 556.0 MiB/703.1 MiB (220.6 MiB/s) with 1 file(s) remaining \rCompleted 556.2 MiB/703.1 MiB (220.7 MiB/s) with 1 file(s) remaining \rCompleted 556.5 MiB/703.1 MiB (220.7 MiB/s) with 1 file(s) remaining \rCompleted 556.8 MiB/703.1 MiB (220.7 MiB/s) with 1 file(s) remaining \rCompleted 557.0 MiB/703.1 MiB (220.8 MiB/s) with 1 file(s) remaining \rCompleted 557.2 MiB/703.1 MiB (220.8 MiB/s) with 1 file(s) remaining \rCompleted 557.5 MiB/703.1 MiB (220.9 MiB/s) with 1 file(s) remaining \rCompleted 557.8 MiB/703.1 MiB (221.0 MiB/s) with 1 file(s) remaining \rCompleted 558.0 MiB/703.1 MiB (221.0 MiB/s) with 1 file(s) remaining \rCompleted 558.2 MiB/703.1 MiB (221.0 MiB/s) with 1 file(s) remaining \rCompleted 558.5 MiB/703.1 MiB (221.1 MiB/s) with 1 file(s) remaining \rCompleted 558.8 MiB/703.1 MiB (221.1 MiB/s) with 1 file(s) remaining \rCompleted 559.0 MiB/703.1 MiB (221.1 MiB/s) with 1 file(s) remaining \rCompleted 559.2 MiB/703.1 MiB (221.2 MiB/s) with 1 file(s) remaining \rCompleted 559.5 MiB/703.1 MiB (221.3 MiB/s) with 1 file(s) remaining \rCompleted 559.8 MiB/703.1 MiB (221.3 MiB/s) with 1 file(s) remaining \rCompleted 560.0 MiB/703.1 MiB (221.3 MiB/s) with 1 file(s) remaining \rCompleted 560.2 MiB/703.1 MiB (221.4 MiB/s) with 1 file(s) remaining \rCompleted 560.5 MiB/703.1 MiB (221.5 MiB/s) with 1 file(s) remaining \rCompleted 560.8 MiB/703.1 MiB (221.5 MiB/s) with 1 file(s) remaining \rCompleted 561.0 MiB/703.1 MiB (221.5 MiB/s) with 1 file(s) remaining \rCompleted 561.2 MiB/703.1 MiB (221.6 MiB/s) with 1 file(s) remaining \rCompleted 561.5 MiB/703.1 MiB (221.6 MiB/s) with 1 file(s) remaining \rCompleted 561.8 MiB/703.1 MiB (221.7 MiB/s) with 1 file(s) remaining \rCompleted 562.0 MiB/703.1 MiB (221.8 MiB/s) with 1 file(s) remaining \rCompleted 562.2 MiB/703.1 MiB (221.9 MiB/s) with 1 file(s) remaining \rCompleted 562.5 MiB/703.1 MiB (221.8 MiB/s) with 1 file(s) remaining \rCompleted 562.8 MiB/703.1 MiB (221.9 MiB/s) with 1 file(s) remaining \rCompleted 563.0 MiB/703.1 MiB (222.0 MiB/s) with 1 file(s) remaining \rCompleted 563.2 MiB/703.1 MiB (221.9 MiB/s) with 1 file(s) remaining \rCompleted 563.5 MiB/703.1 MiB (222.0 MiB/s) with 1 file(s) remaining \rCompleted 563.8 MiB/703.1 MiB (222.0 MiB/s) with 1 file(s) remaining \rCompleted 564.0 MiB/703.1 MiB (222.1 MiB/s) with 1 file(s) remaining \rCompleted 564.2 MiB/703.1 MiB (222.1 MiB/s) with 1 file(s) remaining \rCompleted 564.5 MiB/703.1 MiB (222.2 MiB/s) with 1 file(s) remaining \rCompleted 564.8 MiB/703.1 MiB (222.2 MiB/s) with 1 file(s) remaining \rCompleted 565.0 MiB/703.1 MiB (222.3 MiB/s) with 1 file(s) remaining \rCompleted 565.2 MiB/703.1 MiB (222.4 MiB/s) with 1 file(s) remaining \rCompleted 565.5 MiB/703.1 MiB (222.4 MiB/s) with 1 file(s) remaining \rCompleted 565.8 MiB/703.1 MiB (222.5 MiB/s) with 1 file(s) remaining \rCompleted 566.0 MiB/703.1 MiB (222.6 MiB/s) with 1 file(s) remaining \rCompleted 566.2 MiB/703.1 MiB (222.5 MiB/s) with 1 file(s) remaining \rCompleted 566.5 MiB/703.1 MiB (222.6 MiB/s) with 1 file(s) remaining \rCompleted 566.8 MiB/703.1 MiB (222.7 MiB/s) with 1 file(s) remaining \rCompleted 567.0 MiB/703.1 MiB (222.7 MiB/s) with 1 file(s) remaining \rCompleted 567.2 MiB/703.1 MiB (222.7 MiB/s) with 1 file(s) remaining \rCompleted 567.5 MiB/703.1 MiB (222.8 MiB/s) with 1 file(s) remaining \rCompleted 567.8 MiB/703.1 MiB (222.9 MiB/s) with 1 file(s) remaining \rCompleted 568.0 MiB/703.1 MiB (223.0 MiB/s) with 1 file(s) remaining \rCompleted 568.2 MiB/703.1 MiB (223.0 MiB/s) with 1 file(s) remaining \rCompleted 568.5 MiB/703.1 MiB (223.1 MiB/s) with 1 file(s) remaining \rCompleted 568.8 MiB/703.1 MiB (223.1 MiB/s) with 1 file(s) remaining \rCompleted 569.0 MiB/703.1 MiB (223.2 MiB/s) with 1 file(s) remaining \rCompleted 569.2 MiB/703.1 MiB (223.2 MiB/s) with 1 file(s) remaining \rCompleted 569.5 MiB/703.1 MiB (223.3 MiB/s) with 1 file(s) remaining \rCompleted 569.8 MiB/703.1 MiB (223.4 MiB/s) with 1 file(s) remaining \rCompleted 570.0 MiB/703.1 MiB (223.5 MiB/s) with 1 file(s) remaining \rCompleted 570.2 MiB/703.1 MiB (223.5 MiB/s) with 1 file(s) remaining \rCompleted 570.5 MiB/703.1 MiB (223.6 MiB/s) with 1 file(s) remaining \rCompleted 570.8 MiB/703.1 MiB (223.6 MiB/s) with 1 file(s) remaining \rCompleted 571.0 MiB/703.1 MiB (223.5 MiB/s) with 1 file(s) remaining \rCompleted 571.2 MiB/703.1 MiB (223.6 MiB/s) with 1 file(s) remaining \rCompleted 571.5 MiB/703.1 MiB (223.6 MiB/s) with 1 file(s) remaining \rCompleted 571.8 MiB/703.1 MiB (223.7 MiB/s) with 1 file(s) remaining \rCompleted 572.0 MiB/703.1 MiB (223.8 MiB/s) with 1 file(s) remaining \rCompleted 572.2 MiB/703.1 MiB (223.8 MiB/s) with 1 file(s) remaining \rCompleted 572.5 MiB/703.1 MiB (223.9 MiB/s) with 1 file(s) remaining \rCompleted 572.8 MiB/703.1 MiB (223.9 MiB/s) with 1 file(s) remaining \rCompleted 573.0 MiB/703.1 MiB (224.0 MiB/s) with 1 file(s) remaining \rCompleted 573.2 MiB/703.1 MiB (224.1 MiB/s) with 1 file(s) remaining \rCompleted 573.5 MiB/703.1 MiB (224.1 MiB/s) with 1 file(s) remaining \rCompleted 573.8 MiB/703.1 MiB (224.2 MiB/s) with 1 file(s) remaining \rCompleted 574.0 MiB/703.1 MiB (224.2 MiB/s) with 1 file(s) remaining \rCompleted 574.2 MiB/703.1 MiB (224.3 MiB/s) with 1 file(s) remaining \rCompleted 574.5 MiB/703.1 MiB (224.3 MiB/s) with 1 file(s) remaining \rCompleted 574.8 MiB/703.1 MiB (224.4 MiB/s) with 1 file(s) remaining \rCompleted 575.0 MiB/703.1 MiB (224.4 MiB/s) with 1 file(s) remaining \rCompleted 575.2 MiB/703.1 MiB (224.5 MiB/s) with 1 file(s) remaining \rCompleted 575.5 MiB/703.1 MiB (224.6 MiB/s) with 1 file(s) remaining \rCompleted 575.8 MiB/703.1 MiB (224.6 MiB/s) with 1 file(s) remaining \rCompleted 576.0 MiB/703.1 MiB (224.7 MiB/s) with 1 file(s) remaining \rCompleted 576.2 MiB/703.1 MiB (224.7 MiB/s) with 1 file(s) remaining \rCompleted 576.5 MiB/703.1 MiB (224.8 MiB/s) with 1 file(s) remaining \rCompleted 576.8 MiB/703.1 MiB (224.8 MiB/s) with 1 file(s) remaining \rCompleted 577.0 MiB/703.1 MiB (224.9 MiB/s) with 1 file(s) remaining \rCompleted 577.2 MiB/703.1 MiB (224.9 MiB/s) with 1 file(s) remaining \rCompleted 577.5 MiB/703.1 MiB (225.0 MiB/s) with 1 file(s) remaining \rCompleted 577.8 MiB/703.1 MiB (225.0 MiB/s) with 1 file(s) remaining \rCompleted 578.0 MiB/703.1 MiB (225.0 MiB/s) with 1 file(s) remaining \rCompleted 578.2 MiB/703.1 MiB (225.0 MiB/s) with 1 file(s) remaining \rCompleted 578.5 MiB/703.1 MiB (224.7 MiB/s) with 1 file(s) remaining \rCompleted 578.8 MiB/703.1 MiB (224.7 MiB/s) with 1 file(s) remaining \rCompleted 579.0 MiB/703.1 MiB (224.8 MiB/s) with 1 file(s) remaining \rCompleted 579.2 MiB/703.1 MiB (224.8 MiB/s) with 1 file(s) remaining \rCompleted 579.5 MiB/703.1 MiB (224.8 MiB/s) with 1 file(s) remaining \rCompleted 579.8 MiB/703.1 MiB (224.9 MiB/s) with 1 file(s) remaining \rCompleted 580.0 MiB/703.1 MiB (224.7 MiB/s) with 1 file(s) remaining \rCompleted 580.2 MiB/703.1 MiB (224.7 MiB/s) with 1 file(s) remaining \rCompleted 580.5 MiB/703.1 MiB (224.7 MiB/s) with 1 file(s) remaining \rCompleted 580.8 MiB/703.1 MiB (224.7 MiB/s) with 1 file(s) remaining \rCompleted 581.0 MiB/703.1 MiB (224.8 MiB/s) with 1 file(s) remaining \rCompleted 581.2 MiB/703.1 MiB (224.8 MiB/s) with 1 file(s) remaining \rCompleted 581.5 MiB/703.1 MiB (224.9 MiB/s) with 1 file(s) remaining \rCompleted 581.8 MiB/703.1 MiB (224.9 MiB/s) with 1 file(s) remaining \rCompleted 582.0 MiB/703.1 MiB (224.9 MiB/s) with 1 file(s) remaining \rCompleted 582.2 MiB/703.1 MiB (225.0 MiB/s) with 1 file(s) remaining \rCompleted 582.5 MiB/703.1 MiB (225.0 MiB/s) with 1 file(s) remaining \rCompleted 582.8 MiB/703.1 MiB (225.0 MiB/s) with 1 file(s) remaining \rCompleted 583.0 MiB/703.1 MiB (224.6 MiB/s) with 1 file(s) remaining \rCompleted 583.2 MiB/703.1 MiB (224.6 MiB/s) with 1 file(s) remaining \rCompleted 583.5 MiB/703.1 MiB (224.6 MiB/s) with 1 file(s) remaining \rCompleted 583.8 MiB/703.1 MiB (224.7 MiB/s) with 1 file(s) remaining \rCompleted 584.0 MiB/703.1 MiB (224.7 MiB/s) with 1 file(s) remaining \rCompleted 584.2 MiB/703.1 MiB (224.7 MiB/s) with 1 file(s) remaining \rCompleted 584.5 MiB/703.1 MiB (224.8 MiB/s) with 1 file(s) remaining \rCompleted 584.8 MiB/703.1 MiB (224.9 MiB/s) with 1 file(s) remaining \rCompleted 585.0 MiB/703.1 MiB (224.8 MiB/s) with 1 file(s) remaining \rCompleted 585.2 MiB/703.1 MiB (224.8 MiB/s) with 1 file(s) remaining \rCompleted 585.5 MiB/703.1 MiB (224.9 MiB/s) with 1 file(s) remaining \rCompleted 585.8 MiB/703.1 MiB (224.9 MiB/s) with 1 file(s) remaining \rCompleted 586.0 MiB/703.1 MiB (225.0 MiB/s) with 1 file(s) remaining \rCompleted 586.2 MiB/703.1 MiB (225.0 MiB/s) with 1 file(s) remaining \rCompleted 586.5 MiB/703.1 MiB (225.1 MiB/s) with 1 file(s) remaining \rCompleted 586.8 MiB/703.1 MiB (225.2 MiB/s) with 1 file(s) remaining \rCompleted 587.0 MiB/703.1 MiB (225.2 MiB/s) with 1 file(s) remaining \rCompleted 587.2 MiB/703.1 MiB (225.3 MiB/s) with 1 file(s) remaining \rCompleted 587.5 MiB/703.1 MiB (225.3 MiB/s) with 1 file(s) remaining \rCompleted 587.8 MiB/703.1 MiB (225.3 MiB/s) with 1 file(s) remaining \rCompleted 588.0 MiB/703.1 MiB (225.3 MiB/s) with 1 file(s) remaining \rCompleted 588.2 MiB/703.1 MiB (225.4 MiB/s) with 1 file(s) remaining \rCompleted 588.5 MiB/703.1 MiB (225.1 MiB/s) with 1 file(s) remaining \rCompleted 588.8 MiB/703.1 MiB (225.1 MiB/s) with 1 file(s) remaining \rCompleted 589.0 MiB/703.1 MiB (225.1 MiB/s) with 1 file(s) remaining \rCompleted 589.2 MiB/703.1 MiB (225.2 MiB/s) with 1 file(s) remaining \rCompleted 589.5 MiB/703.1 MiB (225.1 MiB/s) with 1 file(s) remaining \rCompleted 589.8 MiB/703.1 MiB (225.1 MiB/s) with 1 file(s) remaining \rCompleted 590.0 MiB/703.1 MiB (225.2 MiB/s) with 1 file(s) remaining \rCompleted 590.2 MiB/703.1 MiB (225.2 MiB/s) with 1 file(s) remaining \rCompleted 590.5 MiB/703.1 MiB (225.3 MiB/s) with 1 file(s) remaining \rCompleted 590.8 MiB/703.1 MiB (225.3 MiB/s) with 1 file(s) remaining \rCompleted 591.0 MiB/703.1 MiB (225.3 MiB/s) with 1 file(s) remaining \rCompleted 591.2 MiB/703.1 MiB (225.4 MiB/s) with 1 file(s) remaining \rCompleted 591.5 MiB/703.1 MiB (225.4 MiB/s) with 1 file(s) remaining \rCompleted 591.8 MiB/703.1 MiB (225.4 MiB/s) with 1 file(s) remaining \rCompleted 592.0 MiB/703.1 MiB (225.4 MiB/s) with 1 file(s) remaining \rCompleted 592.2 MiB/703.1 MiB (225.5 MiB/s) with 1 file(s) remaining \rCompleted 592.5 MiB/703.1 MiB (225.5 MiB/s) with 1 file(s) remaining \rCompleted 592.8 MiB/703.1 MiB (225.6 MiB/s) with 1 file(s) remaining \rCompleted 593.0 MiB/703.1 MiB (225.6 MiB/s) with 1 file(s) remaining \rCompleted 593.2 MiB/703.1 MiB (225.7 MiB/s) with 1 file(s) remaining \rCompleted 593.5 MiB/703.1 MiB (225.7 MiB/s) with 1 file(s) remaining \rCompleted 593.8 MiB/703.1 MiB (225.8 MiB/s) with 1 file(s) remaining \rCompleted 594.0 MiB/703.1 MiB (225.8 MiB/s) with 1 file(s) remaining \rCompleted 594.2 MiB/703.1 MiB (225.9 MiB/s) with 1 file(s) remaining \rCompleted 594.5 MiB/703.1 MiB (226.0 MiB/s) with 1 file(s) remaining \rCompleted 594.8 MiB/703.1 MiB (226.0 MiB/s) with 1 file(s) remaining \rCompleted 595.0 MiB/703.1 MiB (226.0 MiB/s) with 1 file(s) remaining \rCompleted 595.2 MiB/703.1 MiB (226.1 MiB/s) with 1 file(s) remaining \rCompleted 595.5 MiB/703.1 MiB (226.1 MiB/s) with 1 file(s) remaining \rCompleted 595.8 MiB/703.1 MiB (226.2 MiB/s) with 1 file(s) remaining \rCompleted 596.0 MiB/703.1 MiB (226.2 MiB/s) with 1 file(s) remaining \rCompleted 596.2 MiB/703.1 MiB (226.2 MiB/s) with 1 file(s) remaining \rCompleted 596.5 MiB/703.1 MiB (226.2 MiB/s) with 1 file(s) remaining \rCompleted 596.8 MiB/703.1 MiB (226.3 MiB/s) with 1 file(s) remaining \rCompleted 597.0 MiB/703.1 MiB (226.3 MiB/s) with 1 file(s) remaining \rCompleted 597.2 MiB/703.1 MiB (226.3 MiB/s) with 1 file(s) remaining \rCompleted 597.5 MiB/703.1 MiB (226.4 MiB/s) with 1 file(s) remaining \rCompleted 597.8 MiB/703.1 MiB (226.4 MiB/s) with 1 file(s) remaining \rCompleted 598.0 MiB/703.1 MiB (226.5 MiB/s) with 1 file(s) remaining \rCompleted 598.2 MiB/703.1 MiB (226.5 MiB/s) with 1 file(s) remaining \rCompleted 598.5 MiB/703.1 MiB (226.6 MiB/s) with 1 file(s) remaining \rCompleted 598.8 MiB/703.1 MiB (226.6 MiB/s) with 1 file(s) remaining \rCompleted 599.0 MiB/703.1 MiB (226.6 MiB/s) with 1 file(s) remaining \rCompleted 599.2 MiB/703.1 MiB (226.6 MiB/s) with 1 file(s) remaining \rCompleted 599.5 MiB/703.1 MiB (226.7 MiB/s) with 1 file(s) remaining \rCompleted 599.8 MiB/703.1 MiB (226.7 MiB/s) with 1 file(s) remaining \rCompleted 600.0 MiB/703.1 MiB (226.7 MiB/s) with 1 file(s) remaining \rCompleted 600.2 MiB/703.1 MiB (226.7 MiB/s) with 1 file(s) remaining \rCompleted 600.5 MiB/703.1 MiB (226.7 MiB/s) with 1 file(s) remaining \rCompleted 600.8 MiB/703.1 MiB (226.8 MiB/s) with 1 file(s) remaining \rCompleted 601.0 MiB/703.1 MiB (226.8 MiB/s) with 1 file(s) remaining \rCompleted 601.2 MiB/703.1 MiB (226.9 MiB/s) with 1 file(s) remaining \rCompleted 601.5 MiB/703.1 MiB (226.8 MiB/s) with 1 file(s) remaining \rCompleted 601.8 MiB/703.1 MiB (226.9 MiB/s) with 1 file(s) remaining \rCompleted 602.0 MiB/703.1 MiB (227.0 MiB/s) with 1 file(s) remaining \rCompleted 602.2 MiB/703.1 MiB (226.9 MiB/s) with 1 file(s) remaining \rCompleted 602.5 MiB/703.1 MiB (227.0 MiB/s) with 1 file(s) remaining \rCompleted 602.8 MiB/703.1 MiB (227.1 MiB/s) with 1 file(s) remaining \rCompleted 603.0 MiB/703.1 MiB (227.2 MiB/s) with 1 file(s) remaining \rCompleted 603.2 MiB/703.1 MiB (227.2 MiB/s) with 1 file(s) remaining \rCompleted 603.5 MiB/703.1 MiB (227.2 MiB/s) with 1 file(s) remaining \rCompleted 603.8 MiB/703.1 MiB (227.3 MiB/s) with 1 file(s) remaining \rCompleted 604.0 MiB/703.1 MiB (227.1 MiB/s) with 1 file(s) remaining \rCompleted 604.2 MiB/703.1 MiB (227.2 MiB/s) with 1 file(s) remaining \rCompleted 604.5 MiB/703.1 MiB (227.3 MiB/s) with 1 file(s) remaining \rCompleted 604.8 MiB/703.1 MiB (227.2 MiB/s) with 1 file(s) remaining \rCompleted 605.0 MiB/703.1 MiB (227.3 MiB/s) with 1 file(s) remaining \rCompleted 605.2 MiB/703.1 MiB (227.4 MiB/s) with 1 file(s) remaining \rCompleted 605.5 MiB/703.1 MiB (227.5 MiB/s) with 1 file(s) remaining \rCompleted 605.8 MiB/703.1 MiB (227.5 MiB/s) with 1 file(s) remaining \rCompleted 606.0 MiB/703.1 MiB (227.5 MiB/s) with 1 file(s) remaining \rCompleted 606.2 MiB/703.1 MiB (227.6 MiB/s) with 1 file(s) remaining \rCompleted 606.5 MiB/703.1 MiB (227.6 MiB/s) with 1 file(s) remaining \rCompleted 606.8 MiB/703.1 MiB (227.7 MiB/s) with 1 file(s) remaining \rCompleted 607.0 MiB/703.1 MiB (227.8 MiB/s) with 1 file(s) remaining \rCompleted 607.2 MiB/703.1 MiB (227.7 MiB/s) with 1 file(s) remaining \rCompleted 607.5 MiB/703.1 MiB (227.8 MiB/s) with 1 file(s) remaining \rCompleted 607.8 MiB/703.1 MiB (227.8 MiB/s) with 1 file(s) remaining \rCompleted 608.0 MiB/703.1 MiB (227.9 MiB/s) with 1 file(s) remaining \rCompleted 608.2 MiB/703.1 MiB (227.9 MiB/s) with 1 file(s) remaining \rCompleted 608.5 MiB/703.1 MiB (227.9 MiB/s) with 1 file(s) remaining \rCompleted 608.8 MiB/703.1 MiB (227.9 MiB/s) with 1 file(s) remaining \rCompleted 609.0 MiB/703.1 MiB (228.0 MiB/s) with 1 file(s) remaining \rCompleted 609.2 MiB/703.1 MiB (228.1 MiB/s) with 1 file(s) remaining \rCompleted 609.5 MiB/703.1 MiB (228.0 MiB/s) with 1 file(s) remaining \rCompleted 609.8 MiB/703.1 MiB (228.1 MiB/s) with 1 file(s) remaining \rCompleted 610.0 MiB/703.1 MiB (228.1 MiB/s) with 1 file(s) remaining \rCompleted 610.2 MiB/703.1 MiB (228.1 MiB/s) with 1 file(s) remaining \rCompleted 610.5 MiB/703.1 MiB (228.2 MiB/s) with 1 file(s) remaining \rCompleted 610.8 MiB/703.1 MiB (228.3 MiB/s) with 1 file(s) remaining \rCompleted 611.0 MiB/703.1 MiB (228.3 MiB/s) with 1 file(s) remaining \rCompleted 611.2 MiB/703.1 MiB (228.3 MiB/s) with 1 file(s) remaining \rCompleted 611.5 MiB/703.1 MiB (228.3 MiB/s) with 1 file(s) remaining \rCompleted 611.8 MiB/703.1 MiB (228.4 MiB/s) with 1 file(s) remaining \rCompleted 612.0 MiB/703.1 MiB (228.5 MiB/s) with 1 file(s) remaining \rCompleted 612.2 MiB/703.1 MiB (228.5 MiB/s) with 1 file(s) remaining \rCompleted 612.5 MiB/703.1 MiB (228.5 MiB/s) with 1 file(s) remaining \rCompleted 612.8 MiB/703.1 MiB (228.6 MiB/s) with 1 file(s) remaining \rCompleted 613.0 MiB/703.1 MiB (228.7 MiB/s) with 1 file(s) remaining \rCompleted 613.2 MiB/703.1 MiB (228.7 MiB/s) with 1 file(s) remaining \rCompleted 613.5 MiB/703.1 MiB (228.8 MiB/s) with 1 file(s) remaining \rCompleted 613.8 MiB/703.1 MiB (228.8 MiB/s) with 1 file(s) remaining \rCompleted 614.0 MiB/703.1 MiB (228.8 MiB/s) with 1 file(s) remaining \rCompleted 614.2 MiB/703.1 MiB (228.9 MiB/s) with 1 file(s) remaining \rCompleted 614.5 MiB/703.1 MiB (228.9 MiB/s) with 1 file(s) remaining \rCompleted 614.8 MiB/703.1 MiB (229.0 MiB/s) with 1 file(s) remaining \rCompleted 615.0 MiB/703.1 MiB (228.9 MiB/s) with 1 file(s) remaining \rCompleted 615.2 MiB/703.1 MiB (228.9 MiB/s) with 1 file(s) remaining \rCompleted 615.5 MiB/703.1 MiB (229.0 MiB/s) with 1 file(s) remaining \rCompleted 615.8 MiB/703.1 MiB (229.1 MiB/s) with 1 file(s) remaining \rCompleted 616.0 MiB/703.1 MiB (229.0 MiB/s) with 1 file(s) remaining \rCompleted 616.2 MiB/703.1 MiB (229.1 MiB/s) with 1 file(s) remaining \rCompleted 616.5 MiB/703.1 MiB (229.1 MiB/s) with 1 file(s) remaining \rCompleted 616.8 MiB/703.1 MiB (229.0 MiB/s) with 1 file(s) remaining \rCompleted 617.0 MiB/703.1 MiB (229.1 MiB/s) with 1 file(s) remaining \rCompleted 617.2 MiB/703.1 MiB (229.1 MiB/s) with 1 file(s) remaining \rCompleted 617.5 MiB/703.1 MiB (229.2 MiB/s) with 1 file(s) remaining \rCompleted 617.8 MiB/703.1 MiB (229.3 MiB/s) with 1 file(s) remaining \rCompleted 618.0 MiB/703.1 MiB (229.3 MiB/s) with 1 file(s) remaining \rCompleted 618.2 MiB/703.1 MiB (229.3 MiB/s) with 1 file(s) remaining \rCompleted 618.5 MiB/703.1 MiB (229.3 MiB/s) with 1 file(s) remaining \rCompleted 618.8 MiB/703.1 MiB (229.4 MiB/s) with 1 file(s) remaining \rCompleted 619.0 MiB/703.1 MiB (229.5 MiB/s) with 1 file(s) remaining \rCompleted 619.2 MiB/703.1 MiB (229.4 MiB/s) with 1 file(s) remaining \rCompleted 619.5 MiB/703.1 MiB (229.4 MiB/s) with 1 file(s) remaining \rCompleted 619.8 MiB/703.1 MiB (229.5 MiB/s) with 1 file(s) remaining \rCompleted 620.0 MiB/703.1 MiB (229.5 MiB/s) with 1 file(s) remaining \rCompleted 620.2 MiB/703.1 MiB (229.6 MiB/s) with 1 file(s) remaining \rCompleted 620.5 MiB/703.1 MiB (229.5 MiB/s) with 1 file(s) remaining \rCompleted 620.8 MiB/703.1 MiB (229.6 MiB/s) with 1 file(s) remaining \rCompleted 621.0 MiB/703.1 MiB (229.6 MiB/s) with 1 file(s) remaining \rCompleted 621.2 MiB/703.1 MiB (229.6 MiB/s) with 1 file(s) remaining \rCompleted 621.5 MiB/703.1 MiB (229.7 MiB/s) with 1 file(s) remaining \rCompleted 621.8 MiB/703.1 MiB (229.8 MiB/s) with 1 file(s) remaining \rCompleted 622.0 MiB/703.1 MiB (229.7 MiB/s) with 1 file(s) remaining \rCompleted 622.2 MiB/703.1 MiB (229.8 MiB/s) with 1 file(s) remaining \rCompleted 622.5 MiB/703.1 MiB (229.8 MiB/s) with 1 file(s) remaining \rCompleted 622.8 MiB/703.1 MiB (229.9 MiB/s) with 1 file(s) remaining \rCompleted 623.0 MiB/703.1 MiB (229.9 MiB/s) with 1 file(s) remaining \rCompleted 623.2 MiB/703.1 MiB (229.9 MiB/s) with 1 file(s) remaining \rCompleted 623.5 MiB/703.1 MiB (230.0 MiB/s) with 1 file(s) remaining \rCompleted 623.8 MiB/703.1 MiB (230.1 MiB/s) with 1 file(s) remaining \rCompleted 624.0 MiB/703.1 MiB (230.0 MiB/s) with 1 file(s) remaining \rCompleted 624.2 MiB/703.1 MiB (230.1 MiB/s) with 1 file(s) remaining \rCompleted 624.5 MiB/703.1 MiB (230.2 MiB/s) with 1 file(s) remaining \rCompleted 624.8 MiB/703.1 MiB (230.2 MiB/s) with 1 file(s) remaining \rCompleted 625.0 MiB/703.1 MiB (230.2 MiB/s) with 1 file(s) remaining \rCompleted 625.2 MiB/703.1 MiB (230.3 MiB/s) with 1 file(s) remaining \rCompleted 625.5 MiB/703.1 MiB (230.3 MiB/s) with 1 file(s) remaining \rCompleted 625.8 MiB/703.1 MiB (230.4 MiB/s) with 1 file(s) remaining \rCompleted 626.0 MiB/703.1 MiB (230.3 MiB/s) with 1 file(s) remaining \rCompleted 626.2 MiB/703.1 MiB (230.4 MiB/s) with 1 file(s) remaining \rCompleted 626.5 MiB/703.1 MiB (230.4 MiB/s) with 1 file(s) remaining \rCompleted 626.8 MiB/703.1 MiB (230.5 MiB/s) with 1 file(s) remaining \rCompleted 627.0 MiB/703.1 MiB (230.5 MiB/s) with 1 file(s) remaining \rCompleted 627.2 MiB/703.1 MiB (230.5 MiB/s) with 1 file(s) remaining \rCompleted 627.5 MiB/703.1 MiB (230.6 MiB/s) with 1 file(s) remaining \rCompleted 627.8 MiB/703.1 MiB (230.6 MiB/s) with 1 file(s) remaining \rCompleted 628.0 MiB/703.1 MiB (230.6 MiB/s) with 1 file(s) remaining \rCompleted 628.2 MiB/703.1 MiB (230.7 MiB/s) with 1 file(s) remaining \rCompleted 628.5 MiB/703.1 MiB (230.8 MiB/s) with 1 file(s) remaining \rCompleted 628.8 MiB/703.1 MiB (230.7 MiB/s) with 1 file(s) remaining \rCompleted 629.0 MiB/703.1 MiB (230.7 MiB/s) with 1 file(s) remaining \rCompleted 629.2 MiB/703.1 MiB (230.8 MiB/s) with 1 file(s) remaining \rCompleted 629.5 MiB/703.1 MiB (230.9 MiB/s) with 1 file(s) remaining \rCompleted 629.8 MiB/703.1 MiB (231.0 MiB/s) with 1 file(s) remaining \rCompleted 630.0 MiB/703.1 MiB (230.9 MiB/s) with 1 file(s) remaining \rCompleted 630.2 MiB/703.1 MiB (230.9 MiB/s) with 1 file(s) remaining \rCompleted 630.5 MiB/703.1 MiB (230.9 MiB/s) with 1 file(s) remaining \rCompleted 630.8 MiB/703.1 MiB (231.0 MiB/s) with 1 file(s) remaining \rCompleted 631.0 MiB/703.1 MiB (231.1 MiB/s) with 1 file(s) remaining \rCompleted 631.2 MiB/703.1 MiB (231.1 MiB/s) with 1 file(s) remaining \rCompleted 631.5 MiB/703.1 MiB (231.2 MiB/s) with 1 file(s) remaining \rCompleted 631.8 MiB/703.1 MiB (231.2 MiB/s) with 1 file(s) remaining \rCompleted 632.0 MiB/703.1 MiB (231.2 MiB/s) with 1 file(s) remaining \rCompleted 632.2 MiB/703.1 MiB (231.3 MiB/s) with 1 file(s) remaining \rCompleted 632.5 MiB/703.1 MiB (231.3 MiB/s) with 1 file(s) remaining \rCompleted 632.8 MiB/703.1 MiB (231.3 MiB/s) with 1 file(s) remaining \rCompleted 633.0 MiB/703.1 MiB (231.4 MiB/s) with 1 file(s) remaining \rCompleted 633.2 MiB/703.1 MiB (231.5 MiB/s) with 1 file(s) remaining \rCompleted 633.5 MiB/703.1 MiB (231.4 MiB/s) with 1 file(s) remaining \rCompleted 633.8 MiB/703.1 MiB (231.5 MiB/s) with 1 file(s) remaining \rCompleted 634.0 MiB/703.1 MiB (231.5 MiB/s) with 1 file(s) remaining \rCompleted 634.2 MiB/703.1 MiB (231.6 MiB/s) with 1 file(s) remaining \rCompleted 634.5 MiB/703.1 MiB (231.5 MiB/s) with 1 file(s) remaining \rCompleted 634.8 MiB/703.1 MiB (231.6 MiB/s) with 1 file(s) remaining \rCompleted 635.0 MiB/703.1 MiB (231.6 MiB/s) with 1 file(s) remaining \rCompleted 635.2 MiB/703.1 MiB (231.6 MiB/s) with 1 file(s) remaining \rCompleted 635.5 MiB/703.1 MiB (231.6 MiB/s) with 1 file(s) remaining \rCompleted 635.8 MiB/703.1 MiB (231.6 MiB/s) with 1 file(s) remaining \rCompleted 636.0 MiB/703.1 MiB (231.7 MiB/s) with 1 file(s) remaining \rCompleted 636.2 MiB/703.1 MiB (231.7 MiB/s) with 1 file(s) remaining \rCompleted 636.5 MiB/703.1 MiB (231.6 MiB/s) with 1 file(s) remaining \rCompleted 636.8 MiB/703.1 MiB (231.5 MiB/s) with 1 file(s) remaining \rCompleted 637.0 MiB/703.1 MiB (231.6 MiB/s) with 1 file(s) remaining \rCompleted 637.2 MiB/703.1 MiB (231.6 MiB/s) with 1 file(s) remaining \rCompleted 637.5 MiB/703.1 MiB (231.7 MiB/s) with 1 file(s) remaining \rCompleted 637.8 MiB/703.1 MiB (231.8 MiB/s) with 1 file(s) remaining \rCompleted 638.0 MiB/703.1 MiB (231.6 MiB/s) with 1 file(s) remaining \rCompleted 638.2 MiB/703.1 MiB (231.7 MiB/s) with 1 file(s) remaining \rCompleted 638.5 MiB/703.1 MiB (231.8 MiB/s) with 1 file(s) remaining \rCompleted 638.8 MiB/703.1 MiB (231.8 MiB/s) with 1 file(s) remaining \rCompleted 639.0 MiB/703.1 MiB (231.9 MiB/s) with 1 file(s) remaining \rCompleted 639.2 MiB/703.1 MiB (231.9 MiB/s) with 1 file(s) remaining \rCompleted 639.5 MiB/703.1 MiB (232.0 MiB/s) with 1 file(s) remaining \rCompleted 639.8 MiB/703.1 MiB (232.0 MiB/s) with 1 file(s) remaining \rCompleted 640.0 MiB/703.1 MiB (231.9 MiB/s) with 1 file(s) remaining \rCompleted 640.2 MiB/703.1 MiB (232.0 MiB/s) with 1 file(s) remaining \rCompleted 640.5 MiB/703.1 MiB (232.1 MiB/s) with 1 file(s) remaining \rCompleted 640.8 MiB/703.1 MiB (232.1 MiB/s) with 1 file(s) remaining \rCompleted 641.0 MiB/703.1 MiB (232.2 MiB/s) with 1 file(s) remaining \rCompleted 641.2 MiB/703.1 MiB (232.2 MiB/s) with 1 file(s) remaining \rCompleted 641.5 MiB/703.1 MiB (232.2 MiB/s) with 1 file(s) remaining \rCompleted 641.8 MiB/703.1 MiB (232.2 MiB/s) with 1 file(s) remaining \rCompleted 642.0 MiB/703.1 MiB (232.3 MiB/s) with 1 file(s) remaining \rCompleted 642.2 MiB/703.1 MiB (232.3 MiB/s) with 1 file(s) remaining \rCompleted 642.5 MiB/703.1 MiB (232.3 MiB/s) with 1 file(s) remaining \rCompleted 642.8 MiB/703.1 MiB (232.4 MiB/s) with 1 file(s) remaining \rCompleted 643.0 MiB/703.1 MiB (232.4 MiB/s) with 1 file(s) remaining \rCompleted 643.2 MiB/703.1 MiB (232.4 MiB/s) with 1 file(s) remaining \rCompleted 643.5 MiB/703.1 MiB (232.4 MiB/s) with 1 file(s) remaining \rCompleted 643.8 MiB/703.1 MiB (232.5 MiB/s) with 1 file(s) remaining \rCompleted 644.0 MiB/703.1 MiB (232.5 MiB/s) with 1 file(s) remaining \rCompleted 644.2 MiB/703.1 MiB (232.5 MiB/s) with 1 file(s) remaining \rCompleted 644.5 MiB/703.1 MiB (232.6 MiB/s) with 1 file(s) remaining \rCompleted 644.8 MiB/703.1 MiB (232.6 MiB/s) with 1 file(s) remaining \rCompleted 645.0 MiB/703.1 MiB (232.6 MiB/s) with 1 file(s) remaining \rCompleted 645.2 MiB/703.1 MiB (232.7 MiB/s) with 1 file(s) remaining \rCompleted 645.5 MiB/703.1 MiB (232.7 MiB/s) with 1 file(s) remaining \rCompleted 645.8 MiB/703.1 MiB (232.8 MiB/s) with 1 file(s) remaining \rCompleted 646.0 MiB/703.1 MiB (232.8 MiB/s) with 1 file(s) remaining \rCompleted 646.2 MiB/703.1 MiB (232.9 MiB/s) with 1 file(s) remaining \rCompleted 646.5 MiB/703.1 MiB (232.9 MiB/s) with 1 file(s) remaining \rCompleted 646.8 MiB/703.1 MiB (232.9 MiB/s) with 1 file(s) remaining \rCompleted 647.0 MiB/703.1 MiB (232.9 MiB/s) with 1 file(s) remaining \rCompleted 647.2 MiB/703.1 MiB (233.0 MiB/s) with 1 file(s) remaining \rCompleted 647.5 MiB/703.1 MiB (233.1 MiB/s) with 1 file(s) remaining \rCompleted 647.8 MiB/703.1 MiB (233.1 MiB/s) with 1 file(s) remaining \rCompleted 648.0 MiB/703.1 MiB (233.1 MiB/s) with 1 file(s) remaining \rCompleted 648.2 MiB/703.1 MiB (233.1 MiB/s) with 1 file(s) remaining \rCompleted 648.5 MiB/703.1 MiB (233.2 MiB/s) with 1 file(s) remaining \rCompleted 648.8 MiB/703.1 MiB (233.2 MiB/s) with 1 file(s) remaining \rCompleted 649.0 MiB/703.1 MiB (233.2 MiB/s) with 1 file(s) remaining \rCompleted 649.2 MiB/703.1 MiB (233.2 MiB/s) with 1 file(s) remaining \rCompleted 649.5 MiB/703.1 MiB (233.3 MiB/s) with 1 file(s) remaining \rCompleted 649.8 MiB/703.1 MiB (233.3 MiB/s) with 1 file(s) remaining \rCompleted 650.0 MiB/703.1 MiB (233.4 MiB/s) with 1 file(s) remaining \rCompleted 650.2 MiB/703.1 MiB (233.4 MiB/s) with 1 file(s) remaining \rCompleted 650.5 MiB/703.1 MiB (233.4 MiB/s) with 1 file(s) remaining \rCompleted 650.8 MiB/703.1 MiB (233.5 MiB/s) with 1 file(s) remaining \rCompleted 651.0 MiB/703.1 MiB (233.5 MiB/s) with 1 file(s) remaining \rCompleted 651.2 MiB/703.1 MiB (233.5 MiB/s) with 1 file(s) remaining \rCompleted 651.5 MiB/703.1 MiB (233.5 MiB/s) with 1 file(s) remaining \rCompleted 651.8 MiB/703.1 MiB (233.5 MiB/s) with 1 file(s) remaining \rCompleted 652.0 MiB/703.1 MiB (233.4 MiB/s) with 1 file(s) remaining \rCompleted 652.2 MiB/703.1 MiB (233.5 MiB/s) with 1 file(s) remaining \rCompleted 652.5 MiB/703.1 MiB (233.4 MiB/s) with 1 file(s) remaining \rCompleted 652.8 MiB/703.1 MiB (233.4 MiB/s) with 1 file(s) remaining \rCompleted 653.0 MiB/703.1 MiB (233.5 MiB/s) with 1 file(s) remaining \rCompleted 653.2 MiB/703.1 MiB (233.5 MiB/s) with 1 file(s) remaining \rCompleted 653.5 MiB/703.1 MiB (233.5 MiB/s) with 1 file(s) remaining \rCompleted 653.8 MiB/703.1 MiB (233.5 MiB/s) with 1 file(s) remaining \rCompleted 654.0 MiB/703.1 MiB (233.6 MiB/s) with 1 file(s) remaining \rCompleted 654.2 MiB/703.1 MiB (233.7 MiB/s) with 1 file(s) remaining \rCompleted 654.5 MiB/703.1 MiB (233.6 MiB/s) with 1 file(s) remaining \rCompleted 654.8 MiB/703.1 MiB (233.7 MiB/s) with 1 file(s) remaining \rCompleted 655.0 MiB/703.1 MiB (233.8 MiB/s) with 1 file(s) remaining \rCompleted 655.2 MiB/703.1 MiB (233.8 MiB/s) with 1 file(s) remaining \rCompleted 655.5 MiB/703.1 MiB (233.8 MiB/s) with 1 file(s) remaining \rCompleted 655.8 MiB/703.1 MiB (233.9 MiB/s) with 1 file(s) remaining \rCompleted 656.0 MiB/703.1 MiB (233.8 MiB/s) with 1 file(s) remaining \rCompleted 656.2 MiB/703.1 MiB (233.9 MiB/s) with 1 file(s) remaining \rCompleted 656.5 MiB/703.1 MiB (234.0 MiB/s) with 1 file(s) remaining \rCompleted 656.8 MiB/703.1 MiB (234.0 MiB/s) with 1 file(s) remaining \rCompleted 657.0 MiB/703.1 MiB (234.0 MiB/s) with 1 file(s) remaining \rCompleted 657.2 MiB/703.1 MiB (234.1 MiB/s) with 1 file(s) remaining \rCompleted 657.5 MiB/703.1 MiB (234.0 MiB/s) with 1 file(s) remaining \rCompleted 657.8 MiB/703.1 MiB (234.1 MiB/s) with 1 file(s) remaining \rCompleted 658.0 MiB/703.1 MiB (234.0 MiB/s) with 1 file(s) remaining \rCompleted 658.2 MiB/703.1 MiB (234.0 MiB/s) with 1 file(s) remaining \rCompleted 658.5 MiB/703.1 MiB (234.0 MiB/s) with 1 file(s) remaining \rCompleted 658.8 MiB/703.1 MiB (234.1 MiB/s) with 1 file(s) remaining \rCompleted 659.0 MiB/703.1 MiB (234.2 MiB/s) with 1 file(s) remaining \rCompleted 659.2 MiB/703.1 MiB (234.3 MiB/s) with 1 file(s) remaining \rCompleted 659.5 MiB/703.1 MiB (234.3 MiB/s) with 1 file(s) remaining \rCompleted 659.8 MiB/703.1 MiB (234.3 MiB/s) with 1 file(s) remaining \rCompleted 660.0 MiB/703.1 MiB (234.3 MiB/s) with 1 file(s) remaining \rCompleted 660.2 MiB/703.1 MiB (234.4 MiB/s) with 1 file(s) remaining \rCompleted 660.5 MiB/703.1 MiB (234.4 MiB/s) with 1 file(s) remaining \rCompleted 660.8 MiB/703.1 MiB (234.5 MiB/s) with 1 file(s) remaining \rCompleted 661.0 MiB/703.1 MiB (234.5 MiB/s) with 1 file(s) remaining \rCompleted 661.2 MiB/703.1 MiB (234.6 MiB/s) with 1 file(s) remaining \rCompleted 661.5 MiB/703.1 MiB (234.5 MiB/s) with 1 file(s) remaining \rCompleted 661.8 MiB/703.1 MiB (234.6 MiB/s) with 1 file(s) remaining \rCompleted 662.0 MiB/703.1 MiB (234.7 MiB/s) with 1 file(s) remaining \rCompleted 662.2 MiB/703.1 MiB (234.7 MiB/s) with 1 file(s) remaining \rCompleted 662.5 MiB/703.1 MiB (234.7 MiB/s) with 1 file(s) remaining \rCompleted 662.8 MiB/703.1 MiB (234.8 MiB/s) with 1 file(s) remaining \rCompleted 663.0 MiB/703.1 MiB (234.8 MiB/s) with 1 file(s) remaining \rCompleted 663.2 MiB/703.1 MiB (234.9 MiB/s) with 1 file(s) remaining \rCompleted 663.5 MiB/703.1 MiB (234.9 MiB/s) with 1 file(s) remaining \rCompleted 663.8 MiB/703.1 MiB (234.9 MiB/s) with 1 file(s) remaining \rCompleted 664.0 MiB/703.1 MiB (235.0 MiB/s) with 1 file(s) remaining \rCompleted 664.2 MiB/703.1 MiB (235.0 MiB/s) with 1 file(s) remaining \rCompleted 664.5 MiB/703.1 MiB (235.1 MiB/s) with 1 file(s) remaining \rCompleted 664.8 MiB/703.1 MiB (235.1 MiB/s) with 1 file(s) remaining \rCompleted 665.0 MiB/703.1 MiB (235.1 MiB/s) with 1 file(s) remaining \rCompleted 665.2 MiB/703.1 MiB (235.2 MiB/s) with 1 file(s) remaining \rCompleted 665.5 MiB/703.1 MiB (235.2 MiB/s) with 1 file(s) remaining \rCompleted 665.8 MiB/703.1 MiB (235.2 MiB/s) with 1 file(s) remaining \rCompleted 666.0 MiB/703.1 MiB (235.2 MiB/s) with 1 file(s) remaining \rCompleted 666.2 MiB/703.1 MiB (235.3 MiB/s) with 1 file(s) remaining \rCompleted 666.5 MiB/703.1 MiB (235.4 MiB/s) with 1 file(s) remaining \rCompleted 666.8 MiB/703.1 MiB (235.4 MiB/s) with 1 file(s) remaining \rCompleted 667.0 MiB/703.1 MiB (235.4 MiB/s) with 1 file(s) remaining \rCompleted 667.2 MiB/703.1 MiB (235.5 MiB/s) with 1 file(s) remaining \rCompleted 667.5 MiB/703.1 MiB (235.5 MiB/s) with 1 file(s) remaining \rCompleted 667.8 MiB/703.1 MiB (235.6 MiB/s) with 1 file(s) remaining \rCompleted 668.0 MiB/703.1 MiB (235.6 MiB/s) with 1 file(s) remaining \rCompleted 668.2 MiB/703.1 MiB (235.6 MiB/s) with 1 file(s) remaining \rCompleted 668.5 MiB/703.1 MiB (235.7 MiB/s) with 1 file(s) remaining \rCompleted 668.8 MiB/703.1 MiB (235.7 MiB/s) with 1 file(s) remaining \rCompleted 669.0 MiB/703.1 MiB (235.7 MiB/s) with 1 file(s) remaining \rCompleted 669.2 MiB/703.1 MiB (235.8 MiB/s) with 1 file(s) remaining \rCompleted 669.5 MiB/703.1 MiB (235.8 MiB/s) with 1 file(s) remaining \rCompleted 669.8 MiB/703.1 MiB (235.9 MiB/s) with 1 file(s) remaining \rCompleted 670.0 MiB/703.1 MiB (235.9 MiB/s) with 1 file(s) remaining \rCompleted 670.2 MiB/703.1 MiB (235.9 MiB/s) with 1 file(s) remaining \rCompleted 670.5 MiB/703.1 MiB (235.9 MiB/s) with 1 file(s) remaining \rCompleted 670.8 MiB/703.1 MiB (236.0 MiB/s) with 1 file(s) remaining \rCompleted 671.0 MiB/703.1 MiB (236.0 MiB/s) with 1 file(s) remaining \rCompleted 671.2 MiB/703.1 MiB (236.0 MiB/s) with 1 file(s) remaining \rCompleted 671.5 MiB/703.1 MiB (236.1 MiB/s) with 1 file(s) remaining \rCompleted 671.8 MiB/703.1 MiB (236.0 MiB/s) with 1 file(s) remaining \rCompleted 672.0 MiB/703.1 MiB (236.1 MiB/s) with 1 file(s) remaining \rCompleted 672.2 MiB/703.1 MiB (236.2 MiB/s) with 1 file(s) remaining \rCompleted 672.5 MiB/703.1 MiB (236.2 MiB/s) with 1 file(s) remaining \rCompleted 672.8 MiB/703.1 MiB (236.2 MiB/s) with 1 file(s) remaining \rCompleted 673.0 MiB/703.1 MiB (236.2 MiB/s) with 1 file(s) remaining \rCompleted 673.2 MiB/703.1 MiB (236.3 MiB/s) with 1 file(s) remaining \rCompleted 673.5 MiB/703.1 MiB (236.3 MiB/s) with 1 file(s) remaining \rCompleted 673.8 MiB/703.1 MiB (236.3 MiB/s) with 1 file(s) remaining \rCompleted 674.0 MiB/703.1 MiB (236.4 MiB/s) with 1 file(s) remaining \rCompleted 674.2 MiB/703.1 MiB (236.4 MiB/s) with 1 file(s) remaining \rCompleted 674.5 MiB/703.1 MiB (236.4 MiB/s) with 1 file(s) remaining \rCompleted 674.8 MiB/703.1 MiB (236.5 MiB/s) with 1 file(s) remaining \rCompleted 675.0 MiB/703.1 MiB (236.4 MiB/s) with 1 file(s) remaining \rCompleted 675.2 MiB/703.1 MiB (236.5 MiB/s) with 1 file(s) remaining \rCompleted 675.5 MiB/703.1 MiB (236.6 MiB/s) with 1 file(s) remaining \rCompleted 675.8 MiB/703.1 MiB (236.6 MiB/s) with 1 file(s) remaining \rCompleted 676.0 MiB/703.1 MiB (236.7 MiB/s) with 1 file(s) remaining \rCompleted 676.2 MiB/703.1 MiB (236.7 MiB/s) with 1 file(s) remaining \rCompleted 676.5 MiB/703.1 MiB (236.7 MiB/s) with 1 file(s) remaining \rCompleted 676.8 MiB/703.1 MiB (236.8 MiB/s) with 1 file(s) remaining \rCompleted 677.0 MiB/703.1 MiB (236.7 MiB/s) with 1 file(s) remaining \rCompleted 677.2 MiB/703.1 MiB (236.8 MiB/s) with 1 file(s) remaining \rCompleted 677.5 MiB/703.1 MiB (236.9 MiB/s) with 1 file(s) remaining \rCompleted 677.8 MiB/703.1 MiB (236.9 MiB/s) with 1 file(s) remaining \rCompleted 678.0 MiB/703.1 MiB (237.0 MiB/s) with 1 file(s) remaining \rCompleted 678.2 MiB/703.1 MiB (236.9 MiB/s) with 1 file(s) remaining \rCompleted 678.5 MiB/703.1 MiB (236.9 MiB/s) with 1 file(s) remaining \rCompleted 678.8 MiB/703.1 MiB (237.0 MiB/s) with 1 file(s) remaining \rCompleted 679.0 MiB/703.1 MiB (237.0 MiB/s) with 1 file(s) remaining \rCompleted 679.2 MiB/703.1 MiB (237.1 MiB/s) with 1 file(s) remaining \rCompleted 679.5 MiB/703.1 MiB (237.1 MiB/s) with 1 file(s) remaining \rCompleted 679.8 MiB/703.1 MiB (237.1 MiB/s) with 1 file(s) remaining \rCompleted 680.0 MiB/703.1 MiB (237.1 MiB/s) with 1 file(s) remaining \rCompleted 680.2 MiB/703.1 MiB (237.2 MiB/s) with 1 file(s) remaining \rCompleted 680.5 MiB/703.1 MiB (237.2 MiB/s) with 1 file(s) remaining \rCompleted 680.8 MiB/703.1 MiB (237.3 MiB/s) with 1 file(s) remaining \rCompleted 681.0 MiB/703.1 MiB (237.3 MiB/s) with 1 file(s) remaining \rCompleted 681.2 MiB/703.1 MiB (237.3 MiB/s) with 1 file(s) remaining \rCompleted 681.5 MiB/703.1 MiB (237.4 MiB/s) with 1 file(s) remaining \rCompleted 681.8 MiB/703.1 MiB (237.4 MiB/s) with 1 file(s) remaining \rCompleted 682.0 MiB/703.1 MiB (237.4 MiB/s) with 1 file(s) remaining \rCompleted 682.2 MiB/703.1 MiB (237.5 MiB/s) with 1 file(s) remaining \rCompleted 682.5 MiB/703.1 MiB (237.5 MiB/s) with 1 file(s) remaining \rCompleted 682.8 MiB/703.1 MiB (237.6 MiB/s) with 1 file(s) remaining \rCompleted 683.0 MiB/703.1 MiB (237.6 MiB/s) with 1 file(s) remaining \rCompleted 683.2 MiB/703.1 MiB (237.6 MiB/s) with 1 file(s) remaining \rCompleted 683.5 MiB/703.1 MiB (237.7 MiB/s) with 1 file(s) remaining \rCompleted 683.8 MiB/703.1 MiB (237.7 MiB/s) with 1 file(s) remaining \rCompleted 684.0 MiB/703.1 MiB (237.7 MiB/s) with 1 file(s) remaining \rCompleted 684.2 MiB/703.1 MiB (237.8 MiB/s) with 1 file(s) remaining \rCompleted 684.5 MiB/703.1 MiB (237.8 MiB/s) with 1 file(s) remaining \rCompleted 684.8 MiB/703.1 MiB (237.9 MiB/s) with 1 file(s) remaining \rCompleted 685.0 MiB/703.1 MiB (237.9 MiB/s) with 1 file(s) remaining \rCompleted 685.2 MiB/703.1 MiB (237.9 MiB/s) with 1 file(s) remaining \rCompleted 685.5 MiB/703.1 MiB (237.9 MiB/s) with 1 file(s) remaining \rCompleted 685.8 MiB/703.1 MiB (238.0 MiB/s) with 1 file(s) remaining \rCompleted 686.0 MiB/703.1 MiB (238.0 MiB/s) with 1 file(s) remaining \rCompleted 686.2 MiB/703.1 MiB (238.1 MiB/s) with 1 file(s) remaining \rCompleted 686.5 MiB/703.1 MiB (238.1 MiB/s) with 1 file(s) remaining \rCompleted 686.8 MiB/703.1 MiB (238.2 MiB/s) with 1 file(s) remaining \rCompleted 687.0 MiB/703.1 MiB (238.2 MiB/s) with 1 file(s) remaining \rCompleted 687.2 MiB/703.1 MiB (238.2 MiB/s) with 1 file(s) remaining \rCompleted 687.5 MiB/703.1 MiB (238.3 MiB/s) with 1 file(s) remaining \rCompleted 687.8 MiB/703.1 MiB (238.3 MiB/s) with 1 file(s) remaining \rCompleted 688.0 MiB/703.1 MiB (238.4 MiB/s) with 1 file(s) remaining \rCompleted 688.2 MiB/703.1 MiB (238.4 MiB/s) with 1 file(s) remaining \rCompleted 688.5 MiB/703.1 MiB (238.5 MiB/s) with 1 file(s) remaining \rCompleted 688.8 MiB/703.1 MiB (238.5 MiB/s) with 1 file(s) remaining \rCompleted 689.0 MiB/703.1 MiB (238.6 MiB/s) with 1 file(s) remaining \rCompleted 689.2 MiB/703.1 MiB (238.6 MiB/s) with 1 file(s) remaining \rCompleted 689.5 MiB/703.1 MiB (238.6 MiB/s) with 1 file(s) remaining \rCompleted 689.8 MiB/703.1 MiB (238.7 MiB/s) with 1 file(s) remaining \rCompleted 690.0 MiB/703.1 MiB (238.7 MiB/s) with 1 file(s) remaining \rCompleted 690.2 MiB/703.1 MiB (238.8 MiB/s) with 1 file(s) remaining \rCompleted 690.5 MiB/703.1 MiB (238.8 MiB/s) with 1 file(s) remaining \rCompleted 690.8 MiB/703.1 MiB (238.8 MiB/s) with 1 file(s) remaining \rCompleted 691.0 MiB/703.1 MiB (238.9 MiB/s) with 1 file(s) remaining \rCompleted 691.2 MiB/703.1 MiB (238.9 MiB/s) with 1 file(s) remaining \rCompleted 691.5 MiB/703.1 MiB (239.0 MiB/s) with 1 file(s) remaining \rCompleted 691.8 MiB/703.1 MiB (239.0 MiB/s) with 1 file(s) remaining \rCompleted 692.0 MiB/703.1 MiB (239.1 MiB/s) with 1 file(s) remaining \rCompleted 692.2 MiB/703.1 MiB (239.1 MiB/s) with 1 file(s) remaining \rCompleted 692.5 MiB/703.1 MiB (239.2 MiB/s) with 1 file(s) remaining \rCompleted 692.8 MiB/703.1 MiB (239.2 MiB/s) with 1 file(s) remaining \rCompleted 693.0 MiB/703.1 MiB (239.2 MiB/s) with 1 file(s) remaining \rCompleted 693.2 MiB/703.1 MiB (239.2 MiB/s) with 1 file(s) remaining \rCompleted 693.5 MiB/703.1 MiB (239.2 MiB/s) with 1 file(s) remaining \rCompleted 693.8 MiB/703.1 MiB (239.2 MiB/s) with 1 file(s) remaining \rCompleted 694.0 MiB/703.1 MiB (239.3 MiB/s) with 1 file(s) remaining \rCompleted 694.2 MiB/703.1 MiB (239.3 MiB/s) with 1 file(s) remaining \rCompleted 694.5 MiB/703.1 MiB (239.3 MiB/s) with 1 file(s) remaining \rCompleted 694.8 MiB/703.1 MiB (239.4 MiB/s) with 1 file(s) remaining \rCompleted 695.0 MiB/703.1 MiB (239.4 MiB/s) with 1 file(s) remaining \rCompleted 695.2 MiB/703.1 MiB (239.4 MiB/s) with 1 file(s) remaining \rCompleted 695.5 MiB/703.1 MiB (239.4 MiB/s) with 1 file(s) remaining \rCompleted 695.8 MiB/703.1 MiB (239.0 MiB/s) with 1 file(s) remaining \rCompleted 696.0 MiB/703.1 MiB (239.0 MiB/s) with 1 file(s) remaining \rCompleted 696.2 MiB/703.1 MiB (238.7 MiB/s) with 1 file(s) remaining \rCompleted 696.5 MiB/703.1 MiB (238.7 MiB/s) with 1 file(s) remaining \rCompleted 696.8 MiB/703.1 MiB (238.8 MiB/s) with 1 file(s) remaining \rCompleted 697.0 MiB/703.1 MiB (237.5 MiB/s) with 1 file(s) remaining \rCompleted 697.2 MiB/703.1 MiB (237.5 MiB/s) with 1 file(s) remaining \rCompleted 697.5 MiB/703.1 MiB (237.5 MiB/s) with 1 file(s) remaining \rCompleted 697.8 MiB/703.1 MiB (236.3 MiB/s) with 1 file(s) remaining \rCompleted 698.0 MiB/703.1 MiB (236.3 MiB/s) with 1 file(s) remaining \rCompleted 698.2 MiB/703.1 MiB (236.3 MiB/s) with 1 file(s) remaining \rCompleted 698.5 MiB/703.1 MiB (235.1 MiB/s) with 1 file(s) remaining \rCompleted 698.8 MiB/703.1 MiB (235.1 MiB/s) with 1 file(s) remaining \rCompleted 699.0 MiB/703.1 MiB (235.1 MiB/s) with 1 file(s) remaining \rCompleted 699.2 MiB/703.1 MiB (233.9 MiB/s) with 1 file(s) remaining \rCompleted 699.5 MiB/703.1 MiB (233.9 MiB/s) with 1 file(s) remaining \rCompleted 699.8 MiB/703.1 MiB (234.0 MiB/s) with 1 file(s) remaining \rCompleted 700.0 MiB/703.1 MiB (232.7 MiB/s) with 1 file(s) remaining \rCompleted 700.2 MiB/703.1 MiB (232.7 MiB/s) with 1 file(s) remaining \rCompleted 700.5 MiB/703.1 MiB (232.8 MiB/s) with 1 file(s) remaining \rCompleted 700.8 MiB/703.1 MiB (231.6 MiB/s) with 1 file(s) remaining \rCompleted 701.0 MiB/703.1 MiB (231.6 MiB/s) with 1 file(s) remaining \rCompleted 701.2 MiB/703.1 MiB (231.6 MiB/s) with 1 file(s) remaining \rCompleted 701.5 MiB/703.1 MiB (230.5 MiB/s) with 1 file(s) remaining \rCompleted 701.8 MiB/703.1 MiB (230.5 MiB/s) with 1 file(s) remaining \rCompleted 702.0 MiB/703.1 MiB (230.5 MiB/s) with 1 file(s) remaining \rCompleted 702.2 MiB/703.1 MiB (230.5 MiB/s) with 1 file(s) remaining \rCompleted 702.5 MiB/703.1 MiB (229.3 MiB/s) with 1 file(s) remaining \rCompleted 702.8 MiB/703.1 MiB (229.4 MiB/s) with 1 file(s) remaining \rCompleted 703.0 MiB/703.1 MiB (229.4 MiB/s) with 1 file(s) remaining \rCompleted 703.1 MiB/703.1 MiB (228.2 MiB/s) with 1 file(s) remaining \rdownload: s3://probeengbucket/experiments/alina_mvp_dataset/datasets/activations.pickle to output/datasets/activations.pickle\n",
            "Completed 3.7 KiB/1.6 MiB (9.8 KiB/s) with 2 file(s) remaining\rdownload: s3://probeengbucket/experiments/alina_mvp_train/probes/manifest.json to output/probes/alina_mvp_train/probes/manifest.json\n",
            "Completed 3.7 KiB/1.6 MiB (9.8 KiB/s) with 1 file(s) remaining\rCompleted 259.7 KiB/1.6 MiB (553.2 KiB/s) with 1 file(s) remaining\rCompleted 515.7 KiB/1.6 MiB (1.0 MiB/s) with 1 file(s) remaining  \rCompleted 771.7 KiB/1.6 MiB (1.5 MiB/s) with 1 file(s) remaining  \rCompleted 1.0 MiB/1.6 MiB (2.0 MiB/s) with 1 file(s) remaining    \rCompleted 1.3 MiB/1.6 MiB (2.4 MiB/s) with 1 file(s) remaining    \rCompleted 1.5 MiB/1.6 MiB (2.8 MiB/s) with 1 file(s) remaining    \rCompleted 1.6 MiB/1.6 MiB (3.0 MiB/s) with 1 file(s) remaining    \rdownload: s3://probeengbucket/experiments/alina_mvp_train/probes/probes.pickle to output/probes/alina_mvp_train/probes/probes.pickle\n",
            "total 1.7M\n",
            "drwxr-xr-x 4 root root 4.0K Sep 30 14:48 .\n",
            "drwxr-xr-x 3 root root 4.0K Sep 30 14:35 ..\n",
            "drwxr-xr-x 2 root root 4.0K Sep 30 14:35 alina_mvp_train\n",
            "-rw-r--r-- 1 root root 7.7K Sep 30 14:39 manifest.json\n",
            "drwxr-xr-x 2 root root 4.0K Sep 30 14:48 probes\n",
            "-rw-r--r-- 1 root root 1.7M Sep 30 14:35 probe_train-v1.pickle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, gc, torch\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# clean any old models\n",
        "for name in (\"model\",\"tok\"):\n",
        "    if name in globals(): del globals()[name]\n",
        "gc.collect()\n",
        "if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "MODEL_ID = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model.eval()\n",
        "# IMPORTANT: do NOT call .to(\"cuda\") after using device_map=\"auto\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500,
          "referenced_widgets": [
            "fa36f70b52674721abfb09b2b274a764",
            "23aedea6a2934ad58f7e1e2f73f719f3",
            "dd4e55bffb6b41d8b6f96bfb69a3a141",
            "725f0f4cf78e426b883285a4ab8d1048",
            "5431f80a201b49fbae52946cebd96b16",
            "de77580fa53c45be9be173c71f2aaa29",
            "e9e542d81a2947aca84e1d69b8b024a2",
            "83deffec2e084e1e9352772ecd582330",
            "2de435b0b1fb4b01b376cfc29873d95a",
            "85ee55a2b68149a1bbddd07598e5f36f",
            "7fb7f2ac566947f8bda721f61ec2b3e2"
          ]
        },
        "id": "-E5RwXNjIY05",
        "outputId": "a51a49f8-1dc2-4a7e-afb6-1357c0da582f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa36f70b52674721abfb09b2b274a764"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(32000, 4096)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
              "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc, torch, os\n",
        "if 'model' in globals(): del model\n",
        "if 'tok' in globals(): del tok\n",
        "gc.collect(); torch.cuda.empty_cache()\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n"
      ],
      "metadata": {
        "id": "C-kmlXOxJufg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "MODEL_ID = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=\"auto\",\n",
        "    low_cpu_mem_usage=True,\n",
        "    device_map=\"cuda\",\n",
        ").eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "2df8175d68594329a8c446cb571d0b48",
            "d77e051ddcec482db0d9d215ac26e5c1",
            "81fed2e2583b4875b46618461cc450ab",
            "eea40b92f92142f6a96a579d5615a8a2",
            "36702a36cb6e46efa1cd464e0df4b5dc",
            "46e8e16b18604fe5b3614d828199caa5",
            "afa26fe06f0b4bfc84370b06e2296f6d",
            "e8afce4d57a24b27b3e13af899c27869",
            "7e6a4239048c45288a0e676068c294af",
            "f215d98241aa4d71ae7c8ca2afc478d6",
            "8fd6d369483f489ca9da953834e5f16b"
          ]
        },
        "id": "dWWSnJQZJ6E5",
        "outputId": "f8881226-2feb-48b9-cc70-3aa01e466cd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2df8175d68594329a8c446cb571d0b48"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, numpy as np, pickle, re\n",
        "from pathlib import Path\n",
        "from math import exp\n",
        "\n",
        "MODEL_ID = \"meta-llama/Llama-2-7b-chat-hf\"   # same model you loaded in fp16\n",
        "PROBE_FILE = Path(\"output/probes/alina_mvp_train/probes/probes.pickle\")\n",
        "\n",
        "assert 'model' in globals() and 'tok' in globals(), \"Load model/tokenizer first.\"\n",
        "assert PROBE_FILE.exists(), f\"Missing {PROBE_FILE}\"\n",
        "\n",
        "# ---- load probes dict[str -> TrainedProbe]\n",
        "with open(PROBE_FILE, \"rb\") as f:\n",
        "    probes_dict = pickle.load(f)\n",
        "print(f\"Loaded {len(probes_dict)} trained probes\")\n",
        "\n",
        "# pick 10 layers (match what you trained: 1..19 skip 2)\n",
        "LAYERS = [1,3,5,7,9,11,13,15,17,19]\n",
        "\n",
        "# choose exactly one probe per layer\n",
        "layer_to_probe = {}\n",
        "for L in LAYERS:\n",
        "    # keys look like: \"Llama-2-7b-chat-hf-<dataset>-lr-h{L}-0\"\n",
        "    pat = re.compile(rf\"{re.escape(MODEL_ID.split('/')[-1])}-.+-lr-h{L}-0$\")\n",
        "    # try prefer got_cities\n",
        "    candidates = [k for k in probes_dict.keys() if pat.search(k)]\n",
        "    pref = [k for k in candidates if \"-got_cities-\" in k] or candidates\n",
        "    if pref:\n",
        "        layer_to_probe[L] = probes_dict[pref[0]]\n",
        "    else:\n",
        "        print(f\"Warning: no probe found for layer {L}\")\n",
        "\n",
        "print(\"Using layers:\", sorted(layer_to_probe.keys()))\n",
        "\n",
        "# small helpers\n",
        "def _sigmoid(x):\n",
        "    try:\n",
        "        return 1.0/(1.0+exp(-float(x)))\n",
        "    except OverflowError:\n",
        "        return 0.0 if x < 0 else 1.0\n",
        "\n",
        "def probe_to_estimator(tp):\n",
        "    \"\"\"\n",
        "    ProbeEng's TrainedProbe stores a serialized sklearn model.\n",
        "    Its instance method `deserialize_probe(probe_data)` needs the payload.\n",
        "    We'll try common field names and, if needed, scan the model_dump.\n",
        "    \"\"\"\n",
        "    # 1) common attribute names\n",
        "    for attr in (\"probe\", \"probe_data\", \"serialized_probe\", \"estimator_data\", \"data\"):\n",
        "        if hasattr(tp, attr):\n",
        "            payload = getattr(tp, attr)\n",
        "            if payload is not None:\n",
        "                try:\n",
        "                    return tp.deserialize_probe(payload)\n",
        "                except Exception as e:\n",
        "                    last_err = e  # try next candidate\n",
        "\n",
        "    # 2) search inside the pydantic payload\n",
        "    dump = {}\n",
        "    try:\n",
        "        dump = tp.model_dump()\n",
        "    except Exception:\n",
        "        try:\n",
        "            dump = tp.dict()\n",
        "        except Exception:\n",
        "            dump = {}\n",
        "\n",
        "    # look for a dict that smells like a serialized payload\n",
        "    for k, v in dump.items():\n",
        "        if isinstance(v, dict) and any(x in v for x in (\"data\", \"bytes\", \"blob\", \"pickle\", \"b64\", \"content\", \"payload\")):\n",
        "            try:\n",
        "                return tp.deserialize_probe(v)\n",
        "            except Exception as e:\n",
        "                last_err = e\n",
        "\n",
        "    # 3) if we still couldn't get it, show helpful debug info\n",
        "    raise RuntimeError(\n",
        "        \"Could not deserialize sklearn estimator from TrainedProbe.\\n\"\n",
        "        f\"Fields seen: {list(dump.keys())[:15]}\"\n",
        "    )\n",
        "\n",
        "\n",
        "def layer_last_token_vec(text: str, layer: int) -> np.ndarray:\n",
        "    toks = tok(text, return_tensors=\"pt\", add_special_tokens=True)\n",
        "    toks = {k: v.to(model.device) for k, v in toks.items()}\n",
        "    with torch.no_grad():\n",
        "        out = model(**toks, output_hidden_states=True)\n",
        "    # hidden_states[0]=embeddings, [1]=after layer 1, ... so h{L} = index L\n",
        "    hs = out.hidden_states[layer]           # [1, seq, hidden]\n",
        "    vec = hs[:, -1, :].detach().cpu().numpy()  # last token\n",
        "    return vec  # shape (1, hidden)\n",
        "\n",
        "def est_proba01(est, X_np):\n",
        "    \"\"\"\n",
        "    Return a probability in [0,1] from a variety of estimator APIs.\n",
        "    Handles sklearn (predict_proba / decision_function / predict) and\n",
        "    ProbeEng PredictResult-style returns.\n",
        "    \"\"\"\n",
        "    def _sigmoid(x):\n",
        "        return float(1.0 / (1.0 + np.exp(-float(x))))\n",
        "\n",
        "    # 1) Standard sklearn path\n",
        "    if hasattr(est, \"predict_proba\"):\n",
        "        proba = est.predict_proba(X_np)\n",
        "        proba = np.asarray(proba)\n",
        "        if proba.ndim == 2 and proba.shape[1] >= 2:\n",
        "            return float(proba[0, 1])\n",
        "        return float(proba.ravel()[0])\n",
        "\n",
        "    if hasattr(est, \"decision_function\"):\n",
        "        score = est.decision_function(X_np)\n",
        "        if isinstance(score, (list, tuple, np.ndarray)):\n",
        "            score = np.asarray(score).ravel()[0]\n",
        "        return _sigmoid(score)\n",
        "\n",
        "    # 2) Generic predict path (may be ndarray OR a PredictResult-like object)\n",
        "    y = est.predict(X_np)\n",
        "\n",
        "    # 2a) If it's array-like, grab first element\n",
        "    if isinstance(y, (list, tuple, np.ndarray)):\n",
        "        return float(np.asarray(y).ravel()[0])\n",
        "\n",
        "    # 2b) If it's a PredictResult-like object, try common fields\n",
        "    for attr in (\"proba\", \"probs\", \"probabilities\", \"y_proba\"):\n",
        "        if hasattr(y, attr):\n",
        "            arr = np.asarray(getattr(y, attr))\n",
        "            if arr.ndim == 2 and arr.shape[1] >= 2:\n",
        "                return float(arr[0, 1])\n",
        "            return float(np.clip(arr.ravel()[0], 0.0, 1.0))\n",
        "\n",
        "    for attr in (\"scores\", \"logits\"):\n",
        "        if hasattr(y, attr):\n",
        "            arr = np.asarray(getattr(y, attr))\n",
        "            return _sigmoid(arr.ravel()[0])\n",
        "\n",
        "    for attr in (\"pred\", \"prediction\", \"label\", \"y\"):\n",
        "        if hasattr(y, attr):\n",
        "            return float(getattr(y, attr))\n",
        "\n",
        "    # 2c) Scalar fallback\n",
        "    if np.isscalar(y):\n",
        "        return float(y)\n",
        "\n",
        "    raise RuntimeError(f\"Estimator predict returned unsupported object of type {type(y)}\")\n",
        "\n",
        "\n",
        "def get_credences(claim: str) -> dict:\n",
        "    res = {}\n",
        "    for L, tp in layer_to_probe.items():\n",
        "        est = probe_to_estimator(tp)\n",
        "        x = layer_last_token_vec(claim, L)\n",
        "        res[f\"L{L}\"] = est_proba01(est, x)\n",
        "    return res\n",
        "\n",
        "# TYPE YOUR CLAIM HERE\n",
        "claim = \"Chicago is in Illinois\"\n",
        "credences = get_credences(claim)\n",
        "for k in sorted(credences, key=lambda s: int(s[1:])):\n",
        "    print(f\"{k}: {credences[k]:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "VZ3DqR3UKS_i",
        "outputId": "3ef562c5-717b-42c4-fc3f-ae05c52f525e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Load model/tokenizer first.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2698754120.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mPROBE_FILE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"output/probes/alina_mvp_train/probes/probes.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;34m'model'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'tok'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Load model/tokenizer first.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mPROBE_FILE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Missing {PROBE_FILE}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Load model/tokenizer first."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# main table\n",
        "df = pd.read_csv(\"output/evaluations/alina_mvp_eval/results.csv\")\n",
        "print(df.shape)\n",
        "print(df.columns.tolist())\n",
        "df.head(10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "hTj1VR9MNkvT",
        "outputId": "956b4289-96a6-40f1-af9d-95634d4749f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(250, 14)\n",
            "['accuracy', 'n', 'llm_id', 'train_dataset', 'eval_dataset', 'probe_method', 'layer', 'token_idx', 'split', 'is_supervised', 'is_grouped', 'same_dataset', 'threshold', 'recovered_accuracy']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   accuracy    n              llm_id train_dataset            eval_dataset  \\\n",
              "0  0.500000  100  Llama-2-7b-chat-hf    got_cities              got_cities   \n",
              "1  0.523077   65  Llama-2-7b-chat-hf    got_cities         got_sp_en_trans   \n",
              "2  0.520000  100  Llama-2-7b-chat-hf    got_cities  got_cities_cities_conj   \n",
              "3  0.530000  100  Llama-2-7b-chat-hf    got_cities  got_cities_cities_disj   \n",
              "4  0.550000  100  Llama-2-7b-chat-hf    got_cities         got_larger_than   \n",
              "5  0.520000  100  Llama-2-7b-chat-hf    got_cities              got_cities   \n",
              "6  0.523077   65  Llama-2-7b-chat-hf    got_cities         got_sp_en_trans   \n",
              "7  0.530000  100  Llama-2-7b-chat-hf    got_cities  got_cities_cities_conj   \n",
              "8  0.580000  100  Llama-2-7b-chat-hf    got_cities  got_cities_cities_disj   \n",
              "9  0.530000  100  Llama-2-7b-chat-hf    got_cities         got_larger_than   \n",
              "\n",
              "  probe_method  layer  token_idx       split  is_supervised  is_grouped  \\\n",
              "0           lr      1          0  validation           True       False   \n",
              "1           lr      1          0  validation           True       False   \n",
              "2           lr      1          0  validation           True       False   \n",
              "3           lr      1          0  validation           True       False   \n",
              "4           lr      1          0  validation           True       False   \n",
              "5           lr      3          0  validation           True       False   \n",
              "6           lr      3          0  validation           True       False   \n",
              "7           lr      3          0  validation           True       False   \n",
              "8           lr      3          0  validation           True       False   \n",
              "9           lr      3          0  validation           True       False   \n",
              "\n",
              "   same_dataset  threshold  recovered_accuracy  \n",
              "0          True       1.00            0.500000  \n",
              "1         False       1.00            0.523077  \n",
              "2         False       0.97            0.536082  \n",
              "3         False       0.94            0.563830  \n",
              "4         False       1.00            0.550000  \n",
              "5          True       1.00            0.520000  \n",
              "6         False       1.00            0.523077  \n",
              "7         False       0.97            0.546392  \n",
              "8         False       0.94            0.617021  \n",
              "9         False       1.00            0.530000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a7e1e59f-e876-42d8-a7ea-90352b49807b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>n</th>\n",
              "      <th>llm_id</th>\n",
              "      <th>train_dataset</th>\n",
              "      <th>eval_dataset</th>\n",
              "      <th>probe_method</th>\n",
              "      <th>layer</th>\n",
              "      <th>token_idx</th>\n",
              "      <th>split</th>\n",
              "      <th>is_supervised</th>\n",
              "      <th>is_grouped</th>\n",
              "      <th>same_dataset</th>\n",
              "      <th>threshold</th>\n",
              "      <th>recovered_accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.500000</td>\n",
              "      <td>100</td>\n",
              "      <td>Llama-2-7b-chat-hf</td>\n",
              "      <td>got_cities</td>\n",
              "      <td>got_cities</td>\n",
              "      <td>lr</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>validation</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.523077</td>\n",
              "      <td>65</td>\n",
              "      <td>Llama-2-7b-chat-hf</td>\n",
              "      <td>got_cities</td>\n",
              "      <td>got_sp_en_trans</td>\n",
              "      <td>lr</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>validation</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.523077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.520000</td>\n",
              "      <td>100</td>\n",
              "      <td>Llama-2-7b-chat-hf</td>\n",
              "      <td>got_cities</td>\n",
              "      <td>got_cities_cities_conj</td>\n",
              "      <td>lr</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>validation</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0.97</td>\n",
              "      <td>0.536082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.530000</td>\n",
              "      <td>100</td>\n",
              "      <td>Llama-2-7b-chat-hf</td>\n",
              "      <td>got_cities</td>\n",
              "      <td>got_cities_cities_disj</td>\n",
              "      <td>lr</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>validation</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.563830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.550000</td>\n",
              "      <td>100</td>\n",
              "      <td>Llama-2-7b-chat-hf</td>\n",
              "      <td>got_cities</td>\n",
              "      <td>got_larger_than</td>\n",
              "      <td>lr</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>validation</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.550000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.520000</td>\n",
              "      <td>100</td>\n",
              "      <td>Llama-2-7b-chat-hf</td>\n",
              "      <td>got_cities</td>\n",
              "      <td>got_cities</td>\n",
              "      <td>lr</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>validation</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.520000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.523077</td>\n",
              "      <td>65</td>\n",
              "      <td>Llama-2-7b-chat-hf</td>\n",
              "      <td>got_cities</td>\n",
              "      <td>got_sp_en_trans</td>\n",
              "      <td>lr</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>validation</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.523077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.530000</td>\n",
              "      <td>100</td>\n",
              "      <td>Llama-2-7b-chat-hf</td>\n",
              "      <td>got_cities</td>\n",
              "      <td>got_cities_cities_conj</td>\n",
              "      <td>lr</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>validation</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0.97</td>\n",
              "      <td>0.546392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.580000</td>\n",
              "      <td>100</td>\n",
              "      <td>Llama-2-7b-chat-hf</td>\n",
              "      <td>got_cities</td>\n",
              "      <td>got_cities_cities_disj</td>\n",
              "      <td>lr</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>validation</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.617021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.530000</td>\n",
              "      <td>100</td>\n",
              "      <td>Llama-2-7b-chat-hf</td>\n",
              "      <td>got_cities</td>\n",
              "      <td>got_larger_than</td>\n",
              "      <td>lr</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>validation</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.530000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a7e1e59f-e876-42d8-a7ea-90352b49807b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a7e1e59f-e876-42d8-a7ea-90352b49807b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a7e1e59f-e876-42d8-a7ea-90352b49807b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-5f8f3b0f-79fc-4b2c-83ff-41a39397159c\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5f8f3b0f-79fc-4b2c-83ff-41a39397159c')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-5f8f3b0f-79fc-4b2c-83ff-41a39397159c button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 250,\n  \"fields\": [\n    {\n      \"column\": \"accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.16287838968034268,\n        \"min\": 0.5,\n        \"max\": 1.0,\n        \"num_unique_values\": 66,\n        \"samples\": [\n          0.8461538461538461,\n          0.83,\n          0.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14,\n        \"min\": 65,\n        \"max\": 100,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          65,\n          100\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"llm_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Llama-2-7b-chat-hf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_dataset\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"got_sp_en_trans\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eval_dataset\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"got_sp_en_trans\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"probe_method\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"lr\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"layer\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5,\n        \"min\": 1,\n        \"max\": 19,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          17\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"token_idx\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"split\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"validation\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"is_supervised\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"is_grouped\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"same_dataset\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"threshold\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02404814448168608,\n        \"min\": 0.94,\n        \"max\": 1.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recovered_accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1665799418555566,\n        \"min\": 0.5,\n        \"max\": 1.0,\n        \"num_unique_values\": 108,\n        \"samples\": [\n          0.7731958762886598\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"output/evaluations/alina_mvp_eval/results.csv\")\n",
        "\n",
        "# Overall mean accuracy per layer (averaged over all train/eval pairs)\n",
        "overall_by_layer = (df.groupby(\"layer\")[\"accuracy\"]\n",
        "                      .mean()\n",
        "                      .sort_values(ascending=False))\n",
        "print(\"Overall mean accuracy by layer:\\n\", overall_by_layer.head(10))\n",
        "\n",
        "# Cross-dataset generalization (exclude same_dataset)\n",
        "xd = df[df[\"same_dataset\"] == False]\n",
        "xgen_by_layer = (xd.groupby(\"layer\")[\"accuracy\"]\n",
        "                   .mean()\n",
        "                   .sort_values(ascending=False))\n",
        "print(\"\\nCross-dataset mean accuracy by layer:\\n\", xgen_by_layer.head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzeH7J_pO9q7",
        "outputId": "36095bf1-39f5-42a6-8db1-461948a58066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall mean accuracy by layer:\n",
            " layer\n",
            "17    0.802492\n",
            "19    0.793323\n",
            "15    0.783631\n",
            "13    0.776800\n",
            "11    0.766892\n",
            "9     0.658400\n",
            "7     0.572738\n",
            "5     0.557108\n",
            "3     0.543815\n",
            "1     0.535231\n",
            "Name: accuracy, dtype: float64\n",
            "\n",
            "Cross-dataset mean accuracy by layer:\n",
            " layer\n",
            "17    0.759115\n",
            "19    0.746154\n",
            "15    0.736538\n",
            "13    0.728000\n",
            "11    0.724615\n",
            "9     0.613000\n",
            "7     0.558846\n",
            "5     0.556308\n",
            "3     0.546115\n",
            "1     0.535385\n",
            "Name: accuracy, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pivot = (df.groupby([\"layer\",\"eval_dataset\"])[\"accuracy\"]\n",
        "           .mean()\n",
        "           .reset_index()\n",
        "           .pivot(index=\"layer\", columns=\"eval_dataset\", values=\"accuracy\")\n",
        "           .sort_index())\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.imshow(pivot.values, aspect=\"auto\")\n",
        "plt.xticks(range(pivot.shape[1]), pivot.columns, rotation=45, ha=\"right\")\n",
        "plt.yticks(range(pivot.shape[0]), pivot.index)\n",
        "plt.colorbar(label=\"Accuracy\")\n",
        "plt.title(\"Accuracy heatmap (mean over train datasets)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "l90XJuJ9PD1r",
        "outputId": "52f86367-d3ed-48a1-e7eb-b3815fdc32cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "same = (df[df[\"same_dataset\"] == True].groupby(\"layer\")[\"accuracy\"].mean())\n",
        "diff = (df[df[\"same_dataset\"] == False].groupby(\"layer\")[\"accuracy\"].mean())\n",
        "gap = (same - diff).dropna().sort_values(ascending=False)\n",
        "print(\"Generalization gap (same - cross) by layer:\\n\", gap.head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lYEbLVSPJMJ",
        "outputId": "5d48174b-0b4b-4960-80b5-ff724be07029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generalization gap (same - cross) by layer:\n",
            " layer\n",
            "13    0.244000\n",
            "19    0.235846\n",
            "15    0.235462\n",
            "9     0.227000\n",
            "17    0.216885\n",
            "11    0.211385\n",
            "7     0.069462\n",
            "5     0.004000\n",
            "1    -0.000769\n",
            "3    -0.011500\n",
            "Name: accuracy, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose the 10 most generalizable layers\n",
        "top10_layers = xgen_by_layer.index[:10].tolist()\n",
        "print(\"MVP layers:\", top10_layers)\n",
        "\n",
        "# Save for your inference notebook\n",
        "pd.Series(top10_layers, name=\"layer\").to_frame().to_csv(\"mvp_layers.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTT7y3dMPNTq",
        "outputId": "e049279f-c987-4835-83b0-66c5ce504904"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MVP layers: [17, 19, 15, 13, 11, 9, 7, 5, 3, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# choose one training dataset to define 10 probes\n",
        "TRAIN_DS = \"got_cities\"\n",
        "\n",
        "# load the big probes dict downloaded from S3:\n",
        "import pickle, re\n",
        "from pathlib import Path\n",
        "PROBES_PATH = Path(\"output/probes/alina_mvp_train/probes/probes.pickle\")\n",
        "\n",
        "with open(PROBES_PATH, \"rb\") as f:\n",
        "    all_probes = pickle.load(f)   # dict[str -> TrainedProbe]\n",
        "\n",
        "# keep only keys that match this dataset and layers 1,3,...,19\n",
        "want_layers = {1,3,5,7,9,11,13,15,17,19}\n",
        "selected = {}\n",
        "for k, tp in all_probes.items():\n",
        "    # keys look like: Llama-2-7b-chat-hf-<DATASET>-lr-h{layer}-0\n",
        "    if f\"-{TRAIN_DS}-\" in k:\n",
        "        m = re.search(r\"-h(\\d+)-0$\", k)\n",
        "        if m:\n",
        "            L = int(m.group(1))\n",
        "            if L in want_layers:\n",
        "                selected[f\"L{L}\"] = tp\n",
        "\n",
        "print(\"Picked\", len(selected), \"probes:\", sorted(selected))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIsx7UjuVCZz",
        "outputId": "eae1a643-9fdc-4644-cf02-0054689fcbf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Picked 10 probes: ['L1', 'L11', 'L13', 'L15', 'L17', 'L19', 'L3', 'L5', 'L7', 'L9']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SELECTIONS\n",
        "SELECT_DATASET = \"got_cities\"\n",
        "LAYERS = [1,3,5,7,9,11,13,15,17,19]\n",
        "\n",
        "PROBES_PATH = \"output/probes/alina_mvp_train/probes/probes.pickle\"\n",
        "MODEL_ID = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# ---- load probes ------------------------------------------------\n",
        "import pickle, re\n",
        "with open(PROBES_PATH, \"rb\") as f:\n",
        "    probes = pickle.load(f)\n",
        "\n",
        "# ---- build {layer -> probe} using selections ---------------\n",
        "DATASETS = [\n",
        "    \"got_cities\", \"got_sp_en_trans\",\n",
        "    \"got_cities_cities_conj\", \"got_cities_cities_disj\",\n",
        "    \"got_larger_than\"\n",
        "]\n",
        "\n",
        "layer_to_probe = {}\n",
        "for key, tp in probes.items():\n",
        "    # parse dataset from key\n",
        "    ds = next((d for d in DATASETS if f\"-{d}-\" in key), None)\n",
        "    m = re.search(r\"-h(\\d+)-\", key)\n",
        "    layer = int(m.group(1)) if m else None\n",
        "    if ds == SELECT_DATASET and layer in LAYERS:\n",
        "        layer_to_probe[layer] = tp\n",
        "\n",
        "# check\n",
        "assert set(layer_to_probe.keys()) == set(LAYERS), \\\n",
        "    f\"Got {sorted(layer_to_probe)} but expected {LAYERS}\"\n",
        "print(\"Using layers:\", sorted(layer_to_probe))\n",
        "\n",
        "# ---- 3) keep helper functions exactly as they are -------\n",
        "# helpers\n",
        "def _sigmoid(x):\n",
        "    try:\n",
        "        return 1.0/(1.0+exp(-float(x)))\n",
        "    except OverflowError:\n",
        "        return 0.0 if x < 0 else 1.0\n",
        "\n",
        "def probe_to_estimator(tp):\n",
        "    \"\"\"\n",
        "    ProbeEng's TrainedProbe stores a serialized sklearn model.\n",
        "    Its instance method `deserialize_probe(probe_data)` needs the payload.\n",
        "    We'll try common field names and, if needed, scan the model_dump.\n",
        "    \"\"\"\n",
        "    # 1) common attribute names\n",
        "    for attr in (\"probe\", \"probe_data\", \"serialized_probe\", \"estimator_data\", \"data\"):\n",
        "        if hasattr(tp, attr):\n",
        "            payload = getattr(tp, attr)\n",
        "            if payload is not None:\n",
        "                try:\n",
        "                    return tp.deserialize_probe(payload)\n",
        "                except Exception as e:\n",
        "                    last_err = e  # try next candidate\n",
        "\n",
        "    # 2) search inside the pydantic payload\n",
        "    dump = {}\n",
        "    try:\n",
        "        dump = tp.model_dump()\n",
        "    except Exception:\n",
        "        try:\n",
        "            dump = tp.dict()\n",
        "        except Exception:\n",
        "            dump = {}\n",
        "\n",
        "    # look for a dict that smells like a serialized payload\n",
        "    for k, v in dump.items():\n",
        "        if isinstance(v, dict) and any(x in v for x in (\"data\", \"bytes\", \"blob\", \"pickle\", \"b64\", \"content\", \"payload\")):\n",
        "            try:\n",
        "                return tp.deserialize_probe(v)\n",
        "            except Exception as e:\n",
        "                last_err = e\n",
        "\n",
        "    # 3) if we still couldn't get it, show helpful debug info\n",
        "    raise RuntimeError(\n",
        "        \"Could not deserialize sklearn estimator from TrainedProbe.\\n\"\n",
        "        f\"Fields seen: {list(dump.keys())[:15]}\"\n",
        "    )\n",
        "\n",
        "\n",
        "def layer_last_token_vec(text: str, layer: int) -> np.ndarray:\n",
        "    toks = tok(text, return_tensors=\"pt\", add_special_tokens=True)\n",
        "    toks = {k: v.to(model.device) for k, v in toks.items()}\n",
        "    with torch.no_grad():\n",
        "        out = model(**toks, output_hidden_states=True)\n",
        "    # hidden_states[0]=embeddings, [1]=after layer 1, ... so h{L} = index L\n",
        "    hs = out.hidden_states[layer]           # [1, seq, hidden]\n",
        "    vec = hs[:, -1, :].detach().cpu().numpy()  # last token\n",
        "    return vec  # shape (1, hidden)\n",
        "\n",
        "def est_proba01(est, X_np):\n",
        "    \"\"\"\n",
        "    Return a probability in [0,1] from a variety of estimator APIs.\n",
        "    Handles sklearn (predict_proba / decision_function / predict) and\n",
        "    ProbeEng PredictResult-style returns.\n",
        "    \"\"\"\n",
        "    def _sigmoid(x):\n",
        "        return float(1.0 / (1.0 + np.exp(-float(x))))\n",
        "\n",
        "    # 1) Standard sklearn path\n",
        "    if hasattr(est, \"predict_proba\"):\n",
        "        proba = est.predict_proba(X_np)\n",
        "        proba = np.asarray(proba)\n",
        "        if proba.ndim == 2 and proba.shape[1] >= 2:\n",
        "            return float(proba[0, 1])\n",
        "        return float(proba.ravel()[0])\n",
        "\n",
        "    if hasattr(est, \"decision_function\"):\n",
        "        score = est.decision_function(X_np)\n",
        "        if isinstance(score, (list, tuple, np.ndarray)):\n",
        "            score = np.asarray(score).ravel()[0]\n",
        "        return _sigmoid(score)\n",
        "\n",
        "    # 2) Generic predict path (may be ndarray OR a PredictResult-like object)\n",
        "    y = est.predict(X_np)\n",
        "\n",
        "    # 2a) If it's array-like, grab first element\n",
        "    if isinstance(y, (list, tuple, np.ndarray)):\n",
        "        return float(np.asarray(y).ravel()[0])\n",
        "\n",
        "    # 2b) If it's a PredictResult-like object, try common fields\n",
        "    for attr in (\"proba\", \"probs\", \"probabilities\", \"y_proba\"):\n",
        "        if hasattr(y, attr):\n",
        "            arr = np.asarray(getattr(y, attr))\n",
        "            if arr.ndim == 2 and arr.shape[1] >= 2:\n",
        "                return float(arr[0, 1])\n",
        "            return float(np.clip(arr.ravel()[0], 0.0, 1.0))\n",
        "\n",
        "    for attr in (\"scores\", \"logits\"):\n",
        "        if hasattr(y, attr):\n",
        "            arr = np.asarray(getattr(y, attr))\n",
        "            return _sigmoid(arr.ravel()[0])\n",
        "\n",
        "    for attr in (\"pred\", \"prediction\", \"label\", \"y\"):\n",
        "        if hasattr(y, attr):\n",
        "            return float(getattr(y, attr))\n",
        "\n",
        "    # 2c) Scalar fallback\n",
        "    if np.isscalar(y):\n",
        "        return float(y)\n",
        "\n",
        "    raise RuntimeError(f\"Estimator predict returned unsupported object of type {type(y)}\")\n",
        "\n",
        "\n",
        "def get_credences(claim: str) -> dict:\n",
        "    res = {}\n",
        "    for L, tp in layer_to_probe.items():\n",
        "        est = probe_to_estimator(tp)\n",
        "        x = layer_last_token_vec(claim, L)\n",
        "        res[f\"L{L}\"] = est_proba01(est, x)\n",
        "    return res\n",
        "# ---- 4) ask a claim ------------------------------------------------\n",
        "claim = \"Humans have flown close to the sun.\"\n",
        "credences = get_credences(claim)\n",
        "for k in sorted(credences, key=lambda s: int(s[1:])):\n",
        "    print(f\"{k}: {credences[k]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7Y2vUK7VW4q",
        "outputId": "8481a573-7857-4614-95fa-b30e7b30c8d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using layers: [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]\n",
            "L1: 0.505\n",
            "L3: 0.421\n",
            "L5: 0.698\n",
            "L7: 0.828\n",
            "L9: 0.650\n",
            "L11: 0.604\n",
            "L13: 0.888\n",
            "L15: 0.533\n",
            "L17: 0.509\n",
            "L19: 0.271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8odT4ih4cuDD"
      },
      "source": [
        "Run these after restarting"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fa36f70b52674721abfb09b2b274a764": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_23aedea6a2934ad58f7e1e2f73f719f3",
              "IPY_MODEL_dd4e55bffb6b41d8b6f96bfb69a3a141",
              "IPY_MODEL_725f0f4cf78e426b883285a4ab8d1048"
            ],
            "layout": "IPY_MODEL_5431f80a201b49fbae52946cebd96b16"
          }
        },
        "23aedea6a2934ad58f7e1e2f73f719f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de77580fa53c45be9be173c71f2aaa29",
            "placeholder": "​",
            "style": "IPY_MODEL_e9e542d81a2947aca84e1d69b8b024a2",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "dd4e55bffb6b41d8b6f96bfb69a3a141": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83deffec2e084e1e9352772ecd582330",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2de435b0b1fb4b01b376cfc29873d95a",
            "value": 2
          }
        },
        "725f0f4cf78e426b883285a4ab8d1048": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85ee55a2b68149a1bbddd07598e5f36f",
            "placeholder": "​",
            "style": "IPY_MODEL_7fb7f2ac566947f8bda721f61ec2b3e2",
            "value": " 2/2 [00:04&lt;00:00,  1.95s/it]"
          }
        },
        "5431f80a201b49fbae52946cebd96b16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de77580fa53c45be9be173c71f2aaa29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9e542d81a2947aca84e1d69b8b024a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83deffec2e084e1e9352772ecd582330": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2de435b0b1fb4b01b376cfc29873d95a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "85ee55a2b68149a1bbddd07598e5f36f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fb7f2ac566947f8bda721f61ec2b3e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2df8175d68594329a8c446cb571d0b48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d77e051ddcec482db0d9d215ac26e5c1",
              "IPY_MODEL_81fed2e2583b4875b46618461cc450ab",
              "IPY_MODEL_eea40b92f92142f6a96a579d5615a8a2"
            ],
            "layout": "IPY_MODEL_36702a36cb6e46efa1cd464e0df4b5dc"
          }
        },
        "d77e051ddcec482db0d9d215ac26e5c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46e8e16b18604fe5b3614d828199caa5",
            "placeholder": "​",
            "style": "IPY_MODEL_afa26fe06f0b4bfc84370b06e2296f6d",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "81fed2e2583b4875b46618461cc450ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8afce4d57a24b27b3e13af899c27869",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7e6a4239048c45288a0e676068c294af",
            "value": 2
          }
        },
        "eea40b92f92142f6a96a579d5615a8a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f215d98241aa4d71ae7c8ca2afc478d6",
            "placeholder": "​",
            "style": "IPY_MODEL_8fd6d369483f489ca9da953834e5f16b",
            "value": " 2/2 [00:03&lt;00:00,  1.80s/it]"
          }
        },
        "36702a36cb6e46efa1cd464e0df4b5dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46e8e16b18604fe5b3614d828199caa5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afa26fe06f0b4bfc84370b06e2296f6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8afce4d57a24b27b3e13af899c27869": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e6a4239048c45288a0e676068c294af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f215d98241aa4d71ae7c8ca2afc478d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fd6d369483f489ca9da953834e5f16b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}